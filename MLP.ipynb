{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2734fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch._C import device\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "plt.style.use([\"classic\"])\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd73046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_L(epoch, x, y):\n",
    "    if epoch % 50 != 0: return\n",
    "    plt.plot(x.cpu().detach().numpy(), y.cpu().detach().numpy(), \"k--\",label=\"MLP\")\n",
    "    plt.xlabel(r\"$r$\")\n",
    "    plt.ylabel(r\"$L$\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pic/r_vs_L_epoch%s.jpg\"%epoch,bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "def vis_W(epoch, x, y):\n",
    "    if epoch % 50 != 0: return\n",
    "    plt.plot(x.cpu().detach().numpy(), y.cpu().detach().numpy(), \"k--\",label=\"MLP\")\n",
    "    plt.xlabel(r\"$r$\")\n",
    "    plt.ylabel(r\"$W$\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pic/r_vs_W_epoch%s.jpg\"%epoch,bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "def vis_LE(epoch, x, y1, y2):\n",
    "    if epoch % 50 != 0: return\n",
    "    plt.plot(x.cpu().detach().numpy(),y1.cpu().detach().numpy(),\"k--\",label=\"MLP\")\n",
    "    plt.plot(x.cpu().detach().numpy(),y2.cpu().detach().numpy(),\"r--\",label=\"lattice\")\n",
    "    plt.xlabel(r\"$L$\")\n",
    "    plt.ylabel(r\"$E$\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pic/L_vs_E_epoch%s.jpg\"%epoch,bbox_inches = \"tight\")\n",
    "    plt.close()  \n",
    "def vis_x(epoch, x, y1, y2):\n",
    "    if epoch % 50 != 0: return\n",
    "    plt.plot(x.cpu().detach().numpy(),y1.cpu().detach().numpy(),\"k--\",label=\"MLP\")\n",
    "#     plt.plot(x.cpu().detach().numpy(),y2.cpu().detach().numpy(),\"r--\",label=\"exp(0.4r)\")\n",
    "    plt.xlabel(r\"$r$\")\n",
    "    plt.ylabel(r\"$w$\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pic/x_vs_r_epoch%s.jpg\"%epoch,bbox_inches = \"tight\")\n",
    "    plt.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ca306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3 ,output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = torch.relu(self.fc3(x)) \n",
    "#         x = torch.exp(self.fc4(x))  # 使用Softplus确保输出非负 \n",
    "        x = F.softplus(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09ebfc",
   "metadata": {},
   "source": [
    "# 公式的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881bcc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_train(rmin, rmax, num_sample, learning_rate, epochs, path, device, func_LE):\n",
    "    \"\"\"\n",
    "    rmin, rmax: the min and max of r\n",
    "    num_sample: the num of sample\n",
    "    func_LE: L as a function of HQ energy\n",
    "    note: using the DNN to represent the x, w = x/r^2\n",
    "    \"\"\"\n",
    "    x_func = Net(1,64,128,64,1).to(device) \n",
    "#     x_func = Net(1,8,128,1).to(device)\n",
    "#     opt = optim.AdamW([{\"params\":x_func.parameters()}], lr = learning_rate)\n",
    "    opt = optim.AdamW([{\"params\":x_func.parameters()}], lr = learning_rate,betas=(0.9,0.999))\n",
    "#     opt = optim.SGD([{\"params\": x_func.parameters()}], lr=learning_rate)\n",
    "\n",
    "    lr_step_size =1000\n",
    "    schedular = lr_scheduler.StepLR(opt, step_size=lr_step_size, gamma=1) \n",
    "    \n",
    "    x_func.train()\n",
    "    \n",
    "    ########## loss\n",
    "    def loss_a(r):\n",
    "        \n",
    "        g = 0.176\n",
    "        r.requires_grad = True\n",
    "        r_min = 0.01\n",
    "        deg = 100\n",
    "        xk, wk = np.polynomial.legendre.leggauss(deg)\n",
    "\n",
    "        xk = torch.from_numpy(xk).float().to(device)\n",
    "        wk = torch.from_numpy(wk).float().to(device)\n",
    "        \n",
    "        x_f = x_func(r) \n",
    "        wr_func = x_f / r**2\n",
    "\n",
    "        L_dis  = torch.tensor([]).to(device)  \n",
    "        m1  = torch.tensor([]).to(device)  \n",
    "\n",
    "        for idx in range(len(r)):\n",
    "            \"\"\"\n",
    "            gauss legendre, deg = 50, xn is the node of r, and wn is the node of weight\n",
    "            calculating the integration part of distance L and HQ energy\n",
    "            \"\"\"\n",
    "            xn = (r[idx] - r_min) * 0.5 * xk +(r[idx] + r_min) * 0.5\n",
    "            wn = wk * (r[idx] - r_min) * 0.5\n",
    "\n",
    "            xn = xn.reshape(-1,1)\n",
    "            wn = wn.reshape(-1,1)  \n",
    "\n",
    "            xn = xn.to(torch.float32)\n",
    "            wn = wn.to(torch.float32)\n",
    "            \n",
    "\n",
    "            x_fn = x_func(xn) \n",
    "            w_rn = x_fn / xn**2\n",
    "            hn = torch.tensor([0.0], requires_grad=True, device=device, dtype=torch.float).reshape(-1, 1).to(device)\n",
    "            x_fn0 = x_func(hn) \n",
    "            x_fn0d = torch.autograd.grad(outputs=x_fn0, inputs=hn, create_graph=True)[0]\n",
    "            w_r0 = x_func(r[idx])/r[idx] ** 2\n",
    "            \n",
    "            Inter1 = 2  * 0.1973 * wn * (w_r0 ** 2 / (w_rn ** 2 - w_r0 ** 2)) ** 0.5\n",
    "            Inter2 = wn * w_rn * (1 + w_r0 ** 2 / (w_rn ** 2 - w_r0 ** 2)) ** 0.5  - wn * x_fn0 / xn ** 2 - wn * x_fn0d / xn \n",
    "        \n",
    "            p1 = torch.sum(Inter1)\n",
    "            p2 = torch.sum(Inter2)\n",
    "            \n",
    "            p1 = p1.reshape(-1,1)\n",
    "            p2 = p2.reshape(-1,1)\n",
    "            \n",
    "            L_dis = torch.cat((L_dis,p1),0)\n",
    "            m1 = torch.cat((m1,p2),0) \n",
    "        HQ_pred = 2 * g * m1 - 2 * g * x_fn0 / r + 2*g* x_fn0d*torch.log(r)\n",
    "#         HQ_true = E_t.detach().cpu().numpy()\n",
    "#         L_true = L_t.detach().cpu().numpy()\n",
    "        HQ_true = func_LE(L_dis.detach().cpu().numpy())\n",
    "        HQ_true = torch.FloatTensor(HQ_true).reshape(-1,1).to(device)\n",
    "#         print(\"L\",L_dis)\n",
    "#         L_true = torch.FloatTensor(L_true).reshape(-1,1).to(device)\n",
    "#         loss_mae = nn.MSELoss()\n",
    "#         loss_mae = nn.KLDivLoss()\n",
    "        constraint_loss = (x_fn0 - 1.0) ** 2\n",
    "        loss_mae = nn.L1Loss()\n",
    "        loss_1 = loss_mae(HQ_pred, HQ_true).to(device)\n",
    "\n",
    "        input_tensor = torch.tensor([0.], device=device)\n",
    "        target_tensor = torch.tensor([1], device=device)\n",
    "        loss_2 = loss_mae(x_func(input_tensor), target_tensor)\n",
    "\n",
    "#         loss_2 = loss_mae(x_func(torch.tensor([0.])), torch.tensor([1])).to(device)\n",
    "        loss_tot = loss_1 + loss_2+constraint_loss\n",
    "        \n",
    "        return loss_tot, r, wr_func, L_dis, HQ_pred, HQ_true, x_f\n",
    "\n",
    "    tic = time.time()\n",
    "    Loss_list = []\n",
    "    Loss_mean = 1e5\n",
    "    \n",
    "    r = np.linspace(rmin, rmax, num_sample)\n",
    "    r = torch.FloatTensor(r).reshape(-1,1).to(device) \n",
    "    \n",
    "    # Each step of the training process:\n",
    "    for epoch in range(epochs):\n",
    "        Loss = []\n",
    "        x_func.zero_grad()        \n",
    "        loss, r, wr_func, L_dis, HQ_pred, HQ_true, x_f = loss_a(r)\n",
    "        \n",
    "        # plot same fig during training\n",
    "#         vis_L(epoch, r, L_dis)\n",
    "        vis_x(epoch, r, x_f, torch.exp(0.450*r**2))\n",
    "#         vis_LE(epoch, r, HQ_pred, HQ_true)\n",
    "        vis_LE(epoch,L_dis,HQ_pred,HQ_true)\n",
    "#         vis_W(epoch, r, wr_func)\n",
    " \n",
    "        # Back propagation and optim\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        schedular.step()\n",
    "                \n",
    "        \n",
    "        lr = schedular.get_last_lr()[0]\n",
    "\n",
    "        # print the loss value\n",
    "        Loss.append(loss.item())\n",
    "        print('Train Epoch:{} learning rate:{:.4e}, Loss_tot:{:.4f},'.format(epoch+1, lr, loss.item()))\n",
    "              \n",
    "        Loss = np.array(Loss)\n",
    "        \n",
    "        # 检测Loss是否为NaN\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN. Saving the previous model...\")\n",
    "#             torch.save(checkpoint1, path+\"x_func_model1.pt\")\n",
    "            break\n",
    "            \n",
    "        # check the loss and save the model        \n",
    "        if np.mean(Loss) < Loss_mean:\n",
    "        \n",
    "            checkpoint1 = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': x_func.state_dict(),\n",
    "                    'optimizer': opt.state_dict(),\n",
    "                    'loss': loss.item()\n",
    "                    }\n",
    "            \n",
    "            torch.save(checkpoint1,path+\"x_func_model.pt\")\n",
    "            print(\"save model\")\n",
    "            Loss_mean = np.mean(Loss)\n",
    "            \n",
    "            torch.save(L_dis, path + 'L_dist.pt')\n",
    "            torch.save(HQ_pred, path + 'HQ_pred.pt')\n",
    "            torch.save(x_f, path + 'x_f.pt')\n",
    "            torch.save(r,path + 'r.pt')\n",
    "        \n",
    "        # 大于1000的时候每次都保存模型\n",
    "        if epoch > 1000:\n",
    "            model_state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': x_func.state_dict(),\n",
    "                'optimizer': opt.state_dict(),\n",
    "                'loss': loss.item()\n",
    "            }\n",
    "            torch.save(model_state, \"./model/log/\" + f\"x_func_model_{epoch + 1}.pt\")\n",
    "            print(f\"Save model {epoch + 1}\")\n",
    "            \n",
    "    toc = time.time()\n",
    "    trainTime = toc - tic\n",
    "    print(\"Training time = \", trainTime)\n",
    "    np.save(path + 'Loss_all',np.array(Loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3accdfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  There is this folder!  ---\n",
      "---  There is this folder!  ---\n",
      "---  There is this folder!  ---\n"
     ]
    }
   ],
   "source": [
    "# builf the folder\n",
    "def mkdir(path):\n",
    "\n",
    "    folder = os.path.exists(path)\n",
    "\n",
    "    if not folder:                   \n",
    "        os.makedirs(path)            \n",
    "        print (\"---  new folder...  ---\")\n",
    "        print (\"---  OK  ---\")\n",
    "\n",
    "    else:\n",
    "        print (\"---  There is this folder!  ---\")\n",
    "mkdir('data')\n",
    "mkdir('model')\n",
    "mkdir('pic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1695e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_LE_wrapper(L):\n",
    "    E =0.8404*L - 0.0866/L +0.1033\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93616f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1 learning rate:1.0000e-05, Loss_tot:0.7646,\n",
      "save model\n",
      "Train Epoch:2 learning rate:1.0000e-05, Loss_tot:0.7641,\n",
      "save model\n",
      "Train Epoch:3 learning rate:1.0000e-05, Loss_tot:0.7636,\n",
      "save model\n",
      "Train Epoch:4 learning rate:1.0000e-05, Loss_tot:0.7632,\n",
      "save model\n",
      "Train Epoch:5 learning rate:1.0000e-05, Loss_tot:0.7628,\n",
      "save model\n",
      "Train Epoch:6 learning rate:1.0000e-05, Loss_tot:0.7624,\n",
      "save model\n",
      "Train Epoch:7 learning rate:1.0000e-05, Loss_tot:0.7620,\n",
      "save model\n",
      "Train Epoch:8 learning rate:1.0000e-05, Loss_tot:0.7616,\n",
      "save model\n",
      "Train Epoch:9 learning rate:1.0000e-05, Loss_tot:0.7612,\n",
      "save model\n",
      "Train Epoch:10 learning rate:1.0000e-05, Loss_tot:0.7608,\n",
      "save model\n",
      "Train Epoch:11 learning rate:1.0000e-05, Loss_tot:0.7604,\n",
      "save model\n",
      "Train Epoch:12 learning rate:1.0000e-05, Loss_tot:0.7600,\n",
      "save model\n",
      "Train Epoch:13 learning rate:1.0000e-05, Loss_tot:0.7596,\n",
      "save model\n",
      "Train Epoch:14 learning rate:1.0000e-05, Loss_tot:0.7591,\n",
      "save model\n",
      "Train Epoch:15 learning rate:1.0000e-05, Loss_tot:0.7586,\n",
      "save model\n",
      "Train Epoch:16 learning rate:1.0000e-05, Loss_tot:0.7581,\n",
      "save model\n",
      "Train Epoch:17 learning rate:1.0000e-05, Loss_tot:0.7575,\n",
      "save model\n",
      "Train Epoch:18 learning rate:1.0000e-05, Loss_tot:0.7569,\n",
      "save model\n",
      "Train Epoch:19 learning rate:1.0000e-05, Loss_tot:0.7562,\n",
      "save model\n",
      "Train Epoch:20 learning rate:1.0000e-05, Loss_tot:0.7600,\n",
      "Train Epoch:21 learning rate:1.0000e-05, Loss_tot:0.7597,\n",
      "Train Epoch:22 learning rate:1.0000e-05, Loss_tot:0.7594,\n",
      "Train Epoch:23 learning rate:1.0000e-05, Loss_tot:0.7591,\n",
      "Train Epoch:24 learning rate:1.0000e-05, Loss_tot:0.7588,\n",
      "Train Epoch:25 learning rate:1.0000e-05, Loss_tot:0.7585,\n",
      "Train Epoch:26 learning rate:1.0000e-05, Loss_tot:0.7582,\n",
      "Train Epoch:27 learning rate:1.0000e-05, Loss_tot:0.7578,\n",
      "Train Epoch:28 learning rate:1.0000e-05, Loss_tot:0.7575,\n",
      "Train Epoch:29 learning rate:1.0000e-05, Loss_tot:0.7572,\n",
      "Train Epoch:30 learning rate:1.0000e-05, Loss_tot:0.7569,\n",
      "Train Epoch:31 learning rate:1.0000e-05, Loss_tot:0.7566,\n",
      "Train Epoch:32 learning rate:1.0000e-05, Loss_tot:0.7563,\n",
      "Train Epoch:33 learning rate:1.0000e-05, Loss_tot:0.7560,\n",
      "save model\n",
      "Train Epoch:34 learning rate:1.0000e-05, Loss_tot:0.7556,\n",
      "save model\n",
      "Train Epoch:35 learning rate:1.0000e-05, Loss_tot:0.7553,\n",
      "save model\n",
      "Train Epoch:36 learning rate:1.0000e-05, Loss_tot:0.7550,\n",
      "save model\n",
      "Train Epoch:37 learning rate:1.0000e-05, Loss_tot:0.7547,\n",
      "save model\n",
      "Train Epoch:38 learning rate:1.0000e-05, Loss_tot:0.7559,\n",
      "Train Epoch:39 learning rate:1.0000e-05, Loss_tot:0.7554,\n",
      "Train Epoch:40 learning rate:1.0000e-05, Loss_tot:0.7549,\n",
      "Train Epoch:41 learning rate:1.0000e-05, Loss_tot:0.7545,\n",
      "save model\n",
      "Train Epoch:42 learning rate:1.0000e-05, Loss_tot:0.7541,\n",
      "save model\n",
      "Train Epoch:43 learning rate:1.0000e-05, Loss_tot:0.7537,\n",
      "save model\n",
      "Train Epoch:44 learning rate:1.0000e-05, Loss_tot:0.7533,\n",
      "save model\n",
      "Train Epoch:45 learning rate:1.0000e-05, Loss_tot:0.7529,\n",
      "save model\n",
      "Train Epoch:46 learning rate:1.0000e-05, Loss_tot:0.7526,\n",
      "save model\n",
      "Train Epoch:47 learning rate:1.0000e-05, Loss_tot:0.7522,\n",
      "save model\n",
      "Train Epoch:48 learning rate:1.0000e-05, Loss_tot:0.7519,\n",
      "save model\n",
      "Train Epoch:49 learning rate:1.0000e-05, Loss_tot:0.7516,\n",
      "save model\n",
      "Train Epoch:50 learning rate:1.0000e-05, Loss_tot:0.7512,\n",
      "save model\n",
      "Train Epoch:51 learning rate:1.0000e-05, Loss_tot:0.7509,\n",
      "save model\n",
      "Train Epoch:52 learning rate:1.0000e-05, Loss_tot:0.7506,\n",
      "save model\n",
      "Train Epoch:53 learning rate:1.0000e-05, Loss_tot:0.7502,\n",
      "save model\n",
      "Train Epoch:54 learning rate:1.0000e-05, Loss_tot:0.7499,\n",
      "save model\n",
      "Train Epoch:55 learning rate:1.0000e-05, Loss_tot:0.7496,\n",
      "save model\n",
      "Train Epoch:56 learning rate:1.0000e-05, Loss_tot:0.7493,\n",
      "save model\n",
      "Train Epoch:57 learning rate:1.0000e-05, Loss_tot:0.7490,\n",
      "save model\n",
      "Train Epoch:58 learning rate:1.0000e-05, Loss_tot:0.7486,\n",
      "save model\n",
      "Train Epoch:59 learning rate:1.0000e-05, Loss_tot:0.7483,\n",
      "save model\n",
      "Train Epoch:60 learning rate:1.0000e-05, Loss_tot:0.7480,\n",
      "save model\n",
      "Train Epoch:61 learning rate:1.0000e-05, Loss_tot:0.7477,\n",
      "save model\n",
      "Train Epoch:62 learning rate:1.0000e-05, Loss_tot:0.7474,\n",
      "save model\n",
      "Train Epoch:63 learning rate:1.0000e-05, Loss_tot:0.7471,\n",
      "save model\n",
      "Train Epoch:64 learning rate:1.0000e-05, Loss_tot:0.7467,\n",
      "save model\n",
      "Train Epoch:65 learning rate:1.0000e-05, Loss_tot:0.7464,\n",
      "save model\n",
      "Train Epoch:66 learning rate:1.0000e-05, Loss_tot:0.7461,\n",
      "save model\n",
      "Train Epoch:67 learning rate:1.0000e-05, Loss_tot:0.7458,\n",
      "save model\n",
      "Train Epoch:68 learning rate:1.0000e-05, Loss_tot:0.7455,\n",
      "save model\n",
      "Train Epoch:69 learning rate:1.0000e-05, Loss_tot:0.7452,\n",
      "save model\n",
      "Train Epoch:70 learning rate:1.0000e-05, Loss_tot:0.7449,\n",
      "save model\n",
      "Train Epoch:71 learning rate:1.0000e-05, Loss_tot:0.7446,\n",
      "save model\n",
      "Train Epoch:72 learning rate:1.0000e-05, Loss_tot:0.7442,\n",
      "save model\n",
      "Train Epoch:73 learning rate:1.0000e-05, Loss_tot:0.7439,\n",
      "save model\n",
      "Train Epoch:74 learning rate:1.0000e-05, Loss_tot:0.7436,\n",
      "save model\n",
      "Train Epoch:75 learning rate:1.0000e-05, Loss_tot:0.7433,\n",
      "save model\n",
      "Train Epoch:76 learning rate:1.0000e-05, Loss_tot:0.7430,\n",
      "save model\n",
      "Train Epoch:77 learning rate:1.0000e-05, Loss_tot:0.7427,\n",
      "save model\n",
      "Train Epoch:78 learning rate:1.0000e-05, Loss_tot:0.7424,\n",
      "save model\n",
      "Train Epoch:79 learning rate:1.0000e-05, Loss_tot:0.7421,\n",
      "save model\n",
      "Train Epoch:80 learning rate:1.0000e-05, Loss_tot:0.7418,\n",
      "save model\n",
      "Train Epoch:81 learning rate:1.0000e-05, Loss_tot:0.7414,\n",
      "save model\n",
      "Train Epoch:82 learning rate:1.0000e-05, Loss_tot:0.7411,\n",
      "save model\n",
      "Train Epoch:83 learning rate:1.0000e-05, Loss_tot:0.7408,\n",
      "save model\n",
      "Train Epoch:84 learning rate:1.0000e-05, Loss_tot:0.7405,\n",
      "save model\n",
      "Train Epoch:85 learning rate:1.0000e-05, Loss_tot:0.7402,\n",
      "save model\n",
      "Train Epoch:86 learning rate:1.0000e-05, Loss_tot:0.7399,\n",
      "save model\n",
      "Train Epoch:87 learning rate:1.0000e-05, Loss_tot:0.7396,\n",
      "save model\n",
      "Train Epoch:88 learning rate:1.0000e-05, Loss_tot:0.7393,\n",
      "save model\n",
      "Train Epoch:89 learning rate:1.0000e-05, Loss_tot:0.7390,\n",
      "save model\n",
      "Train Epoch:90 learning rate:1.0000e-05, Loss_tot:0.7386,\n",
      "save model\n",
      "Train Epoch:91 learning rate:1.0000e-05, Loss_tot:0.7383,\n",
      "save model\n",
      "Train Epoch:92 learning rate:1.0000e-05, Loss_tot:0.7380,\n",
      "save model\n",
      "Train Epoch:93 learning rate:1.0000e-05, Loss_tot:0.7377,\n",
      "save model\n",
      "Train Epoch:94 learning rate:1.0000e-05, Loss_tot:0.7374,\n",
      "save model\n",
      "Train Epoch:95 learning rate:1.0000e-05, Loss_tot:0.7371,\n",
      "save model\n",
      "Train Epoch:96 learning rate:1.0000e-05, Loss_tot:0.7368,\n",
      "save model\n",
      "Train Epoch:97 learning rate:1.0000e-05, Loss_tot:0.7365,\n",
      "save model\n",
      "Train Epoch:98 learning rate:1.0000e-05, Loss_tot:0.7361,\n",
      "save model\n",
      "Train Epoch:99 learning rate:1.0000e-05, Loss_tot:0.7358,\n",
      "save model\n",
      "Train Epoch:100 learning rate:1.0000e-05, Loss_tot:0.7355,\n",
      "save model\n",
      "Train Epoch:101 learning rate:1.0000e-05, Loss_tot:0.7352,\n",
      "save model\n",
      "Train Epoch:102 learning rate:1.0000e-05, Loss_tot:0.7349,\n",
      "save model\n",
      "Train Epoch:103 learning rate:1.0000e-05, Loss_tot:0.7348,\n",
      "save model\n",
      "Train Epoch:104 learning rate:1.0000e-05, Loss_tot:0.7345,\n",
      "save model\n",
      "Train Epoch:105 learning rate:1.0000e-05, Loss_tot:0.7341,\n",
      "save model\n",
      "Train Epoch:106 learning rate:1.0000e-05, Loss_tot:0.7338,\n",
      "save model\n",
      "Train Epoch:107 learning rate:1.0000e-05, Loss_tot:0.7333,\n",
      "save model\n",
      "Train Epoch:108 learning rate:1.0000e-05, Loss_tot:0.7330,\n",
      "save model\n",
      "Train Epoch:109 learning rate:1.0000e-05, Loss_tot:0.7327,\n",
      "save model\n",
      "Train Epoch:110 learning rate:1.0000e-05, Loss_tot:0.7324,\n",
      "save model\n",
      "Train Epoch:111 learning rate:1.0000e-05, Loss_tot:0.7321,\n",
      "save model\n",
      "Train Epoch:112 learning rate:1.0000e-05, Loss_tot:0.7318,\n",
      "save model\n",
      "Train Epoch:113 learning rate:1.0000e-05, Loss_tot:0.7314,\n",
      "save model\n",
      "Train Epoch:114 learning rate:1.0000e-05, Loss_tot:0.7311,\n",
      "save model\n",
      "Train Epoch:115 learning rate:1.0000e-05, Loss_tot:0.7308,\n",
      "save model\n",
      "Train Epoch:116 learning rate:1.0000e-05, Loss_tot:0.7305,\n",
      "save model\n",
      "Train Epoch:117 learning rate:1.0000e-05, Loss_tot:0.7302,\n",
      "save model\n",
      "Train Epoch:118 learning rate:1.0000e-05, Loss_tot:0.7299,\n",
      "save model\n",
      "Train Epoch:119 learning rate:1.0000e-05, Loss_tot:0.7295,\n",
      "save model\n",
      "Train Epoch:120 learning rate:1.0000e-05, Loss_tot:0.7292,\n",
      "save model\n",
      "Train Epoch:121 learning rate:1.0000e-05, Loss_tot:0.7289,\n",
      "save model\n",
      "Train Epoch:122 learning rate:1.0000e-05, Loss_tot:0.7286,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:123 learning rate:1.0000e-05, Loss_tot:0.7282,\n",
      "save model\n",
      "Train Epoch:124 learning rate:1.0000e-05, Loss_tot:0.7279,\n",
      "save model\n",
      "Train Epoch:125 learning rate:1.0000e-05, Loss_tot:0.7276,\n",
      "save model\n",
      "Train Epoch:126 learning rate:1.0000e-05, Loss_tot:0.7273,\n",
      "save model\n",
      "Train Epoch:127 learning rate:1.0000e-05, Loss_tot:0.7269,\n",
      "save model\n",
      "Train Epoch:128 learning rate:1.0000e-05, Loss_tot:0.7266,\n",
      "save model\n",
      "Train Epoch:129 learning rate:1.0000e-05, Loss_tot:0.7263,\n",
      "save model\n",
      "Train Epoch:130 learning rate:1.0000e-05, Loss_tot:0.7259,\n",
      "save model\n",
      "Train Epoch:131 learning rate:1.0000e-05, Loss_tot:0.7256,\n",
      "save model\n",
      "Train Epoch:132 learning rate:1.0000e-05, Loss_tot:0.7253,\n",
      "save model\n",
      "Train Epoch:133 learning rate:1.0000e-05, Loss_tot:0.7249,\n",
      "save model\n",
      "Train Epoch:134 learning rate:1.0000e-05, Loss_tot:0.7246,\n",
      "save model\n",
      "Train Epoch:135 learning rate:1.0000e-05, Loss_tot:0.7242,\n",
      "save model\n",
      "Train Epoch:136 learning rate:1.0000e-05, Loss_tot:0.7239,\n",
      "save model\n",
      "Train Epoch:137 learning rate:1.0000e-05, Loss_tot:0.7235,\n",
      "save model\n",
      "Train Epoch:138 learning rate:1.0000e-05, Loss_tot:0.7232,\n",
      "save model\n",
      "Train Epoch:139 learning rate:1.0000e-05, Loss_tot:0.7228,\n",
      "save model\n",
      "Train Epoch:140 learning rate:1.0000e-05, Loss_tot:0.7225,\n",
      "save model\n",
      "Train Epoch:141 learning rate:1.0000e-05, Loss_tot:0.7230,\n",
      "Train Epoch:142 learning rate:1.0000e-05, Loss_tot:0.7227,\n",
      "Train Epoch:143 learning rate:1.0000e-05, Loss_tot:0.7224,\n",
      "save model\n",
      "Train Epoch:144 learning rate:1.0000e-05, Loss_tot:0.7221,\n",
      "save model\n",
      "Train Epoch:145 learning rate:1.0000e-05, Loss_tot:0.7217,\n",
      "save model\n",
      "Train Epoch:146 learning rate:1.0000e-05, Loss_tot:0.7214,\n",
      "save model\n",
      "Train Epoch:147 learning rate:1.0000e-05, Loss_tot:0.7211,\n",
      "save model\n",
      "Train Epoch:148 learning rate:1.0000e-05, Loss_tot:0.7208,\n",
      "save model\n",
      "Train Epoch:149 learning rate:1.0000e-05, Loss_tot:0.7204,\n",
      "save model\n",
      "Train Epoch:150 learning rate:1.0000e-05, Loss_tot:0.7201,\n",
      "save model\n",
      "Train Epoch:151 learning rate:1.0000e-05, Loss_tot:0.7198,\n",
      "save model\n",
      "Train Epoch:152 learning rate:1.0000e-05, Loss_tot:0.7194,\n",
      "save model\n",
      "Train Epoch:153 learning rate:1.0000e-05, Loss_tot:0.7191,\n",
      "save model\n",
      "Train Epoch:154 learning rate:1.0000e-05, Loss_tot:0.7188,\n",
      "save model\n",
      "Train Epoch:155 learning rate:1.0000e-05, Loss_tot:0.7185,\n",
      "save model\n",
      "Train Epoch:156 learning rate:1.0000e-05, Loss_tot:0.7181,\n",
      "save model\n",
      "Train Epoch:157 learning rate:1.0000e-05, Loss_tot:0.7178,\n",
      "save model\n",
      "Train Epoch:158 learning rate:1.0000e-05, Loss_tot:0.7170,\n",
      "save model\n",
      "Train Epoch:159 learning rate:1.0000e-05, Loss_tot:0.7171,\n",
      "Train Epoch:160 learning rate:1.0000e-05, Loss_tot:0.7168,\n",
      "save model\n",
      "Train Epoch:161 learning rate:1.0000e-05, Loss_tot:0.7165,\n",
      "save model\n",
      "Train Epoch:162 learning rate:1.0000e-05, Loss_tot:0.7162,\n",
      "save model\n",
      "Train Epoch:163 learning rate:1.0000e-05, Loss_tot:0.7158,\n",
      "save model\n",
      "Train Epoch:164 learning rate:1.0000e-05, Loss_tot:0.7155,\n",
      "save model\n",
      "Train Epoch:165 learning rate:1.0000e-05, Loss_tot:0.7152,\n",
      "save model\n",
      "Train Epoch:166 learning rate:1.0000e-05, Loss_tot:0.7148,\n",
      "save model\n",
      "Train Epoch:167 learning rate:1.0000e-05, Loss_tot:0.7145,\n",
      "save model\n",
      "Train Epoch:168 learning rate:1.0000e-05, Loss_tot:0.7142,\n",
      "save model\n",
      "Train Epoch:169 learning rate:1.0000e-05, Loss_tot:0.7138,\n",
      "save model\n",
      "Train Epoch:170 learning rate:1.0000e-05, Loss_tot:0.7135,\n",
      "save model\n",
      "Train Epoch:171 learning rate:1.0000e-05, Loss_tot:0.7132,\n",
      "save model\n",
      "Train Epoch:172 learning rate:1.0000e-05, Loss_tot:0.7128,\n",
      "save model\n",
      "Train Epoch:173 learning rate:1.0000e-05, Loss_tot:0.7125,\n",
      "save model\n",
      "Train Epoch:174 learning rate:1.0000e-05, Loss_tot:0.7121,\n",
      "save model\n",
      "Train Epoch:175 learning rate:1.0000e-05, Loss_tot:0.7118,\n",
      "save model\n",
      "Train Epoch:176 learning rate:1.0000e-05, Loss_tot:0.7115,\n",
      "save model\n",
      "Train Epoch:177 learning rate:1.0000e-05, Loss_tot:0.7111,\n",
      "save model\n",
      "Train Epoch:178 learning rate:1.0000e-05, Loss_tot:0.7108,\n",
      "save model\n",
      "Train Epoch:179 learning rate:1.0000e-05, Loss_tot:0.7104,\n",
      "save model\n",
      "Train Epoch:180 learning rate:1.0000e-05, Loss_tot:0.7101,\n",
      "save model\n",
      "Train Epoch:181 learning rate:1.0000e-05, Loss_tot:0.7097,\n",
      "save model\n",
      "Train Epoch:182 learning rate:1.0000e-05, Loss_tot:0.7094,\n",
      "save model\n",
      "Train Epoch:183 learning rate:1.0000e-05, Loss_tot:0.7090,\n",
      "save model\n",
      "Train Epoch:184 learning rate:1.0000e-05, Loss_tot:0.7086,\n",
      "save model\n",
      "Train Epoch:185 learning rate:1.0000e-05, Loss_tot:0.7083,\n",
      "save model\n",
      "Train Epoch:186 learning rate:1.0000e-05, Loss_tot:0.7079,\n",
      "save model\n",
      "Train Epoch:187 learning rate:1.0000e-05, Loss_tot:0.7076,\n",
      "save model\n",
      "Train Epoch:188 learning rate:1.0000e-05, Loss_tot:0.7072,\n",
      "save model\n",
      "Train Epoch:189 learning rate:1.0000e-05, Loss_tot:0.7069,\n",
      "save model\n",
      "Train Epoch:190 learning rate:1.0000e-05, Loss_tot:0.7065,\n",
      "save model\n",
      "Train Epoch:191 learning rate:1.0000e-05, Loss_tot:0.7061,\n",
      "save model\n",
      "Train Epoch:192 learning rate:1.0000e-05, Loss_tot:0.7057,\n",
      "save model\n",
      "Train Epoch:193 learning rate:1.0000e-05, Loss_tot:0.7067,\n",
      "Train Epoch:194 learning rate:1.0000e-05, Loss_tot:0.7063,\n",
      "Train Epoch:195 learning rate:1.0000e-05, Loss_tot:0.7059,\n",
      "Train Epoch:196 learning rate:1.0000e-05, Loss_tot:0.7055,\n",
      "save model\n",
      "Train Epoch:197 learning rate:1.0000e-05, Loss_tot:0.7051,\n",
      "save model\n",
      "Train Epoch:198 learning rate:1.0000e-05, Loss_tot:0.7047,\n",
      "save model\n",
      "Train Epoch:199 learning rate:1.0000e-05, Loss_tot:0.7043,\n",
      "save model\n",
      "Train Epoch:200 learning rate:1.0000e-05, Loss_tot:0.7038,\n",
      "save model\n",
      "Train Epoch:201 learning rate:1.0000e-05, Loss_tot:0.7034,\n",
      "save model\n",
      "Train Epoch:202 learning rate:1.0000e-05, Loss_tot:0.7047,\n",
      "Train Epoch:203 learning rate:1.0000e-05, Loss_tot:0.7043,\n",
      "Train Epoch:204 learning rate:1.0000e-05, Loss_tot:0.7046,\n",
      "Train Epoch:205 learning rate:1.0000e-05, Loss_tot:0.7043,\n",
      "Train Epoch:206 learning rate:1.0000e-05, Loss_tot:0.7039,\n",
      "Train Epoch:207 learning rate:1.0000e-05, Loss_tot:0.7036,\n",
      "Train Epoch:208 learning rate:1.0000e-05, Loss_tot:0.7032,\n",
      "save model\n",
      "Train Epoch:209 learning rate:1.0000e-05, Loss_tot:0.7010,\n",
      "save model\n",
      "Train Epoch:210 learning rate:1.0000e-05, Loss_tot:0.7020,\n",
      "Train Epoch:211 learning rate:1.0000e-05, Loss_tot:0.7016,\n",
      "Train Epoch:212 learning rate:1.0000e-05, Loss_tot:0.7018,\n",
      "Train Epoch:213 learning rate:1.0000e-05, Loss_tot:0.7015,\n",
      "Train Epoch:214 learning rate:1.0000e-05, Loss_tot:0.7012,\n",
      "Train Epoch:215 learning rate:1.0000e-05, Loss_tot:0.7003,\n",
      "save model\n",
      "Train Epoch:216 learning rate:1.0000e-05, Loss_tot:0.7000,\n",
      "save model\n",
      "Train Epoch:217 learning rate:1.0000e-05, Loss_tot:0.7002,\n",
      "Train Epoch:218 learning rate:1.0000e-05, Loss_tot:0.6999,\n",
      "save model\n",
      "Train Epoch:219 learning rate:1.0000e-05, Loss_tot:0.6995,\n",
      "save model\n",
      "Train Epoch:220 learning rate:1.0000e-05, Loss_tot:0.6992,\n",
      "save model\n",
      "Train Epoch:221 learning rate:1.0000e-05, Loss_tot:0.6984,\n",
      "save model\n",
      "Train Epoch:222 learning rate:1.0000e-05, Loss_tot:0.6980,\n",
      "save model\n",
      "Train Epoch:223 learning rate:1.0000e-05, Loss_tot:0.6977,\n",
      "save model\n",
      "Train Epoch:224 learning rate:1.0000e-05, Loss_tot:0.6973,\n",
      "save model\n",
      "Train Epoch:225 learning rate:1.0000e-05, Loss_tot:0.6975,\n",
      "Train Epoch:226 learning rate:1.0000e-05, Loss_tot:0.6972,\n",
      "save model\n",
      "Train Epoch:227 learning rate:1.0000e-05, Loss_tot:0.6968,\n",
      "save model\n",
      "Train Epoch:228 learning rate:1.0000e-05, Loss_tot:0.6965,\n",
      "save model\n",
      "Train Epoch:229 learning rate:1.0000e-05, Loss_tot:0.6957,\n",
      "save model\n",
      "Train Epoch:230 learning rate:1.0000e-05, Loss_tot:0.6953,\n",
      "save model\n",
      "Train Epoch:231 learning rate:1.0000e-05, Loss_tot:0.6949,\n",
      "save model\n",
      "Train Epoch:232 learning rate:1.0000e-05, Loss_tot:0.6952,\n",
      "Train Epoch:233 learning rate:1.0000e-05, Loss_tot:0.6926,\n",
      "save model\n",
      "Train Epoch:234 learning rate:1.0000e-05, Loss_tot:0.6946,\n",
      "Train Epoch:235 learning rate:1.0000e-05, Loss_tot:0.6942,\n",
      "Train Epoch:236 learning rate:1.0000e-05, Loss_tot:0.6939,\n",
      "Train Epoch:237 learning rate:1.0000e-05, Loss_tot:0.6930,\n",
      "Train Epoch:238 learning rate:1.0000e-05, Loss_tot:0.6927,\n",
      "Train Epoch:239 learning rate:1.0000e-05, Loss_tot:0.6924,\n",
      "save model\n",
      "Train Epoch:240 learning rate:1.0000e-05, Loss_tot:0.6926,\n",
      "Train Epoch:241 learning rate:1.0000e-05, Loss_tot:0.6923,\n",
      "save model\n",
      "Train Epoch:242 learning rate:1.0000e-05, Loss_tot:0.6920,\n",
      "save model\n",
      "Train Epoch:243 learning rate:1.0000e-05, Loss_tot:0.6916,\n",
      "save model\n",
      "Train Epoch:244 learning rate:1.0000e-05, Loss_tot:0.6886,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:245 learning rate:1.0000e-05, Loss_tot:0.6905,\n",
      "Train Epoch:246 learning rate:1.0000e-05, Loss_tot:0.6907,\n",
      "Train Epoch:247 learning rate:1.0000e-05, Loss_tot:0.6904,\n",
      "Train Epoch:248 learning rate:1.0000e-05, Loss_tot:0.6896,\n",
      "Train Epoch:249 learning rate:1.0000e-05, Loss_tot:0.6892,\n",
      "Train Epoch:250 learning rate:1.0000e-05, Loss_tot:0.6894,\n",
      "Train Epoch:251 learning rate:1.0000e-05, Loss_tot:0.6890,\n",
      "Train Epoch:252 learning rate:1.0000e-05, Loss_tot:0.6886,\n",
      "Train Epoch:253 learning rate:1.0000e-05, Loss_tot:0.6878,\n",
      "save model\n",
      "Train Epoch:254 learning rate:1.0000e-05, Loss_tot:0.6874,\n",
      "save model\n",
      "Train Epoch:255 learning rate:1.0000e-05, Loss_tot:0.6884,\n",
      "Train Epoch:256 learning rate:1.0000e-05, Loss_tot:0.6859,\n",
      "save model\n",
      "Train Epoch:257 learning rate:1.0000e-05, Loss_tot:0.6877,\n",
      "Train Epoch:258 learning rate:1.0000e-05, Loss_tot:0.6875,\n",
      "Train Epoch:259 learning rate:1.0000e-05, Loss_tot:0.6872,\n",
      "Train Epoch:260 learning rate:1.0000e-05, Loss_tot:0.6869,\n",
      "Train Epoch:261 learning rate:1.0000e-05, Loss_tot:0.6866,\n",
      "Train Epoch:262 learning rate:1.0000e-05, Loss_tot:0.6862,\n",
      "Train Epoch:263 learning rate:1.0000e-05, Loss_tot:0.6859,\n",
      "Train Epoch:264 learning rate:1.0000e-05, Loss_tot:0.6856,\n",
      "save model\n",
      "Train Epoch:265 learning rate:1.0000e-05, Loss_tot:0.6855,\n",
      "save model\n",
      "Train Epoch:266 learning rate:1.0000e-05, Loss_tot:0.6852,\n",
      "save model\n",
      "Train Epoch:267 learning rate:1.0000e-05, Loss_tot:0.6849,\n",
      "save model\n",
      "Train Epoch:268 learning rate:1.0000e-05, Loss_tot:0.6826,\n",
      "save model\n",
      "Train Epoch:269 learning rate:1.0000e-05, Loss_tot:0.6843,\n",
      "Train Epoch:270 learning rate:1.0000e-05, Loss_tot:0.6841,\n",
      "Train Epoch:271 learning rate:1.0000e-05, Loss_tot:0.6838,\n",
      "Train Epoch:272 learning rate:1.0000e-05, Loss_tot:0.6835,\n",
      "Train Epoch:273 learning rate:1.0000e-05, Loss_tot:0.6832,\n",
      "Train Epoch:274 learning rate:1.0000e-05, Loss_tot:0.6829,\n",
      "Train Epoch:275 learning rate:1.0000e-05, Loss_tot:0.6826,\n",
      "save model\n",
      "Train Epoch:276 learning rate:1.0000e-05, Loss_tot:0.6823,\n",
      "save model\n",
      "Train Epoch:277 learning rate:1.0000e-05, Loss_tot:0.6820,\n",
      "save model\n",
      "Train Epoch:278 learning rate:1.0000e-05, Loss_tot:0.6799,\n",
      "save model\n",
      "Train Epoch:279 learning rate:1.0000e-05, Loss_tot:0.6815,\n",
      "Train Epoch:280 learning rate:1.0000e-05, Loss_tot:0.6812,\n",
      "Train Epoch:281 learning rate:1.0000e-05, Loss_tot:0.6809,\n",
      "Train Epoch:282 learning rate:1.0000e-05, Loss_tot:0.6806,\n",
      "Train Epoch:283 learning rate:1.0000e-05, Loss_tot:0.6803,\n",
      "Train Epoch:284 learning rate:1.0000e-05, Loss_tot:0.6800,\n",
      "Train Epoch:285 learning rate:1.0000e-05, Loss_tot:0.6797,\n",
      "save model\n",
      "Train Epoch:286 learning rate:1.0000e-05, Loss_tot:0.6794,\n",
      "save model\n",
      "Train Epoch:287 learning rate:1.0000e-05, Loss_tot:0.6791,\n",
      "save model\n",
      "Train Epoch:288 learning rate:1.0000e-05, Loss_tot:0.6788,\n",
      "save model\n",
      "Train Epoch:289 learning rate:1.0000e-05, Loss_tot:0.6763,\n",
      "save model\n",
      "Train Epoch:290 learning rate:1.0000e-05, Loss_tot:0.6783,\n",
      "Train Epoch:291 learning rate:1.0000e-05, Loss_tot:0.6780,\n",
      "Train Epoch:292 learning rate:1.0000e-05, Loss_tot:0.6777,\n",
      "Train Epoch:293 learning rate:1.0000e-05, Loss_tot:0.6774,\n",
      "Train Epoch:294 learning rate:1.0000e-05, Loss_tot:0.6771,\n",
      "Train Epoch:295 learning rate:1.0000e-05, Loss_tot:0.6768,\n",
      "Train Epoch:296 learning rate:1.0000e-05, Loss_tot:0.6765,\n",
      "Train Epoch:297 learning rate:1.0000e-05, Loss_tot:0.6762,\n",
      "save model\n",
      "Train Epoch:298 learning rate:1.0000e-05, Loss_tot:0.6759,\n",
      "save model\n",
      "Train Epoch:299 learning rate:1.0000e-05, Loss_tot:0.6739,\n",
      "save model\n",
      "Train Epoch:300 learning rate:1.0000e-05, Loss_tot:0.6754,\n",
      "Train Epoch:301 learning rate:1.0000e-05, Loss_tot:0.6751,\n",
      "Train Epoch:302 learning rate:1.0000e-05, Loss_tot:0.6748,\n",
      "Train Epoch:303 learning rate:1.0000e-05, Loss_tot:0.6745,\n",
      "Train Epoch:304 learning rate:1.0000e-05, Loss_tot:0.6742,\n",
      "Train Epoch:305 learning rate:1.0000e-05, Loss_tot:0.6739,\n",
      "Train Epoch:306 learning rate:1.0000e-05, Loss_tot:0.6736,\n",
      "save model\n",
      "Train Epoch:307 learning rate:1.0000e-05, Loss_tot:0.6733,\n",
      "save model\n",
      "Train Epoch:308 learning rate:1.0000e-05, Loss_tot:0.6730,\n",
      "save model\n",
      "Train Epoch:309 learning rate:1.0000e-05, Loss_tot:0.6711,\n",
      "save model\n",
      "Train Epoch:310 learning rate:1.0000e-05, Loss_tot:0.6725,\n",
      "Train Epoch:311 learning rate:1.0000e-05, Loss_tot:0.6722,\n",
      "Train Epoch:312 learning rate:1.0000e-05, Loss_tot:0.6719,\n",
      "Train Epoch:313 learning rate:1.0000e-05, Loss_tot:0.6716,\n",
      "Train Epoch:314 learning rate:1.0000e-05, Loss_tot:0.6713,\n",
      "Train Epoch:315 learning rate:1.0000e-05, Loss_tot:0.6710,\n",
      "save model\n",
      "Train Epoch:316 learning rate:1.0000e-05, Loss_tot:0.6707,\n",
      "save model\n",
      "Train Epoch:317 learning rate:1.0000e-05, Loss_tot:0.6687,\n",
      "save model\n",
      "Train Epoch:318 learning rate:1.0000e-05, Loss_tot:0.6701,\n",
      "Train Epoch:319 learning rate:1.0000e-05, Loss_tot:0.6698,\n",
      "Train Epoch:320 learning rate:1.0000e-05, Loss_tot:0.6696,\n",
      "Train Epoch:321 learning rate:1.0000e-05, Loss_tot:0.6692,\n",
      "Train Epoch:322 learning rate:1.0000e-05, Loss_tot:0.6689,\n",
      "Train Epoch:323 learning rate:1.0000e-05, Loss_tot:0.6686,\n",
      "save model\n",
      "Train Epoch:324 learning rate:1.0000e-05, Loss_tot:0.6685,\n",
      "save model\n",
      "Train Epoch:325 learning rate:1.0000e-05, Loss_tot:0.6682,\n",
      "save model\n",
      "Train Epoch:326 learning rate:1.0000e-05, Loss_tot:0.6663,\n",
      "save model\n",
      "Train Epoch:327 learning rate:1.0000e-05, Loss_tot:0.6676,\n",
      "Train Epoch:328 learning rate:1.0000e-05, Loss_tot:0.6674,\n",
      "Train Epoch:329 learning rate:1.0000e-05, Loss_tot:0.6671,\n",
      "Train Epoch:330 learning rate:1.0000e-05, Loss_tot:0.6668,\n",
      "Train Epoch:331 learning rate:1.0000e-05, Loss_tot:0.6665,\n",
      "Train Epoch:332 learning rate:1.0000e-05, Loss_tot:0.6662,\n",
      "save model\n",
      "Train Epoch:333 learning rate:1.0000e-05, Loss_tot:0.6643,\n",
      "save model\n",
      "Train Epoch:334 learning rate:1.0000e-05, Loss_tot:0.6656,\n",
      "Train Epoch:335 learning rate:1.0000e-05, Loss_tot:0.6653,\n",
      "Train Epoch:336 learning rate:1.0000e-05, Loss_tot:0.6650,\n",
      "Train Epoch:337 learning rate:1.0000e-05, Loss_tot:0.6648,\n",
      "Train Epoch:338 learning rate:1.0000e-05, Loss_tot:0.6645,\n",
      "Train Epoch:339 learning rate:1.0000e-05, Loss_tot:0.6642,\n",
      "save model\n",
      "Train Epoch:340 learning rate:1.0000e-05, Loss_tot:0.6639,\n",
      "save model\n",
      "Train Epoch:341 learning rate:1.0000e-05, Loss_tot:0.6620,\n",
      "save model\n",
      "Train Epoch:342 learning rate:1.0000e-05, Loss_tot:0.6633,\n",
      "Train Epoch:343 learning rate:1.0000e-05, Loss_tot:0.6629,\n",
      "Train Epoch:344 learning rate:1.0000e-05, Loss_tot:0.6629,\n",
      "Train Epoch:345 learning rate:1.0000e-05, Loss_tot:0.6626,\n",
      "Train Epoch:346 learning rate:1.0000e-05, Loss_tot:0.6623,\n",
      "Train Epoch:347 learning rate:1.0000e-05, Loss_tot:0.6620,\n",
      "save model\n",
      "Train Epoch:348 learning rate:1.0000e-05, Loss_tot:0.6601,\n",
      "save model\n",
      "Train Epoch:349 learning rate:1.0000e-05, Loss_tot:0.6614,\n",
      "Train Epoch:350 learning rate:1.0000e-05, Loss_tot:0.6611,\n",
      "Train Epoch:351 learning rate:1.0000e-05, Loss_tot:0.6608,\n",
      "Train Epoch:352 learning rate:1.0000e-05, Loss_tot:0.6605,\n",
      "Train Epoch:353 learning rate:1.0000e-05, Loss_tot:0.6603,\n",
      "Train Epoch:354 learning rate:1.0000e-05, Loss_tot:0.6600,\n",
      "save model\n",
      "Train Epoch:355 learning rate:1.0000e-05, Loss_tot:0.6597,\n",
      "save model\n",
      "Train Epoch:356 learning rate:1.0000e-05, Loss_tot:0.6578,\n",
      "save model\n",
      "Train Epoch:357 learning rate:1.0000e-05, Loss_tot:0.6591,\n",
      "Train Epoch:358 learning rate:1.0000e-05, Loss_tot:0.6588,\n",
      "Train Epoch:359 learning rate:1.0000e-05, Loss_tot:0.6585,\n",
      "Train Epoch:360 learning rate:1.0000e-05, Loss_tot:0.6582,\n",
      "Train Epoch:361 learning rate:1.0000e-05, Loss_tot:0.6579,\n",
      "Train Epoch:362 learning rate:1.0000e-05, Loss_tot:0.6576,\n",
      "save model\n",
      "Train Epoch:363 learning rate:1.0000e-05, Loss_tot:0.6558,\n",
      "save model\n",
      "Train Epoch:364 learning rate:1.0000e-05, Loss_tot:0.6555,\n",
      "save model\n",
      "Train Epoch:365 learning rate:1.0000e-05, Loss_tot:0.6567,\n",
      "Train Epoch:366 learning rate:1.0000e-05, Loss_tot:0.6565,\n",
      "Train Epoch:367 learning rate:1.0000e-05, Loss_tot:0.6562,\n",
      "Train Epoch:368 learning rate:1.0000e-05, Loss_tot:0.6559,\n",
      "Train Epoch:369 learning rate:1.0000e-05, Loss_tot:0.6559,\n",
      "Train Epoch:370 learning rate:1.0000e-05, Loss_tot:0.6575,\n",
      "Train Epoch:371 learning rate:1.0000e-05, Loss_tot:0.6570,\n",
      "Train Epoch:372 learning rate:1.0000e-05, Loss_tot:0.6564,\n",
      "Train Epoch:373 learning rate:1.0000e-05, Loss_tot:0.6559,\n",
      "Train Epoch:374 learning rate:1.0000e-05, Loss_tot:0.6555,\n",
      "Train Epoch:375 learning rate:1.0000e-05, Loss_tot:0.6551,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:376 learning rate:1.0000e-05, Loss_tot:0.6547,\n",
      "save model\n",
      "Train Epoch:377 learning rate:1.0000e-05, Loss_tot:0.6543,\n",
      "save model\n",
      "Train Epoch:378 learning rate:1.0000e-05, Loss_tot:0.6524,\n",
      "save model\n",
      "Train Epoch:379 learning rate:1.0000e-05, Loss_tot:0.6536,\n",
      "Train Epoch:380 learning rate:1.0000e-05, Loss_tot:0.6518,\n",
      "save model\n",
      "Train Epoch:381 learning rate:1.0000e-05, Loss_tot:0.6530,\n",
      "Train Epoch:382 learning rate:1.0000e-05, Loss_tot:0.6527,\n",
      "Train Epoch:383 learning rate:1.0000e-05, Loss_tot:0.6524,\n",
      "Train Epoch:384 learning rate:1.0000e-05, Loss_tot:0.6521,\n",
      "Train Epoch:385 learning rate:1.0000e-05, Loss_tot:0.6517,\n",
      "save model\n",
      "Train Epoch:386 learning rate:1.0000e-05, Loss_tot:0.6514,\n",
      "save model\n",
      "Train Epoch:387 learning rate:1.0000e-05, Loss_tot:0.6510,\n",
      "save model\n",
      "Train Epoch:388 learning rate:1.0000e-05, Loss_tot:0.6508,\n",
      "save model\n",
      "Train Epoch:389 learning rate:1.0000e-05, Loss_tot:0.6491,\n",
      "save model\n",
      "Train Epoch:390 learning rate:1.0000e-05, Loss_tot:0.6488,\n",
      "save model\n",
      "Train Epoch:391 learning rate:1.0000e-05, Loss_tot:0.6499,\n",
      "Train Epoch:392 learning rate:1.0000e-05, Loss_tot:0.6497,\n",
      "Train Epoch:393 learning rate:1.0000e-05, Loss_tot:0.6494,\n",
      "Train Epoch:394 learning rate:1.0000e-05, Loss_tot:0.6491,\n",
      "Train Epoch:395 learning rate:1.0000e-05, Loss_tot:0.6488,\n",
      "Train Epoch:396 learning rate:1.0000e-05, Loss_tot:0.6485,\n",
      "save model\n",
      "Train Epoch:397 learning rate:1.0000e-05, Loss_tot:0.6482,\n",
      "save model\n",
      "Train Epoch:398 learning rate:1.0000e-05, Loss_tot:0.6479,\n",
      "save model\n",
      "Train Epoch:399 learning rate:1.0000e-05, Loss_tot:0.6477,\n",
      "save model\n",
      "Train Epoch:400 learning rate:1.0000e-05, Loss_tot:0.6474,\n",
      "save model\n",
      "Train Epoch:401 learning rate:1.0000e-05, Loss_tot:0.6470,\n",
      "save model\n",
      "Train Epoch:402 learning rate:1.0000e-05, Loss_tot:0.6452,\n",
      "save model\n",
      "Train Epoch:403 learning rate:1.0000e-05, Loss_tot:0.6432,\n",
      "save model\n",
      "Train Epoch:404 learning rate:1.0000e-05, Loss_tot:0.6462,\n",
      "Train Epoch:405 learning rate:1.0000e-05, Loss_tot:0.6459,\n",
      "Train Epoch:406 learning rate:1.0000e-05, Loss_tot:0.6456,\n",
      "Train Epoch:407 learning rate:1.0000e-05, Loss_tot:0.6454,\n",
      "Train Epoch:408 learning rate:1.0000e-05, Loss_tot:0.6451,\n",
      "Train Epoch:409 learning rate:1.0000e-05, Loss_tot:0.6448,\n",
      "Train Epoch:410 learning rate:1.0000e-05, Loss_tot:0.6445,\n",
      "Train Epoch:411 learning rate:1.0000e-05, Loss_tot:0.6429,\n",
      "save model\n",
      "Train Epoch:412 learning rate:1.0000e-05, Loss_tot:0.6426,\n",
      "save model\n",
      "Train Epoch:413 learning rate:1.0000e-05, Loss_tot:0.6437,\n",
      "Train Epoch:414 learning rate:1.0000e-05, Loss_tot:0.6434,\n",
      "Train Epoch:415 learning rate:1.0000e-05, Loss_tot:0.6431,\n",
      "Train Epoch:416 learning rate:1.0000e-05, Loss_tot:0.6429,\n",
      "Train Epoch:417 learning rate:1.0000e-05, Loss_tot:0.6426,\n",
      "Train Epoch:418 learning rate:1.0000e-05, Loss_tot:0.6423,\n",
      "save model\n",
      "Train Epoch:419 learning rate:1.0000e-05, Loss_tot:0.6420,\n",
      "save model\n",
      "Train Epoch:420 learning rate:1.0000e-05, Loss_tot:0.6414,\n",
      "save model\n",
      "Train Epoch:421 learning rate:1.0000e-05, Loss_tot:0.6414,\n",
      "Train Epoch:422 learning rate:1.0000e-05, Loss_tot:0.6411,\n",
      "save model\n",
      "Train Epoch:423 learning rate:1.0000e-05, Loss_tot:0.6395,\n",
      "save model\n",
      "Train Epoch:424 learning rate:1.0000e-05, Loss_tot:0.6392,\n",
      "save model\n",
      "Train Epoch:425 learning rate:1.0000e-05, Loss_tot:0.6403,\n",
      "Train Epoch:426 learning rate:1.0000e-05, Loss_tot:0.6401,\n",
      "Train Epoch:427 learning rate:1.0000e-05, Loss_tot:0.6398,\n",
      "Train Epoch:428 learning rate:1.0000e-05, Loss_tot:0.6395,\n",
      "Train Epoch:429 learning rate:1.0000e-05, Loss_tot:0.6392,\n",
      "Train Epoch:430 learning rate:1.0000e-05, Loss_tot:0.6389,\n",
      "save model\n",
      "Train Epoch:431 learning rate:1.0000e-05, Loss_tot:0.6386,\n",
      "save model\n",
      "Train Epoch:432 learning rate:1.0000e-05, Loss_tot:0.6371,\n",
      "save model\n",
      "Train Epoch:433 learning rate:1.0000e-05, Loss_tot:0.6368,\n",
      "save model\n",
      "Train Epoch:434 learning rate:1.0000e-05, Loss_tot:0.6378,\n",
      "Train Epoch:435 learning rate:1.0000e-05, Loss_tot:0.6376,\n",
      "Train Epoch:436 learning rate:1.0000e-05, Loss_tot:0.6373,\n",
      "Train Epoch:437 learning rate:1.0000e-05, Loss_tot:0.6370,\n",
      "Train Epoch:438 learning rate:1.0000e-05, Loss_tot:0.6367,\n",
      "save model\n",
      "Train Epoch:439 learning rate:1.0000e-05, Loss_tot:0.6364,\n",
      "save model\n",
      "Train Epoch:440 learning rate:1.0000e-05, Loss_tot:0.6361,\n",
      "save model\n",
      "Train Epoch:441 learning rate:1.0000e-05, Loss_tot:0.6347,\n",
      "save model\n",
      "Train Epoch:442 learning rate:1.0000e-05, Loss_tot:0.6344,\n",
      "save model\n",
      "Train Epoch:443 learning rate:1.0000e-05, Loss_tot:0.6353,\n",
      "Train Epoch:444 learning rate:1.0000e-05, Loss_tot:0.6350,\n",
      "Train Epoch:445 learning rate:1.0000e-05, Loss_tot:0.6348,\n",
      "Train Epoch:446 learning rate:1.0000e-05, Loss_tot:0.6325,\n",
      "save model\n",
      "Train Epoch:447 learning rate:1.0000e-05, Loss_tot:0.6342,\n",
      "Train Epoch:448 learning rate:1.0000e-05, Loss_tot:0.6327,\n",
      "Train Epoch:449 learning rate:1.0000e-05, Loss_tot:0.6324,\n",
      "save model\n",
      "Train Epoch:450 learning rate:1.0000e-05, Loss_tot:0.6334,\n",
      "Train Epoch:451 learning rate:1.0000e-05, Loss_tot:0.6331,\n",
      "Train Epoch:452 learning rate:1.0000e-05, Loss_tot:0.6329,\n",
      "Train Epoch:453 learning rate:1.0000e-05, Loss_tot:0.6326,\n",
      "Train Epoch:454 learning rate:1.0000e-05, Loss_tot:0.6323,\n",
      "save model\n",
      "Train Epoch:455 learning rate:1.0000e-05, Loss_tot:0.6319,\n",
      "save model\n",
      "Train Epoch:456 learning rate:1.0000e-05, Loss_tot:0.6316,\n",
      "save model\n",
      "Train Epoch:457 learning rate:1.0000e-05, Loss_tot:0.6313,\n",
      "save model\n",
      "Train Epoch:458 learning rate:1.0000e-05, Loss_tot:0.6297,\n",
      "save model\n",
      "Train Epoch:459 learning rate:1.0000e-05, Loss_tot:0.6297,\n",
      "Train Epoch:460 learning rate:1.0000e-05, Loss_tot:0.6294,\n",
      "save model\n",
      "Train Epoch:461 learning rate:1.0000e-05, Loss_tot:0.6303,\n",
      "Train Epoch:462 learning rate:1.0000e-05, Loss_tot:0.6300,\n",
      "Train Epoch:463 learning rate:1.0000e-05, Loss_tot:0.6298,\n",
      "Train Epoch:464 learning rate:1.0000e-05, Loss_tot:0.6295,\n",
      "Train Epoch:465 learning rate:1.0000e-05, Loss_tot:0.6292,\n",
      "save model\n",
      "Train Epoch:466 learning rate:1.0000e-05, Loss_tot:0.6284,\n",
      "save model\n",
      "Train Epoch:467 learning rate:1.0000e-05, Loss_tot:0.6281,\n",
      "save model\n",
      "Train Epoch:468 learning rate:1.0000e-05, Loss_tot:0.6278,\n",
      "save model\n",
      "Train Epoch:469 learning rate:1.0000e-05, Loss_tot:0.6275,\n",
      "save model\n",
      "Train Epoch:470 learning rate:1.0000e-05, Loss_tot:0.6267,\n",
      "save model\n",
      "Train Epoch:471 learning rate:1.0000e-05, Loss_tot:0.6264,\n",
      "save model\n",
      "Train Epoch:472 learning rate:1.0000e-05, Loss_tot:0.6261,\n",
      "save model\n",
      "Train Epoch:473 learning rate:1.0000e-05, Loss_tot:0.6269,\n",
      "Train Epoch:474 learning rate:1.0000e-05, Loss_tot:0.6266,\n",
      "Train Epoch:475 learning rate:1.0000e-05, Loss_tot:0.6264,\n",
      "Train Epoch:476 learning rate:1.0000e-05, Loss_tot:0.6261,\n",
      "save model\n",
      "Train Epoch:477 learning rate:1.0000e-05, Loss_tot:0.6258,\n",
      "save model\n",
      "Train Epoch:478 learning rate:1.0000e-05, Loss_tot:0.6255,\n",
      "save model\n",
      "Train Epoch:479 learning rate:1.0000e-05, Loss_tot:0.6238,\n",
      "save model\n",
      "Train Epoch:480 learning rate:1.0000e-05, Loss_tot:0.6238,\n",
      "save model\n",
      "Train Epoch:481 learning rate:1.0000e-05, Loss_tot:0.6246,\n",
      "Train Epoch:482 learning rate:1.0000e-05, Loss_tot:0.6256,\n",
      "Train Epoch:483 learning rate:1.0000e-05, Loss_tot:0.6243,\n",
      "Train Epoch:484 learning rate:1.0000e-05, Loss_tot:0.6238,\n",
      "Train Epoch:485 learning rate:1.0000e-05, Loss_tot:0.6235,\n",
      "save model\n",
      "Train Epoch:486 learning rate:1.0000e-05, Loss_tot:0.6225,\n",
      "save model\n",
      "Train Epoch:487 learning rate:1.0000e-05, Loss_tot:0.6201,\n",
      "save model\n",
      "Train Epoch:488 learning rate:1.0000e-05, Loss_tot:0.6230,\n",
      "Train Epoch:489 learning rate:1.0000e-05, Loss_tot:0.6227,\n",
      "Train Epoch:490 learning rate:1.0000e-05, Loss_tot:0.6229,\n",
      "Train Epoch:491 learning rate:1.0000e-05, Loss_tot:0.6209,\n",
      "Train Epoch:492 learning rate:1.0000e-05, Loss_tot:0.6206,\n",
      "Train Epoch:493 learning rate:1.0000e-05, Loss_tot:0.6219,\n",
      "Train Epoch:494 learning rate:1.0000e-05, Loss_tot:0.6209,\n",
      "Train Epoch:495 learning rate:1.0000e-05, Loss_tot:0.6206,\n",
      "Train Epoch:496 learning rate:1.0000e-05, Loss_tot:0.6208,\n",
      "Train Epoch:497 learning rate:1.0000e-05, Loss_tot:0.6193,\n",
      "save model\n",
      "Train Epoch:498 learning rate:1.0000e-05, Loss_tot:0.6190,\n",
      "save model\n",
      "Train Epoch:499 learning rate:1.0000e-05, Loss_tot:0.6198,\n",
      "Train Epoch:500 learning rate:1.0000e-05, Loss_tot:0.6195,\n",
      "Train Epoch:501 learning rate:1.0000e-05, Loss_tot:0.6181,\n",
      "save model\n",
      "Train Epoch:502 learning rate:1.0000e-05, Loss_tot:0.6183,\n",
      "Train Epoch:503 learning rate:1.0000e-05, Loss_tot:0.6180,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:504 learning rate:1.0000e-05, Loss_tot:0.6163,\n",
      "save model\n",
      "Train Epoch:505 learning rate:1.0000e-05, Loss_tot:0.6169,\n",
      "Train Epoch:506 learning rate:1.0000e-05, Loss_tot:0.6178,\n",
      "Train Epoch:507 learning rate:1.0000e-05, Loss_tot:0.6176,\n",
      "Train Epoch:508 learning rate:1.0000e-05, Loss_tot:0.6173,\n",
      "Train Epoch:509 learning rate:1.0000e-05, Loss_tot:0.6170,\n",
      "Train Epoch:510 learning rate:1.0000e-05, Loss_tot:0.6166,\n",
      "Train Epoch:511 learning rate:1.0000e-05, Loss_tot:0.6152,\n",
      "save model\n",
      "Train Epoch:512 learning rate:1.0000e-05, Loss_tot:0.6149,\n",
      "save model\n",
      "Train Epoch:513 learning rate:1.0000e-05, Loss_tot:0.6158,\n",
      "Train Epoch:514 learning rate:1.0000e-05, Loss_tot:0.6155,\n",
      "Train Epoch:515 learning rate:1.0000e-05, Loss_tot:0.6152,\n",
      "Train Epoch:516 learning rate:1.0000e-05, Loss_tot:0.6149,\n",
      "save model\n",
      "Train Epoch:517 learning rate:1.0000e-05, Loss_tot:0.6136,\n",
      "save model\n",
      "Train Epoch:518 learning rate:1.0000e-05, Loss_tot:0.6124,\n",
      "save model\n",
      "Train Epoch:519 learning rate:1.0000e-05, Loss_tot:0.6121,\n",
      "save model\n",
      "Train Epoch:520 learning rate:1.0000e-05, Loss_tot:0.6138,\n",
      "Train Epoch:521 learning rate:1.0000e-05, Loss_tot:0.6136,\n",
      "Train Epoch:522 learning rate:1.0000e-05, Loss_tot:0.6133,\n",
      "Train Epoch:523 learning rate:1.0000e-05, Loss_tot:0.6130,\n",
      "Train Epoch:524 learning rate:1.0000e-05, Loss_tot:0.6104,\n",
      "save model\n",
      "Train Epoch:525 learning rate:1.0000e-05, Loss_tot:0.6124,\n",
      "Train Epoch:526 learning rate:1.0000e-05, Loss_tot:0.6110,\n",
      "Train Epoch:527 learning rate:1.0000e-05, Loss_tot:0.6106,\n",
      "Train Epoch:528 learning rate:1.0000e-05, Loss_tot:0.6108,\n",
      "Train Epoch:529 learning rate:1.0000e-05, Loss_tot:0.6104,\n",
      "Train Epoch:530 learning rate:1.0000e-05, Loss_tot:0.6100,\n",
      "save model\n",
      "Train Epoch:531 learning rate:1.0000e-05, Loss_tot:0.6092,\n",
      "save model\n",
      "Train Epoch:532 learning rate:1.0000e-05, Loss_tot:0.6089,\n",
      "save model\n",
      "Train Epoch:533 learning rate:1.0000e-05, Loss_tot:0.6095,\n",
      "Train Epoch:534 learning rate:1.0000e-05, Loss_tot:0.6092,\n",
      "Train Epoch:535 learning rate:1.0000e-05, Loss_tot:0.6079,\n",
      "save model\n",
      "Train Epoch:536 learning rate:1.0000e-05, Loss_tot:0.6079,\n",
      "save model\n",
      "Train Epoch:537 learning rate:1.0000e-05, Loss_tot:0.6076,\n",
      "save model\n",
      "Train Epoch:538 learning rate:1.0000e-05, Loss_tot:0.6070,\n",
      "save model\n",
      "Train Epoch:539 learning rate:1.0000e-05, Loss_tot:0.6067,\n",
      "save model\n",
      "Train Epoch:540 learning rate:1.0000e-05, Loss_tot:0.6074,\n",
      "Train Epoch:541 learning rate:1.0000e-05, Loss_tot:0.6071,\n",
      "Train Epoch:542 learning rate:1.0000e-05, Loss_tot:0.6068,\n",
      "Train Epoch:543 learning rate:1.0000e-05, Loss_tot:0.6065,\n",
      "save model\n",
      "Train Epoch:544 learning rate:1.0000e-05, Loss_tot:0.6042,\n",
      "save model\n",
      "Train Epoch:545 learning rate:1.0000e-05, Loss_tot:0.6039,\n",
      "save model\n",
      "Train Epoch:546 learning rate:1.0000e-05, Loss_tot:0.6046,\n",
      "Train Epoch:547 learning rate:1.0000e-05, Loss_tot:0.6053,\n",
      "Train Epoch:548 learning rate:1.0000e-05, Loss_tot:0.6051,\n",
      "Train Epoch:549 learning rate:1.0000e-05, Loss_tot:0.6048,\n",
      "Train Epoch:550 learning rate:1.0000e-05, Loss_tot:0.6045,\n",
      "Train Epoch:551 learning rate:1.0000e-05, Loss_tot:0.6019,\n",
      "save model\n",
      "Train Epoch:552 learning rate:1.0000e-05, Loss_tot:0.6038,\n",
      "Train Epoch:553 learning rate:1.0000e-05, Loss_tot:0.6026,\n",
      "Train Epoch:554 learning rate:1.0000e-05, Loss_tot:0.6023,\n",
      "Train Epoch:555 learning rate:1.0000e-05, Loss_tot:0.6021,\n",
      "Train Epoch:556 learning rate:1.0000e-05, Loss_tot:0.6007,\n",
      "save model\n",
      "Train Epoch:557 learning rate:1.0000e-05, Loss_tot:0.6004,\n",
      "save model\n",
      "Train Epoch:558 learning rate:1.0000e-05, Loss_tot:0.6001,\n",
      "save model\n",
      "Train Epoch:559 learning rate:1.0000e-05, Loss_tot:0.6008,\n",
      "Train Epoch:560 learning rate:1.0000e-05, Loss_tot:0.6005,\n",
      "Train Epoch:561 learning rate:1.0000e-05, Loss_tot:0.6014,\n",
      "Train Epoch:562 learning rate:1.0000e-05, Loss_tot:0.6008,\n",
      "Train Epoch:563 learning rate:1.0000e-05, Loss_tot:0.6003,\n",
      "Train Epoch:564 learning rate:1.0000e-05, Loss_tot:0.5999,\n",
      "save model\n",
      "Train Epoch:565 learning rate:1.0000e-05, Loss_tot:0.5995,\n",
      "save model\n",
      "Train Epoch:566 learning rate:1.0000e-05, Loss_tot:0.5991,\n",
      "save model\n",
      "Train Epoch:567 learning rate:1.0000e-05, Loss_tot:0.5989,\n",
      "save model\n",
      "Train Epoch:568 learning rate:1.0000e-05, Loss_tot:0.5986,\n",
      "save model\n",
      "Train Epoch:569 learning rate:1.0000e-05, Loss_tot:0.5983,\n",
      "save model\n",
      "Train Epoch:570 learning rate:1.0000e-05, Loss_tot:0.5979,\n",
      "save model\n",
      "Train Epoch:571 learning rate:1.0000e-05, Loss_tot:0.5976,\n",
      "save model\n",
      "Train Epoch:572 learning rate:1.0000e-05, Loss_tot:0.5961,\n",
      "save model\n",
      "Train Epoch:573 learning rate:1.0000e-05, Loss_tot:0.5957,\n",
      "save model\n",
      "Train Epoch:574 learning rate:1.0000e-05, Loss_tot:0.5954,\n",
      "save model\n",
      "Train Epoch:575 learning rate:1.0000e-05, Loss_tot:0.5963,\n",
      "Train Epoch:576 learning rate:1.0000e-05, Loss_tot:0.5938,\n",
      "save model\n",
      "Train Epoch:577 learning rate:1.0000e-05, Loss_tot:0.5957,\n",
      "Train Epoch:578 learning rate:1.0000e-05, Loss_tot:0.5953,\n",
      "Train Epoch:579 learning rate:1.0000e-05, Loss_tot:0.5950,\n",
      "Train Epoch:580 learning rate:1.0000e-05, Loss_tot:0.5947,\n",
      "Train Epoch:581 learning rate:1.0000e-05, Loss_tot:0.5944,\n",
      "Train Epoch:582 learning rate:1.0000e-05, Loss_tot:0.5941,\n",
      "Train Epoch:583 learning rate:1.0000e-05, Loss_tot:0.5938,\n",
      "save model\n",
      "Train Epoch:584 learning rate:1.0000e-05, Loss_tot:0.5934,\n",
      "save model\n",
      "Train Epoch:585 learning rate:1.0000e-05, Loss_tot:0.5931,\n",
      "save model\n",
      "Train Epoch:586 learning rate:1.0000e-05, Loss_tot:0.5916,\n",
      "save model\n",
      "Train Epoch:587 learning rate:1.0000e-05, Loss_tot:0.5913,\n",
      "save model\n",
      "Train Epoch:588 learning rate:1.0000e-05, Loss_tot:0.5921,\n",
      "Train Epoch:589 learning rate:1.0000e-05, Loss_tot:0.5918,\n",
      "Train Epoch:590 learning rate:1.0000e-05, Loss_tot:0.5915,\n",
      "Train Epoch:591 learning rate:1.0000e-05, Loss_tot:0.5912,\n",
      "save model\n",
      "Train Epoch:592 learning rate:1.0000e-05, Loss_tot:0.5896,\n",
      "save model\n",
      "Train Epoch:593 learning rate:1.0000e-05, Loss_tot:0.5905,\n",
      "Train Epoch:594 learning rate:1.0000e-05, Loss_tot:0.5902,\n",
      "Train Epoch:595 learning rate:1.0000e-05, Loss_tot:0.5902,\n",
      "Train Epoch:596 learning rate:1.0000e-05, Loss_tot:0.5899,\n",
      "Train Epoch:597 learning rate:1.0000e-05, Loss_tot:0.5883,\n",
      "save model\n",
      "Train Epoch:598 learning rate:1.0000e-05, Loss_tot:0.5893,\n",
      "Train Epoch:599 learning rate:1.0000e-05, Loss_tot:0.5890,\n",
      "Train Epoch:600 learning rate:1.0000e-05, Loss_tot:0.5875,\n",
      "save model\n",
      "Train Epoch:601 learning rate:1.0000e-05, Loss_tot:0.5883,\n",
      "Train Epoch:602 learning rate:1.0000e-05, Loss_tot:0.5879,\n",
      "Train Epoch:603 learning rate:1.0000e-05, Loss_tot:0.5876,\n",
      "Train Epoch:604 learning rate:1.0000e-05, Loss_tot:0.5849,\n",
      "save model\n",
      "Train Epoch:605 learning rate:1.0000e-05, Loss_tot:0.5858,\n",
      "Train Epoch:606 learning rate:1.0000e-05, Loss_tot:0.5855,\n",
      "Train Epoch:607 learning rate:1.0000e-05, Loss_tot:0.5863,\n",
      "Train Epoch:608 learning rate:1.0000e-05, Loss_tot:0.5860,\n",
      "Train Epoch:609 learning rate:1.0000e-05, Loss_tot:0.5857,\n",
      "Train Epoch:610 learning rate:1.0000e-05, Loss_tot:0.5854,\n",
      "Train Epoch:611 learning rate:1.0000e-05, Loss_tot:0.5851,\n",
      "Train Epoch:612 learning rate:1.0000e-05, Loss_tot:0.5847,\n",
      "save model\n",
      "Train Epoch:613 learning rate:1.0000e-05, Loss_tot:0.5846,\n",
      "save model\n",
      "Train Epoch:614 learning rate:1.0000e-05, Loss_tot:0.5841,\n",
      "save model\n",
      "Train Epoch:615 learning rate:1.0000e-05, Loss_tot:0.5827,\n",
      "save model\n",
      "Train Epoch:616 learning rate:1.0000e-05, Loss_tot:0.5823,\n",
      "save model\n",
      "Train Epoch:617 learning rate:1.0000e-05, Loss_tot:0.5820,\n",
      "save model\n",
      "Train Epoch:618 learning rate:1.0000e-05, Loss_tot:0.5829,\n",
      "Train Epoch:619 learning rate:1.0000e-05, Loss_tot:0.5826,\n",
      "Train Epoch:620 learning rate:1.0000e-05, Loss_tot:0.5822,\n",
      "Train Epoch:621 learning rate:1.0000e-05, Loss_tot:0.5819,\n",
      "save model\n",
      "Train Epoch:622 learning rate:1.0000e-05, Loss_tot:0.5816,\n",
      "save model\n",
      "Train Epoch:623 learning rate:1.0000e-05, Loss_tot:0.5813,\n",
      "save model\n",
      "Train Epoch:624 learning rate:1.0000e-05, Loss_tot:0.5810,\n",
      "save model\n",
      "Train Epoch:625 learning rate:1.0000e-05, Loss_tot:0.5807,\n",
      "save model\n",
      "Train Epoch:626 learning rate:1.0000e-05, Loss_tot:0.5803,\n",
      "save model\n",
      "Train Epoch:627 learning rate:1.0000e-05, Loss_tot:0.5802,\n",
      "save model\n",
      "Train Epoch:628 learning rate:1.0000e-05, Loss_tot:0.5786,\n",
      "save model\n",
      "Train Epoch:629 learning rate:1.0000e-05, Loss_tot:0.5783,\n",
      "save model\n",
      "Train Epoch:630 learning rate:1.0000e-05, Loss_tot:0.5779,\n",
      "save model\n",
      "Train Epoch:631 learning rate:1.0000e-05, Loss_tot:0.5788,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:632 learning rate:1.0000e-05, Loss_tot:0.5785,\n",
      "Train Epoch:633 learning rate:1.0000e-05, Loss_tot:0.5781,\n",
      "Train Epoch:634 learning rate:1.0000e-05, Loss_tot:0.5778,\n",
      "save model\n",
      "Train Epoch:635 learning rate:1.0000e-05, Loss_tot:0.5775,\n",
      "save model\n",
      "Train Epoch:636 learning rate:1.0000e-05, Loss_tot:0.5772,\n",
      "save model\n",
      "Train Epoch:637 learning rate:1.0000e-05, Loss_tot:0.5769,\n",
      "save model\n",
      "Train Epoch:638 learning rate:1.0000e-05, Loss_tot:0.5733,\n",
      "save model\n",
      "Train Epoch:639 learning rate:1.0000e-05, Loss_tot:0.5751,\n",
      "Train Epoch:640 learning rate:1.0000e-05, Loss_tot:0.5759,\n",
      "Train Epoch:641 learning rate:1.0000e-05, Loss_tot:0.5756,\n",
      "Train Epoch:642 learning rate:1.0000e-05, Loss_tot:0.5753,\n",
      "Train Epoch:643 learning rate:1.0000e-05, Loss_tot:0.5738,\n",
      "Train Epoch:644 learning rate:1.0000e-05, Loss_tot:0.5746,\n",
      "Train Epoch:645 learning rate:1.0000e-05, Loss_tot:0.5743,\n",
      "Train Epoch:646 learning rate:1.0000e-05, Loss_tot:0.5738,\n",
      "Train Epoch:647 learning rate:1.0000e-05, Loss_tot:0.5737,\n",
      "Train Epoch:648 learning rate:1.0000e-05, Loss_tot:0.5722,\n",
      "save model\n",
      "Train Epoch:649 learning rate:1.0000e-05, Loss_tot:0.5719,\n",
      "save model\n",
      "Train Epoch:650 learning rate:1.0000e-05, Loss_tot:0.5728,\n",
      "Train Epoch:651 learning rate:1.0000e-05, Loss_tot:0.5724,\n",
      "Train Epoch:652 learning rate:1.0000e-05, Loss_tot:0.5721,\n",
      "Train Epoch:653 learning rate:1.0000e-05, Loss_tot:0.5718,\n",
      "save model\n",
      "Train Epoch:654 learning rate:1.0000e-05, Loss_tot:0.5715,\n",
      "save model\n",
      "Train Epoch:655 learning rate:1.0000e-05, Loss_tot:0.5700,\n",
      "save model\n",
      "Train Epoch:656 learning rate:1.0000e-05, Loss_tot:0.5697,\n",
      "save model\n",
      "Train Epoch:657 learning rate:1.0000e-05, Loss_tot:0.5703,\n",
      "Train Epoch:658 learning rate:1.0000e-05, Loss_tot:0.5699,\n",
      "Train Epoch:659 learning rate:1.0000e-05, Loss_tot:0.5699,\n",
      "Train Epoch:660 learning rate:1.0000e-05, Loss_tot:0.5724,\n",
      "Train Epoch:661 learning rate:1.0000e-05, Loss_tot:0.5717,\n",
      "Train Epoch:662 learning rate:1.0000e-05, Loss_tot:0.5700,\n",
      "Train Epoch:663 learning rate:1.0000e-05, Loss_tot:0.5696,\n",
      "save model\n",
      "Train Epoch:664 learning rate:1.0000e-05, Loss_tot:0.5688,\n",
      "save model\n",
      "Train Epoch:665 learning rate:1.0000e-05, Loss_tot:0.5699,\n",
      "Train Epoch:666 learning rate:1.0000e-05, Loss_tot:0.5696,\n",
      "Train Epoch:667 learning rate:1.0000e-05, Loss_tot:0.5693,\n",
      "Train Epoch:668 learning rate:1.0000e-05, Loss_tot:0.5687,\n",
      "save model\n",
      "Train Epoch:669 learning rate:1.0000e-05, Loss_tot:0.5682,\n",
      "save model\n",
      "Train Epoch:670 learning rate:1.0000e-05, Loss_tot:0.5683,\n",
      "Train Epoch:671 learning rate:1.0000e-05, Loss_tot:0.5680,\n",
      "save model\n",
      "Train Epoch:672 learning rate:1.0000e-05, Loss_tot:0.5654,\n",
      "save model\n",
      "Train Epoch:673 learning rate:1.0000e-05, Loss_tot:0.5659,\n",
      "Train Epoch:674 learning rate:1.0000e-05, Loss_tot:0.5661,\n",
      "Train Epoch:675 learning rate:1.0000e-05, Loss_tot:0.5658,\n",
      "Train Epoch:676 learning rate:1.0000e-05, Loss_tot:0.5664,\n",
      "Train Epoch:677 learning rate:1.0000e-05, Loss_tot:0.5661,\n",
      "Train Epoch:678 learning rate:1.0000e-05, Loss_tot:0.5653,\n",
      "save model\n",
      "Train Epoch:679 learning rate:1.0000e-05, Loss_tot:0.5655,\n",
      "Train Epoch:680 learning rate:1.0000e-05, Loss_tot:0.5651,\n",
      "save model\n",
      "Train Epoch:681 learning rate:1.0000e-05, Loss_tot:0.5648,\n",
      "save model\n",
      "Train Epoch:682 learning rate:1.0000e-05, Loss_tot:0.5631,\n",
      "save model\n",
      "Train Epoch:683 learning rate:1.0000e-05, Loss_tot:0.5632,\n",
      "Train Epoch:684 learning rate:1.0000e-05, Loss_tot:0.5629,\n",
      "save model\n",
      "Train Epoch:685 learning rate:1.0000e-05, Loss_tot:0.5635,\n",
      "Train Epoch:686 learning rate:1.0000e-05, Loss_tot:0.5627,\n",
      "save model\n",
      "Train Epoch:687 learning rate:1.0000e-05, Loss_tot:0.5629,\n",
      "Train Epoch:688 learning rate:1.0000e-05, Loss_tot:0.5626,\n",
      "save model\n",
      "Train Epoch:689 learning rate:1.0000e-05, Loss_tot:0.5622,\n",
      "save model\n",
      "Train Epoch:690 learning rate:1.0000e-05, Loss_tot:0.5614,\n",
      "save model\n",
      "Train Epoch:691 learning rate:1.0000e-05, Loss_tot:0.5616,\n",
      "Train Epoch:692 learning rate:1.0000e-05, Loss_tot:0.5603,\n",
      "save model\n",
      "Train Epoch:693 learning rate:1.0000e-05, Loss_tot:0.5597,\n",
      "save model\n",
      "Train Epoch:694 learning rate:1.0000e-05, Loss_tot:0.5592,\n",
      "save model\n",
      "Train Epoch:695 learning rate:1.0000e-05, Loss_tot:0.5603,\n",
      "Train Epoch:696 learning rate:1.0000e-05, Loss_tot:0.5618,\n",
      "Train Epoch:697 learning rate:1.0000e-05, Loss_tot:0.5614,\n",
      "Train Epoch:698 learning rate:1.0000e-05, Loss_tot:0.5610,\n",
      "Train Epoch:699 learning rate:1.0000e-05, Loss_tot:0.5602,\n",
      "Train Epoch:700 learning rate:1.0000e-05, Loss_tot:0.5598,\n",
      "Train Epoch:701 learning rate:1.0000e-05, Loss_tot:0.5589,\n",
      "save model\n",
      "Train Epoch:702 learning rate:1.0000e-05, Loss_tot:0.5584,\n",
      "save model\n",
      "Train Epoch:703 learning rate:1.0000e-05, Loss_tot:0.5580,\n",
      "save model\n",
      "Train Epoch:704 learning rate:1.0000e-05, Loss_tot:0.5586,\n",
      "Train Epoch:705 learning rate:1.0000e-05, Loss_tot:0.5582,\n",
      "Train Epoch:706 learning rate:1.0000e-05, Loss_tot:0.5579,\n",
      "save model\n",
      "Train Epoch:707 learning rate:1.0000e-05, Loss_tot:0.5575,\n",
      "save model\n",
      "Train Epoch:708 learning rate:1.0000e-05, Loss_tot:0.5571,\n",
      "save model\n",
      "Train Epoch:709 learning rate:1.0000e-05, Loss_tot:0.5563,\n",
      "save model\n",
      "Train Epoch:710 learning rate:1.0000e-05, Loss_tot:0.5528,\n",
      "save model\n",
      "Train Epoch:711 learning rate:1.0000e-05, Loss_tot:0.5546,\n",
      "Train Epoch:712 learning rate:1.0000e-05, Loss_tot:0.5557,\n",
      "Train Epoch:713 learning rate:1.0000e-05, Loss_tot:0.5561,\n",
      "Train Epoch:714 learning rate:1.0000e-05, Loss_tot:0.5549,\n",
      "Train Epoch:715 learning rate:1.0000e-05, Loss_tot:0.5554,\n",
      "Train Epoch:716 learning rate:1.0000e-05, Loss_tot:0.5539,\n",
      "Train Epoch:717 learning rate:1.0000e-05, Loss_tot:0.5547,\n",
      "Train Epoch:718 learning rate:1.0000e-05, Loss_tot:0.5543,\n",
      "Train Epoch:719 learning rate:1.0000e-05, Loss_tot:0.5533,\n",
      "Train Epoch:720 learning rate:1.0000e-05, Loss_tot:0.5520,\n",
      "save model\n",
      "Train Epoch:721 learning rate:1.0000e-05, Loss_tot:0.5517,\n",
      "save model\n",
      "Train Epoch:722 learning rate:1.0000e-05, Loss_tot:0.5523,\n",
      "Train Epoch:723 learning rate:1.0000e-05, Loss_tot:0.5520,\n",
      "Train Epoch:724 learning rate:1.0000e-05, Loss_tot:0.5512,\n",
      "save model\n",
      "Train Epoch:725 learning rate:1.0000e-05, Loss_tot:0.5508,\n",
      "save model\n",
      "Train Epoch:726 learning rate:1.0000e-05, Loss_tot:0.5495,\n",
      "save model\n",
      "Train Epoch:727 learning rate:1.0000e-05, Loss_tot:0.5491,\n",
      "save model\n",
      "Train Epoch:728 learning rate:1.0000e-05, Loss_tot:0.5503,\n",
      "Train Epoch:729 learning rate:1.0000e-05, Loss_tot:0.5500,\n",
      "Train Epoch:730 learning rate:1.0000e-05, Loss_tot:0.5496,\n",
      "Train Epoch:731 learning rate:1.0000e-05, Loss_tot:0.5493,\n",
      "Train Epoch:732 learning rate:1.0000e-05, Loss_tot:0.5480,\n",
      "save model\n",
      "Train Epoch:733 learning rate:1.0000e-05, Loss_tot:0.5476,\n",
      "save model\n",
      "Train Epoch:734 learning rate:1.0000e-05, Loss_tot:0.5470,\n",
      "save model\n",
      "Train Epoch:735 learning rate:1.0000e-05, Loss_tot:0.5480,\n",
      "Train Epoch:736 learning rate:1.0000e-05, Loss_tot:0.5476,\n",
      "Train Epoch:737 learning rate:1.0000e-05, Loss_tot:0.5473,\n",
      "Train Epoch:738 learning rate:1.0000e-05, Loss_tot:0.5469,\n",
      "save model\n",
      "Train Epoch:739 learning rate:1.0000e-05, Loss_tot:0.5466,\n",
      "save model\n",
      "Train Epoch:740 learning rate:1.0000e-05, Loss_tot:0.5458,\n",
      "save model\n",
      "Train Epoch:741 learning rate:1.0000e-05, Loss_tot:0.5444,\n",
      "save model\n",
      "Train Epoch:742 learning rate:1.0000e-05, Loss_tot:0.5441,\n",
      "save model\n",
      "Train Epoch:743 learning rate:1.0000e-05, Loss_tot:0.5442,\n",
      "Train Epoch:744 learning rate:1.0000e-05, Loss_tot:0.5449,\n",
      "Train Epoch:745 learning rate:1.0000e-05, Loss_tot:0.5425,\n",
      "save model\n",
      "Train Epoch:746 learning rate:1.0000e-05, Loss_tot:0.5442,\n",
      "Train Epoch:747 learning rate:1.0000e-05, Loss_tot:0.5439,\n",
      "Train Epoch:748 learning rate:1.0000e-05, Loss_tot:0.5436,\n",
      "Train Epoch:749 learning rate:1.0000e-05, Loss_tot:0.5417,\n",
      "save model\n",
      "Train Epoch:750 learning rate:1.0000e-05, Loss_tot:0.5414,\n",
      "save model\n",
      "Train Epoch:751 learning rate:1.0000e-05, Loss_tot:0.5408,\n",
      "save model\n",
      "Train Epoch:752 learning rate:1.0000e-05, Loss_tot:0.5422,\n",
      "Train Epoch:753 learning rate:1.0000e-05, Loss_tot:0.5419,\n",
      "Train Epoch:754 learning rate:1.0000e-05, Loss_tot:0.5416,\n",
      "Train Epoch:755 learning rate:1.0000e-05, Loss_tot:0.5412,\n",
      "Train Epoch:756 learning rate:1.0000e-05, Loss_tot:0.5409,\n",
      "Train Epoch:757 learning rate:1.0000e-05, Loss_tot:0.5395,\n",
      "save model\n",
      "Train Epoch:758 learning rate:1.0000e-05, Loss_tot:0.5392,\n",
      "save model\n",
      "Train Epoch:759 learning rate:1.0000e-05, Loss_tot:0.5399,\n",
      "Train Epoch:760 learning rate:1.0000e-05, Loss_tot:0.5390,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:761 learning rate:1.0000e-05, Loss_tot:0.5377,\n",
      "save model\n",
      "Train Epoch:762 learning rate:1.0000e-05, Loss_tot:0.5389,\n",
      "Train Epoch:763 learning rate:1.0000e-05, Loss_tot:0.5385,\n",
      "Train Epoch:764 learning rate:1.0000e-05, Loss_tot:0.5372,\n",
      "save model\n",
      "Train Epoch:765 learning rate:1.0000e-05, Loss_tot:0.5363,\n",
      "save model\n",
      "Train Epoch:766 learning rate:1.0000e-05, Loss_tot:0.5367,\n",
      "Train Epoch:767 learning rate:1.0000e-05, Loss_tot:0.5372,\n",
      "Train Epoch:768 learning rate:1.0000e-05, Loss_tot:0.5368,\n",
      "Train Epoch:769 learning rate:1.0000e-05, Loss_tot:0.5365,\n",
      "Train Epoch:770 learning rate:1.0000e-05, Loss_tot:0.5351,\n",
      "save model\n",
      "Train Epoch:771 learning rate:1.0000e-05, Loss_tot:0.5348,\n",
      "save model\n",
      "Train Epoch:772 learning rate:1.0000e-05, Loss_tot:0.5355,\n",
      "Train Epoch:773 learning rate:1.0000e-05, Loss_tot:0.5346,\n",
      "save model\n",
      "Train Epoch:774 learning rate:1.0000e-05, Loss_tot:0.5343,\n",
      "save model\n",
      "Train Epoch:775 learning rate:1.0000e-05, Loss_tot:0.5329,\n",
      "save model\n",
      "Train Epoch:776 learning rate:1.0000e-05, Loss_tot:0.5331,\n",
      "Train Epoch:777 learning rate:1.0000e-05, Loss_tot:0.5338,\n",
      "Train Epoch:778 learning rate:1.0000e-05, Loss_tot:0.5334,\n",
      "Train Epoch:779 learning rate:1.0000e-05, Loss_tot:0.5328,\n",
      "save model\n",
      "Train Epoch:780 learning rate:1.0000e-05, Loss_tot:0.5317,\n",
      "save model\n",
      "Train Epoch:781 learning rate:1.0000e-05, Loss_tot:0.5313,\n",
      "save model\n",
      "Train Epoch:782 learning rate:1.0000e-05, Loss_tot:0.5299,\n",
      "save model\n",
      "Train Epoch:783 learning rate:1.0000e-05, Loss_tot:0.5312,\n",
      "Train Epoch:784 learning rate:1.0000e-05, Loss_tot:0.5308,\n",
      "Train Epoch:785 learning rate:1.0000e-05, Loss_tot:0.5300,\n",
      "Train Epoch:786 learning rate:1.0000e-05, Loss_tot:0.5296,\n",
      "save model\n",
      "Train Epoch:787 learning rate:1.0000e-05, Loss_tot:0.5303,\n",
      "Train Epoch:788 learning rate:1.0000e-05, Loss_tot:0.5294,\n",
      "save model\n",
      "Train Epoch:789 learning rate:1.0000e-05, Loss_tot:0.5291,\n",
      "save model\n",
      "Train Epoch:790 learning rate:1.0000e-05, Loss_tot:0.5293,\n",
      "Train Epoch:791 learning rate:1.0000e-05, Loss_tot:0.5279,\n",
      "save model\n",
      "Train Epoch:792 learning rate:1.0000e-05, Loss_tot:0.5273,\n",
      "save model\n",
      "Train Epoch:793 learning rate:1.0000e-05, Loss_tot:0.5272,\n",
      "save model\n",
      "Train Epoch:794 learning rate:1.0000e-05, Loss_tot:0.5279,\n",
      "Train Epoch:795 learning rate:1.0000e-05, Loss_tot:0.5270,\n",
      "save model\n",
      "Train Epoch:796 learning rate:1.0000e-05, Loss_tot:0.5266,\n",
      "save model\n",
      "Train Epoch:797 learning rate:1.0000e-05, Loss_tot:0.5268,\n",
      "Train Epoch:798 learning rate:1.0000e-05, Loss_tot:0.5254,\n",
      "save model\n",
      "Train Epoch:799 learning rate:1.0000e-05, Loss_tot:0.5251,\n",
      "save model\n",
      "Train Epoch:800 learning rate:1.0000e-05, Loss_tot:0.5258,\n",
      "Train Epoch:801 learning rate:1.0000e-05, Loss_tot:0.5249,\n",
      "save model\n",
      "Train Epoch:802 learning rate:1.0000e-05, Loss_tot:0.5235,\n",
      "save model\n",
      "Train Epoch:803 learning rate:1.0000e-05, Loss_tot:0.5231,\n",
      "save model\n",
      "Train Epoch:804 learning rate:1.0000e-05, Loss_tot:0.5241,\n",
      "Train Epoch:805 learning rate:1.0000e-05, Loss_tot:0.5240,\n",
      "Train Epoch:806 learning rate:1.0000e-05, Loss_tot:0.5237,\n",
      "Train Epoch:807 learning rate:1.0000e-05, Loss_tot:0.5233,\n",
      "Train Epoch:808 learning rate:1.0000e-05, Loss_tot:0.5219,\n",
      "save model\n",
      "Train Epoch:809 learning rate:1.0000e-05, Loss_tot:0.5215,\n",
      "save model\n",
      "Train Epoch:810 learning rate:1.0000e-05, Loss_tot:0.5212,\n",
      "save model\n",
      "Train Epoch:811 learning rate:1.0000e-05, Loss_tot:0.5213,\n",
      "Train Epoch:812 learning rate:1.0000e-05, Loss_tot:0.5210,\n",
      "save model\n",
      "Train Epoch:813 learning rate:1.0000e-05, Loss_tot:0.5206,\n",
      "save model\n",
      "Train Epoch:814 learning rate:1.0000e-05, Loss_tot:0.5208,\n",
      "Train Epoch:815 learning rate:1.0000e-05, Loss_tot:0.5194,\n",
      "save model\n",
      "Train Epoch:816 learning rate:1.0000e-05, Loss_tot:0.5190,\n",
      "save model\n",
      "Train Epoch:817 learning rate:1.0000e-05, Loss_tot:0.5184,\n",
      "save model\n",
      "Train Epoch:818 learning rate:1.0000e-05, Loss_tot:0.5173,\n",
      "save model\n",
      "Train Epoch:819 learning rate:1.0000e-05, Loss_tot:0.5185,\n",
      "Train Epoch:820 learning rate:1.0000e-05, Loss_tot:0.5181,\n",
      "Train Epoch:821 learning rate:1.0000e-05, Loss_tot:0.5177,\n",
      "Train Epoch:822 learning rate:1.0000e-05, Loss_tot:0.5169,\n",
      "save model\n",
      "Train Epoch:823 learning rate:1.0000e-05, Loss_tot:0.5165,\n",
      "save model\n",
      "Train Epoch:824 learning rate:1.0000e-05, Loss_tot:0.5162,\n",
      "save model\n",
      "Train Epoch:825 learning rate:1.0000e-05, Loss_tot:0.5169,\n",
      "Train Epoch:826 learning rate:1.0000e-05, Loss_tot:0.5165,\n",
      "Train Epoch:827 learning rate:1.0000e-05, Loss_tot:0.5153,\n",
      "save model\n",
      "Train Epoch:828 learning rate:1.0000e-05, Loss_tot:0.5152,\n",
      "save model\n",
      "Train Epoch:829 learning rate:1.0000e-05, Loss_tot:0.5138,\n",
      "save model\n",
      "Train Epoch:830 learning rate:1.0000e-05, Loss_tot:0.5140,\n",
      "Train Epoch:831 learning rate:1.0000e-05, Loss_tot:0.5136,\n",
      "save model\n",
      "Train Epoch:832 learning rate:1.0000e-05, Loss_tot:0.5143,\n",
      "Train Epoch:833 learning rate:1.0000e-05, Loss_tot:0.5140,\n",
      "Train Epoch:834 learning rate:1.0000e-05, Loss_tot:0.5136,\n",
      "save model\n",
      "Train Epoch:835 learning rate:1.0000e-05, Loss_tot:0.5121,\n",
      "save model\n",
      "Train Epoch:836 learning rate:1.0000e-05, Loss_tot:0.5112,\n",
      "save model\n",
      "Train Epoch:837 learning rate:1.0000e-05, Loss_tot:0.5125,\n",
      "Train Epoch:838 learning rate:1.0000e-05, Loss_tot:0.5105,\n",
      "save model\n",
      "Train Epoch:839 learning rate:1.0000e-05, Loss_tot:0.5112,\n",
      "Train Epoch:840 learning rate:1.0000e-05, Loss_tot:0.5111,\n",
      "Train Epoch:841 learning rate:1.0000e-05, Loss_tot:0.5099,\n",
      "save model\n",
      "Train Epoch:842 learning rate:1.0000e-05, Loss_tot:0.5112,\n",
      "Train Epoch:843 learning rate:1.0000e-05, Loss_tot:0.5099,\n",
      "save model\n",
      "Train Epoch:844 learning rate:1.0000e-05, Loss_tot:0.5088,\n",
      "save model\n",
      "Train Epoch:845 learning rate:1.0000e-05, Loss_tot:0.5095,\n",
      "Train Epoch:846 learning rate:1.0000e-05, Loss_tot:0.5086,\n",
      "save model\n",
      "Train Epoch:847 learning rate:1.0000e-05, Loss_tot:0.5071,\n",
      "save model\n",
      "Train Epoch:848 learning rate:1.0000e-05, Loss_tot:0.5078,\n",
      "Train Epoch:849 learning rate:1.0000e-05, Loss_tot:0.5064,\n",
      "save model\n",
      "Train Epoch:850 learning rate:1.0000e-05, Loss_tot:0.5074,\n",
      "Train Epoch:851 learning rate:1.0000e-05, Loss_tot:0.5062,\n",
      "save model\n",
      "Train Epoch:852 learning rate:1.0000e-05, Loss_tot:0.5038,\n",
      "save model\n",
      "Train Epoch:853 learning rate:1.0000e-05, Loss_tot:0.5065,\n",
      "Train Epoch:854 learning rate:1.0000e-05, Loss_tot:0.5061,\n",
      "Train Epoch:855 learning rate:1.0000e-05, Loss_tot:0.5052,\n",
      "Train Epoch:856 learning rate:1.0000e-05, Loss_tot:0.5037,\n",
      "save model\n",
      "Train Epoch:857 learning rate:1.0000e-05, Loss_tot:0.5033,\n",
      "save model\n",
      "Train Epoch:858 learning rate:1.0000e-05, Loss_tot:0.5035,\n",
      "Train Epoch:859 learning rate:1.0000e-05, Loss_tot:0.5042,\n",
      "Train Epoch:860 learning rate:1.0000e-05, Loss_tot:0.5038,\n",
      "Train Epoch:861 learning rate:1.0000e-05, Loss_tot:0.5027,\n",
      "save model\n",
      "Train Epoch:862 learning rate:1.0000e-05, Loss_tot:0.5009,\n",
      "save model\n",
      "Train Epoch:863 learning rate:1.0000e-05, Loss_tot:0.5004,\n",
      "save model\n",
      "Train Epoch:864 learning rate:1.0000e-05, Loss_tot:0.5007,\n",
      "Train Epoch:865 learning rate:1.0000e-05, Loss_tot:0.5020,\n",
      "Train Epoch:866 learning rate:1.0000e-05, Loss_tot:0.5016,\n",
      "Train Epoch:867 learning rate:1.0000e-05, Loss_tot:0.5001,\n",
      "save model\n",
      "Train Epoch:868 learning rate:1.0000e-05, Loss_tot:0.4998,\n",
      "save model\n",
      "Train Epoch:869 learning rate:1.0000e-05, Loss_tot:0.5005,\n",
      "Train Epoch:870 learning rate:1.0000e-05, Loss_tot:0.4995,\n",
      "save model\n",
      "Train Epoch:871 learning rate:1.0000e-05, Loss_tot:0.4991,\n",
      "save model\n",
      "Train Epoch:872 learning rate:1.0000e-05, Loss_tot:0.4974,\n",
      "save model\n",
      "Train Epoch:873 learning rate:1.0000e-05, Loss_tot:0.4973,\n",
      "save model\n",
      "Train Epoch:874 learning rate:1.0000e-05, Loss_tot:0.4975,\n",
      "Train Epoch:875 learning rate:1.0000e-05, Loss_tot:0.4971,\n",
      "save model\n",
      "Train Epoch:876 learning rate:1.0000e-05, Loss_tot:0.4978,\n",
      "Train Epoch:877 learning rate:1.0000e-05, Loss_tot:0.4974,\n",
      "Train Epoch:878 learning rate:1.0000e-05, Loss_tot:0.4970,\n",
      "save model\n",
      "Train Epoch:879 learning rate:1.0000e-05, Loss_tot:0.4960,\n",
      "save model\n",
      "Train Epoch:880 learning rate:1.0000e-05, Loss_tot:0.4946,\n",
      "save model\n",
      "Train Epoch:881 learning rate:1.0000e-05, Loss_tot:0.4939,\n",
      "save model\n",
      "Train Epoch:882 learning rate:1.0000e-05, Loss_tot:0.4944,\n",
      "Train Epoch:883 learning rate:1.0000e-05, Loss_tot:0.4951,\n",
      "Train Epoch:884 learning rate:1.0000e-05, Loss_tot:0.4947,\n",
      "Train Epoch:885 learning rate:1.0000e-05, Loss_tot:0.4926,\n",
      "save model\n",
      "Train Epoch:886 learning rate:1.0000e-05, Loss_tot:0.4922,\n",
      "save model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:887 learning rate:1.0000e-05, Loss_tot:0.4907,\n",
      "save model\n",
      "Train Epoch:888 learning rate:1.0000e-05, Loss_tot:0.4931,\n",
      "Train Epoch:889 learning rate:1.0000e-05, Loss_tot:0.4916,\n",
      "Train Epoch:890 learning rate:1.0000e-05, Loss_tot:0.4900,\n",
      "save model\n",
      "Train Epoch:891 learning rate:1.0000e-05, Loss_tot:0.4890,\n",
      "save model\n",
      "Train Epoch:892 learning rate:1.0000e-05, Loss_tot:0.4909,\n",
      "Train Epoch:893 learning rate:1.0000e-05, Loss_tot:0.4927,\n",
      "Train Epoch:894 learning rate:1.0000e-05, Loss_tot:0.4922,\n",
      "Train Epoch:895 learning rate:1.0000e-05, Loss_tot:0.4908,\n",
      "Train Epoch:896 learning rate:1.0000e-05, Loss_tot:0.4909,\n",
      "Train Epoch:897 learning rate:1.0000e-05, Loss_tot:0.4905,\n",
      "Train Epoch:898 learning rate:1.0000e-05, Loss_tot:0.4895,\n",
      "Train Epoch:899 learning rate:1.0000e-05, Loss_tot:0.4891,\n",
      "Train Epoch:900 learning rate:1.0000e-05, Loss_tot:0.4902,\n",
      "Train Epoch:901 learning rate:1.0000e-05, Loss_tot:0.4897,\n",
      "Train Epoch:902 learning rate:1.0000e-05, Loss_tot:0.4887,\n",
      "save model\n",
      "Train Epoch:903 learning rate:1.0000e-05, Loss_tot:0.4879,\n",
      "save model\n",
      "Train Epoch:904 learning rate:1.0000e-05, Loss_tot:0.4865,\n",
      "save model\n",
      "Train Epoch:905 learning rate:1.0000e-05, Loss_tot:0.4901,\n",
      "Train Epoch:906 learning rate:1.0000e-05, Loss_tot:0.4905,\n",
      "Train Epoch:907 learning rate:1.0000e-05, Loss_tot:0.4908,\n",
      "Train Epoch:908 learning rate:1.0000e-05, Loss_tot:0.4893,\n",
      "Train Epoch:909 learning rate:1.0000e-05, Loss_tot:0.4883,\n",
      "Train Epoch:910 learning rate:1.0000e-05, Loss_tot:0.4878,\n",
      "Train Epoch:911 learning rate:1.0000e-05, Loss_tot:0.4882,\n",
      "Train Epoch:912 learning rate:1.0000e-05, Loss_tot:0.4868,\n",
      "Train Epoch:913 learning rate:1.0000e-05, Loss_tot:0.4878,\n",
      "Train Epoch:914 learning rate:1.0000e-05, Loss_tot:0.4886,\n",
      "Train Epoch:915 learning rate:1.0000e-05, Loss_tot:0.4876,\n",
      "Train Epoch:916 learning rate:1.0000e-05, Loss_tot:0.4873,\n",
      "Train Epoch:917 learning rate:1.0000e-05, Loss_tot:0.4867,\n",
      "Train Epoch:918 learning rate:1.0000e-05, Loss_tot:0.4855,\n",
      "save model\n",
      "Train Epoch:919 learning rate:1.0000e-05, Loss_tot:0.4849,\n",
      "save model\n",
      "Train Epoch:920 learning rate:1.0000e-05, Loss_tot:0.4853,\n",
      "Train Epoch:921 learning rate:1.0000e-05, Loss_tot:0.4832,\n",
      "save model\n",
      "Train Epoch:922 learning rate:1.0000e-05, Loss_tot:0.4842,\n",
      "Train Epoch:923 learning rate:1.0000e-05, Loss_tot:0.4837,\n",
      "Train Epoch:924 learning rate:1.0000e-05, Loss_tot:0.4829,\n",
      "save model\n",
      "Train Epoch:925 learning rate:1.0000e-05, Loss_tot:0.4817,\n",
      "save model\n",
      "Train Epoch:926 learning rate:1.0000e-05, Loss_tot:0.4812,\n",
      "save model\n",
      "Train Epoch:927 learning rate:1.0000e-05, Loss_tot:0.4812,\n",
      "Train Epoch:928 learning rate:1.0000e-05, Loss_tot:0.4810,\n",
      "save model\n",
      "Train Epoch:929 learning rate:1.0000e-05, Loss_tot:0.4799,\n",
      "save model\n",
      "Train Epoch:930 learning rate:1.0000e-05, Loss_tot:0.4795,\n",
      "save model\n",
      "Train Epoch:931 learning rate:1.0000e-05, Loss_tot:0.4809,\n",
      "Train Epoch:932 learning rate:1.0000e-05, Loss_tot:0.4804,\n",
      "Train Epoch:933 learning rate:1.0000e-05, Loss_tot:0.4787,\n",
      "save model\n",
      "Train Epoch:934 learning rate:1.0000e-05, Loss_tot:0.4801,\n",
      "Train Epoch:935 learning rate:1.0000e-05, Loss_tot:0.4787,\n",
      "Train Epoch:936 learning rate:1.0000e-05, Loss_tot:0.4767,\n",
      "save model\n",
      "Train Epoch:937 learning rate:1.0000e-05, Loss_tot:0.4753,\n",
      "save model\n",
      "Train Epoch:938 learning rate:1.0000e-05, Loss_tot:0.4764,\n",
      "Train Epoch:939 learning rate:1.0000e-05, Loss_tot:0.4762,\n",
      "Train Epoch:940 learning rate:1.0000e-05, Loss_tot:0.4758,\n",
      "Train Epoch:941 learning rate:1.0000e-05, Loss_tot:0.4753,\n",
      "Train Epoch:942 learning rate:1.0000e-05, Loss_tot:0.4755,\n",
      "Train Epoch:943 learning rate:1.0000e-05, Loss_tot:0.4760,\n",
      "Train Epoch:944 learning rate:1.0000e-05, Loss_tot:0.4750,\n",
      "save model\n",
      "Train Epoch:945 learning rate:1.0000e-05, Loss_tot:0.4746,\n",
      "save model\n",
      "Train Epoch:946 learning rate:1.0000e-05, Loss_tot:0.4731,\n",
      "save model\n",
      "Train Epoch:947 learning rate:1.0000e-05, Loss_tot:0.4731,\n",
      "Train Epoch:948 learning rate:1.0000e-05, Loss_tot:0.4723,\n",
      "save model\n",
      "Train Epoch:949 learning rate:1.0000e-05, Loss_tot:0.4719,\n",
      "save model\n",
      "Train Epoch:950 learning rate:1.0000e-05, Loss_tot:0.4714,\n",
      "save model\n",
      "Train Epoch:951 learning rate:1.0000e-05, Loss_tot:0.4709,\n",
      "save model\n",
      "Train Epoch:952 learning rate:1.0000e-05, Loss_tot:0.4707,\n",
      "save model\n",
      "Train Epoch:953 learning rate:1.0000e-05, Loss_tot:0.4712,\n",
      "Train Epoch:954 learning rate:1.0000e-05, Loss_tot:0.4707,\n",
      "Train Epoch:955 learning rate:1.0000e-05, Loss_tot:0.4700,\n",
      "save model\n",
      "Train Epoch:956 learning rate:1.0000e-05, Loss_tot:0.4689,\n",
      "save model\n",
      "Train Epoch:957 learning rate:1.0000e-05, Loss_tot:0.4685,\n",
      "save model\n",
      "Train Epoch:958 learning rate:1.0000e-05, Loss_tot:0.4687,\n",
      "Train Epoch:959 learning rate:1.0000e-05, Loss_tot:0.4680,\n",
      "save model\n",
      "Train Epoch:960 learning rate:1.0000e-05, Loss_tot:0.4682,\n",
      "Train Epoch:961 learning rate:1.0000e-05, Loss_tot:0.4678,\n",
      "save model\n",
      "Train Epoch:962 learning rate:1.0000e-05, Loss_tot:0.4673,\n",
      "save model\n",
      "Train Epoch:963 learning rate:1.0000e-05, Loss_tot:0.4659,\n",
      "save model\n",
      "Train Epoch:964 learning rate:1.0000e-05, Loss_tot:0.4655,\n",
      "save model\n",
      "Train Epoch:965 learning rate:1.0000e-05, Loss_tot:0.4657,\n",
      "Train Epoch:966 learning rate:1.0000e-05, Loss_tot:0.4638,\n",
      "save model\n",
      "Train Epoch:967 learning rate:1.0000e-05, Loss_tot:0.4642,\n",
      "Train Epoch:968 learning rate:1.0000e-05, Loss_tot:0.4635,\n",
      "save model\n",
      "Train Epoch:969 learning rate:1.0000e-05, Loss_tot:0.4626,\n",
      "save model\n",
      "Train Epoch:970 learning rate:1.0000e-05, Loss_tot:0.4636,\n",
      "Train Epoch:971 learning rate:1.0000e-05, Loss_tot:0.4625,\n",
      "save model\n",
      "Train Epoch:972 learning rate:1.0000e-05, Loss_tot:0.4637,\n",
      "Train Epoch:973 learning rate:1.0000e-05, Loss_tot:0.4626,\n",
      "Train Epoch:974 learning rate:1.0000e-05, Loss_tot:0.4618,\n",
      "save model\n",
      "Train Epoch:975 learning rate:1.0000e-05, Loss_tot:0.4593,\n",
      "save model\n",
      "Train Epoch:976 learning rate:1.0000e-05, Loss_tot:0.4602,\n",
      "Train Epoch:977 learning rate:1.0000e-05, Loss_tot:0.4597,\n",
      "Train Epoch:978 learning rate:1.0000e-05, Loss_tot:0.4600,\n",
      "Train Epoch:979 learning rate:1.0000e-05, Loss_tot:0.4597,\n",
      "Train Epoch:980 learning rate:1.0000e-05, Loss_tot:0.4581,\n",
      "save model\n",
      "Train Epoch:981 learning rate:1.0000e-05, Loss_tot:0.4544,\n",
      "save model\n",
      "Train Epoch:982 learning rate:1.0000e-05, Loss_tot:0.4578,\n",
      "Train Epoch:983 learning rate:1.0000e-05, Loss_tot:0.4581,\n",
      "Train Epoch:984 learning rate:1.0000e-05, Loss_tot:0.4576,\n",
      "Train Epoch:985 learning rate:1.0000e-05, Loss_tot:0.4566,\n",
      "Train Epoch:986 learning rate:1.0000e-05, Loss_tot:0.4561,\n",
      "Train Epoch:987 learning rate:1.0000e-05, Loss_tot:0.4567,\n",
      "Train Epoch:988 learning rate:1.0000e-05, Loss_tot:0.4567,\n",
      "Train Epoch:989 learning rate:1.0000e-05, Loss_tot:0.4548,\n",
      "Train Epoch:990 learning rate:1.0000e-05, Loss_tot:0.4544,\n",
      "save model\n",
      "Train Epoch:991 learning rate:1.0000e-05, Loss_tot:0.4531,\n",
      "save model\n",
      "Train Epoch:992 learning rate:1.0000e-05, Loss_tot:0.4542,\n",
      "Train Epoch:993 learning rate:1.0000e-05, Loss_tot:0.4541,\n",
      "Train Epoch:994 learning rate:1.0000e-05, Loss_tot:0.4526,\n",
      "save model\n",
      "Train Epoch:995 learning rate:1.0000e-05, Loss_tot:0.4522,\n",
      "save model\n",
      "Train Epoch:996 learning rate:1.0000e-05, Loss_tot:0.4518,\n",
      "save model\n",
      "Train Epoch:997 learning rate:1.0000e-05, Loss_tot:0.4516,\n",
      "save model\n",
      "Train Epoch:998 learning rate:1.0000e-05, Loss_tot:0.4513,\n",
      "save model\n",
      "Train Epoch:999 learning rate:1.0000e-05, Loss_tot:0.4511,\n",
      "save model\n",
      "Train Epoch:1000 learning rate:1.0000e-05, Loss_tot:0.4500,\n",
      "save model\n",
      "Train Epoch:1001 learning rate:1.0000e-05, Loss_tot:0.4496,\n",
      "save model\n",
      "Train Epoch:1002 learning rate:1.0000e-05, Loss_tot:0.4502,\n",
      "Save model 1002\n",
      "Train Epoch:1003 learning rate:1.0000e-05, Loss_tot:0.4487,\n",
      "save model\n",
      "Save model 1003\n",
      "Train Epoch:1004 learning rate:1.0000e-05, Loss_tot:0.4467,\n",
      "save model\n",
      "Save model 1004\n",
      "Train Epoch:1005 learning rate:1.0000e-05, Loss_tot:0.4478,\n",
      "Save model 1005\n",
      "Train Epoch:1006 learning rate:1.0000e-05, Loss_tot:0.4482,\n",
      "Save model 1006\n",
      "Train Epoch:1007 learning rate:1.0000e-05, Loss_tot:0.4470,\n",
      "Save model 1007\n",
      "Train Epoch:1008 learning rate:1.0000e-05, Loss_tot:0.4465,\n",
      "save model\n",
      "Save model 1008\n",
      "Train Epoch:1009 learning rate:1.0000e-05, Loss_tot:0.4468,\n",
      "Save model 1009\n",
      "Train Epoch:1010 learning rate:1.0000e-05, Loss_tot:0.4463,\n",
      "save model\n",
      "Save model 1010\n",
      "Train Epoch:1011 learning rate:1.0000e-05, Loss_tot:0.4462,\n",
      "save model\n",
      "Save model 1011\n",
      "Train Epoch:1012 learning rate:1.0000e-05, Loss_tot:0.4441,\n",
      "save model\n",
      "Save model 1012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1013 learning rate:1.0000e-05, Loss_tot:0.4430,\n",
      "save model\n",
      "Save model 1013\n",
      "Train Epoch:1014 learning rate:1.0000e-05, Loss_tot:0.4439,\n",
      "Save model 1014\n",
      "Train Epoch:1015 learning rate:1.0000e-05, Loss_tot:0.4419,\n",
      "save model\n",
      "Save model 1015\n",
      "Train Epoch:1016 learning rate:1.0000e-05, Loss_tot:0.4430,\n",
      "Save model 1016\n",
      "Train Epoch:1017 learning rate:1.0000e-05, Loss_tot:0.4420,\n",
      "Save model 1017\n",
      "Train Epoch:1018 learning rate:1.0000e-05, Loss_tot:0.4431,\n",
      "Save model 1018\n",
      "Train Epoch:1019 learning rate:1.0000e-05, Loss_tot:0.4417,\n",
      "save model\n",
      "Save model 1019\n",
      "Train Epoch:1020 learning rate:1.0000e-05, Loss_tot:0.4420,\n",
      "Save model 1020\n",
      "Train Epoch:1021 learning rate:1.0000e-05, Loss_tot:0.4415,\n",
      "save model\n",
      "Save model 1021\n",
      "Train Epoch:1022 learning rate:1.0000e-05, Loss_tot:0.4378,\n",
      "save model\n",
      "Save model 1022\n",
      "Train Epoch:1023 learning rate:1.0000e-05, Loss_tot:0.4399,\n",
      "Save model 1023\n",
      "Train Epoch:1024 learning rate:1.0000e-05, Loss_tot:0.4402,\n",
      "Save model 1024\n",
      "Train Epoch:1025 learning rate:1.0000e-05, Loss_tot:0.4388,\n",
      "Save model 1025\n",
      "Train Epoch:1026 learning rate:1.0000e-05, Loss_tot:0.4370,\n",
      "save model\n",
      "Save model 1026\n",
      "Train Epoch:1027 learning rate:1.0000e-05, Loss_tot:0.4366,\n",
      "save model\n",
      "Save model 1027\n",
      "Train Epoch:1028 learning rate:1.0000e-05, Loss_tot:0.4370,\n",
      "Save model 1028\n",
      "Train Epoch:1029 learning rate:1.0000e-05, Loss_tot:0.4366,\n",
      "Save model 1029\n",
      "Train Epoch:1030 learning rate:1.0000e-05, Loss_tot:0.4368,\n",
      "Save model 1030\n",
      "Train Epoch:1031 learning rate:1.0000e-05, Loss_tot:0.4371,\n",
      "Save model 1031\n",
      "Train Epoch:1032 learning rate:1.0000e-05, Loss_tot:0.4359,\n",
      "save model\n",
      "Save model 1032\n",
      "Train Epoch:1033 learning rate:1.0000e-05, Loss_tot:0.4355,\n",
      "save model\n",
      "Save model 1033\n",
      "Train Epoch:1034 learning rate:1.0000e-05, Loss_tot:0.4351,\n",
      "save model\n",
      "Save model 1034\n",
      "Train Epoch:1035 learning rate:1.0000e-05, Loss_tot:0.4344,\n",
      "save model\n",
      "Save model 1035\n",
      "Train Epoch:1036 learning rate:1.0000e-05, Loss_tot:0.4342,\n",
      "save model\n",
      "Save model 1036\n",
      "Train Epoch:1037 learning rate:1.0000e-05, Loss_tot:0.4337,\n",
      "save model\n",
      "Save model 1037\n",
      "Train Epoch:1038 learning rate:1.0000e-05, Loss_tot:0.4333,\n",
      "save model\n",
      "Save model 1038\n",
      "Train Epoch:1039 learning rate:1.0000e-05, Loss_tot:0.4328,\n",
      "save model\n",
      "Save model 1039\n",
      "Train Epoch:1040 learning rate:1.0000e-05, Loss_tot:0.4324,\n",
      "save model\n",
      "Save model 1040\n",
      "Train Epoch:1041 learning rate:1.0000e-05, Loss_tot:0.4319,\n",
      "save model\n",
      "Save model 1041\n",
      "Train Epoch:1042 learning rate:1.0000e-05, Loss_tot:0.4312,\n",
      "save model\n",
      "Save model 1042\n",
      "Train Epoch:1043 learning rate:1.0000e-05, Loss_tot:0.4310,\n",
      "save model\n",
      "Save model 1043\n",
      "Train Epoch:1044 learning rate:1.0000e-05, Loss_tot:0.4305,\n",
      "save model\n",
      "Save model 1044\n",
      "Train Epoch:1045 learning rate:1.0000e-05, Loss_tot:0.4300,\n",
      "save model\n",
      "Save model 1045\n",
      "Train Epoch:1046 learning rate:1.0000e-05, Loss_tot:0.4303,\n",
      "Save model 1046\n",
      "Train Epoch:1047 learning rate:1.0000e-05, Loss_tot:0.4284,\n",
      "save model\n",
      "Save model 1047\n",
      "Train Epoch:1048 learning rate:1.0000e-05, Loss_tot:0.4285,\n",
      "Save model 1048\n",
      "Train Epoch:1049 learning rate:1.0000e-05, Loss_tot:0.4260,\n",
      "save model\n",
      "Save model 1049\n",
      "Train Epoch:1050 learning rate:1.0000e-05, Loss_tot:0.4262,\n",
      "Save model 1050\n",
      "Train Epoch:1051 learning rate:1.0000e-05, Loss_tot:0.4257,\n",
      "save model\n",
      "Save model 1051\n",
      "Train Epoch:1052 learning rate:1.0000e-05, Loss_tot:0.4269,\n",
      "Save model 1052\n",
      "Train Epoch:1053 learning rate:1.0000e-05, Loss_tot:0.4264,\n",
      "Save model 1053\n",
      "Train Epoch:1054 learning rate:1.0000e-05, Loss_tot:0.4259,\n",
      "Save model 1054\n",
      "Train Epoch:1055 learning rate:1.0000e-05, Loss_tot:0.4255,\n",
      "save model\n",
      "Save model 1055\n",
      "Train Epoch:1056 learning rate:1.0000e-05, Loss_tot:0.4250,\n",
      "save model\n",
      "Save model 1056\n",
      "Train Epoch:1057 learning rate:1.0000e-05, Loss_tot:0.4231,\n",
      "save model\n",
      "Save model 1057\n",
      "Train Epoch:1058 learning rate:1.0000e-05, Loss_tot:0.4241,\n",
      "Save model 1058\n",
      "Train Epoch:1059 learning rate:1.0000e-05, Loss_tot:0.4236,\n",
      "Save model 1059\n",
      "Train Epoch:1060 learning rate:1.0000e-05, Loss_tot:0.4225,\n",
      "save model\n",
      "Save model 1060\n",
      "Train Epoch:1061 learning rate:1.0000e-05, Loss_tot:0.4220,\n",
      "save model\n",
      "Save model 1061\n",
      "Train Epoch:1062 learning rate:1.0000e-05, Loss_tot:0.4216,\n",
      "save model\n",
      "Save model 1062\n",
      "Train Epoch:1063 learning rate:1.0000e-05, Loss_tot:0.4185,\n",
      "save model\n",
      "Save model 1063\n",
      "Train Epoch:1064 learning rate:1.0000e-05, Loss_tot:0.4213,\n",
      "Save model 1064\n",
      "Train Epoch:1065 learning rate:1.0000e-05, Loss_tot:0.4190,\n",
      "Save model 1065\n",
      "Train Epoch:1066 learning rate:1.0000e-05, Loss_tot:0.4186,\n",
      "Save model 1066\n",
      "Train Epoch:1067 learning rate:1.0000e-05, Loss_tot:0.4197,\n",
      "Save model 1067\n",
      "Train Epoch:1068 learning rate:1.0000e-05, Loss_tot:0.4193,\n",
      "Save model 1068\n",
      "Train Epoch:1069 learning rate:1.0000e-05, Loss_tot:0.4188,\n",
      "Save model 1069\n",
      "Train Epoch:1070 learning rate:1.0000e-05, Loss_tot:0.4183,\n",
      "save model\n",
      "Save model 1070\n",
      "Train Epoch:1071 learning rate:1.0000e-05, Loss_tot:0.4179,\n",
      "save model\n",
      "Save model 1071\n",
      "Train Epoch:1072 learning rate:1.0000e-05, Loss_tot:0.4174,\n",
      "save model\n",
      "Save model 1072\n",
      "Train Epoch:1073 learning rate:1.0000e-05, Loss_tot:0.4169,\n",
      "save model\n",
      "Save model 1073\n",
      "Train Epoch:1074 learning rate:1.0000e-05, Loss_tot:0.4164,\n",
      "save model\n",
      "Save model 1074\n",
      "Train Epoch:1075 learning rate:1.0000e-05, Loss_tot:0.4153,\n",
      "save model\n",
      "Save model 1075\n",
      "Train Epoch:1076 learning rate:1.0000e-05, Loss_tot:0.4148,\n",
      "save model\n",
      "Save model 1076\n",
      "Train Epoch:1077 learning rate:1.0000e-05, Loss_tot:0.4143,\n",
      "save model\n",
      "Save model 1077\n",
      "Train Epoch:1078 learning rate:1.0000e-05, Loss_tot:0.4122,\n",
      "save model\n",
      "Save model 1078\n",
      "Train Epoch:1079 learning rate:1.0000e-05, Loss_tot:0.4124,\n",
      "Save model 1079\n",
      "Train Epoch:1080 learning rate:1.0000e-05, Loss_tot:0.4136,\n",
      "Save model 1080\n",
      "Train Epoch:1081 learning rate:1.0000e-05, Loss_tot:0.4131,\n",
      "Save model 1081\n",
      "Train Epoch:1082 learning rate:1.0000e-05, Loss_tot:0.4126,\n",
      "Save model 1082\n",
      "Train Epoch:1083 learning rate:1.0000e-05, Loss_tot:0.4121,\n",
      "save model\n",
      "Save model 1083\n",
      "Train Epoch:1084 learning rate:1.0000e-05, Loss_tot:0.4117,\n",
      "save model\n",
      "Save model 1084\n",
      "Train Epoch:1085 learning rate:1.0000e-05, Loss_tot:0.4098,\n",
      "save model\n",
      "Save model 1085\n",
      "Train Epoch:1086 learning rate:1.0000e-05, Loss_tot:0.4100,\n",
      "Save model 1086\n",
      "Train Epoch:1087 learning rate:1.0000e-05, Loss_tot:0.4079,\n",
      "save model\n",
      "Save model 1087\n",
      "Train Epoch:1088 learning rate:1.0000e-05, Loss_tot:0.4091,\n",
      "Save model 1088\n",
      "Train Epoch:1089 learning rate:1.0000e-05, Loss_tot:0.4086,\n",
      "Save model 1089\n",
      "Train Epoch:1090 learning rate:1.0000e-05, Loss_tot:0.4081,\n",
      "Save model 1090\n",
      "Train Epoch:1091 learning rate:1.0000e-05, Loss_tot:0.4060,\n",
      "save model\n",
      "Save model 1091\n",
      "Train Epoch:1092 learning rate:1.0000e-05, Loss_tot:0.4071,\n",
      "Save model 1092\n",
      "Train Epoch:1093 learning rate:1.0000e-05, Loss_tot:0.4066,\n",
      "Save model 1093\n",
      "Train Epoch:1094 learning rate:1.0000e-05, Loss_tot:0.4061,\n",
      "Save model 1094\n",
      "Train Epoch:1095 learning rate:1.0000e-05, Loss_tot:0.4040,\n",
      "save model\n",
      "Save model 1095\n",
      "Train Epoch:1096 learning rate:1.0000e-05, Loss_tot:0.4037,\n",
      "save model\n",
      "Save model 1096\n",
      "Train Epoch:1097 learning rate:1.0000e-05, Loss_tot:0.4020,\n",
      "save model\n",
      "Save model 1097\n",
      "Train Epoch:1098 learning rate:1.0000e-05, Loss_tot:0.4041,\n",
      "Save model 1098\n",
      "Train Epoch:1099 learning rate:1.0000e-05, Loss_tot:0.4020,\n",
      "save model\n",
      "Save model 1099\n",
      "Train Epoch:1100 learning rate:1.0000e-05, Loss_tot:0.4032,\n",
      "Save model 1100\n",
      "Train Epoch:1101 learning rate:1.0000e-05, Loss_tot:0.4027,\n",
      "Save model 1101\n",
      "Train Epoch:1102 learning rate:1.0000e-05, Loss_tot:0.4022,\n",
      "Save model 1102\n",
      "Train Epoch:1103 learning rate:1.0000e-05, Loss_tot:0.4017,\n",
      "save model\n",
      "Save model 1103\n",
      "Train Epoch:1104 learning rate:1.0000e-05, Loss_tot:0.3995,\n",
      "save model\n",
      "Save model 1104\n",
      "Train Epoch:1105 learning rate:1.0000e-05, Loss_tot:0.4007,\n",
      "Save model 1105\n",
      "Train Epoch:1106 learning rate:1.0000e-05, Loss_tot:0.4001,\n",
      "Save model 1106\n",
      "Train Epoch:1107 learning rate:1.0000e-05, Loss_tot:0.3980,\n",
      "save model\n",
      "Save model 1107\n",
      "Train Epoch:1108 learning rate:1.0000e-05, Loss_tot:0.3991,\n",
      "Save model 1108\n",
      "Train Epoch:1109 learning rate:1.0000e-05, Loss_tot:0.3986,\n",
      "Save model 1109\n",
      "Train Epoch:1110 learning rate:1.0000e-05, Loss_tot:0.3981,\n",
      "Save model 1110\n",
      "Train Epoch:1111 learning rate:1.0000e-05, Loss_tot:0.3976,\n",
      "save model\n",
      "Save model 1111\n",
      "Train Epoch:1112 learning rate:1.0000e-05, Loss_tot:0.3954,\n",
      "save model\n",
      "Save model 1112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1113 learning rate:1.0000e-05, Loss_tot:0.3965,\n",
      "Save model 1113\n",
      "Train Epoch:1114 learning rate:1.0000e-05, Loss_tot:0.3959,\n",
      "Save model 1114\n",
      "Train Epoch:1115 learning rate:1.0000e-05, Loss_tot:0.3938,\n",
      "save model\n",
      "Save model 1115\n",
      "Train Epoch:1116 learning rate:1.0000e-05, Loss_tot:0.3949,\n",
      "Save model 1116\n",
      "Train Epoch:1117 learning rate:1.0000e-05, Loss_tot:0.3943,\n",
      "Save model 1117\n",
      "Train Epoch:1118 learning rate:1.0000e-05, Loss_tot:0.3937,\n",
      "save model\n",
      "Save model 1118\n",
      "Train Epoch:1119 learning rate:1.0000e-05, Loss_tot:0.3901,\n",
      "save model\n",
      "Save model 1119\n",
      "Train Epoch:1120 learning rate:1.0000e-05, Loss_tot:0.3925,\n",
      "Save model 1120\n",
      "Train Epoch:1121 learning rate:1.0000e-05, Loss_tot:0.3919,\n",
      "Save model 1121\n",
      "Train Epoch:1122 learning rate:1.0000e-05, Loss_tot:0.3896,\n",
      "save model\n",
      "Save model 1122\n",
      "Train Epoch:1123 learning rate:1.0000e-05, Loss_tot:0.3905,\n",
      "Save model 1123\n",
      "Train Epoch:1124 learning rate:1.0000e-05, Loss_tot:0.3896,\n",
      "Save model 1124\n",
      "Train Epoch:1125 learning rate:1.0000e-05, Loss_tot:0.3898,\n",
      "Save model 1125\n",
      "Train Epoch:1126 learning rate:1.0000e-05, Loss_tot:0.3876,\n",
      "save model\n",
      "Save model 1126\n",
      "Train Epoch:1127 learning rate:1.0000e-05, Loss_tot:0.3871,\n",
      "save model\n",
      "Save model 1127\n",
      "Train Epoch:1128 learning rate:1.0000e-05, Loss_tot:0.3882,\n",
      "Save model 1128\n",
      "Train Epoch:1129 learning rate:1.0000e-05, Loss_tot:0.3851,\n",
      "save model\n",
      "Save model 1129\n",
      "Train Epoch:1130 learning rate:1.0000e-05, Loss_tot:0.3873,\n",
      "Save model 1130\n",
      "Train Epoch:1131 learning rate:1.0000e-05, Loss_tot:0.3868,\n",
      "Save model 1131\n",
      "Train Epoch:1132 learning rate:1.0000e-05, Loss_tot:0.3862,\n",
      "Save model 1132\n",
      "Train Epoch:1133 learning rate:1.0000e-05, Loss_tot:0.3857,\n",
      "Save model 1133\n",
      "Train Epoch:1134 learning rate:1.0000e-05, Loss_tot:0.3821,\n",
      "save model\n",
      "Save model 1134\n",
      "Train Epoch:1135 learning rate:1.0000e-05, Loss_tot:0.3831,\n",
      "Save model 1135\n",
      "Train Epoch:1136 learning rate:1.0000e-05, Loss_tot:0.3842,\n",
      "Save model 1136\n",
      "Train Epoch:1137 learning rate:1.0000e-05, Loss_tot:0.3837,\n",
      "Save model 1137\n",
      "Train Epoch:1138 learning rate:1.0000e-05, Loss_tot:0.3832,\n",
      "Save model 1138\n",
      "Train Epoch:1139 learning rate:1.0000e-05, Loss_tot:0.3810,\n",
      "save model\n",
      "Save model 1139\n",
      "Train Epoch:1140 learning rate:1.0000e-05, Loss_tot:0.3822,\n",
      "Save model 1140\n",
      "Train Epoch:1141 learning rate:1.0000e-05, Loss_tot:0.3817,\n",
      "Save model 1141\n",
      "Train Epoch:1142 learning rate:1.0000e-05, Loss_tot:0.3795,\n",
      "save model\n",
      "Save model 1142\n",
      "Train Epoch:1143 learning rate:1.0000e-05, Loss_tot:0.3790,\n",
      "save model\n",
      "Save model 1143\n",
      "Train Epoch:1144 learning rate:1.0000e-05, Loss_tot:0.3802,\n",
      "Save model 1144\n",
      "Train Epoch:1145 learning rate:1.0000e-05, Loss_tot:0.3797,\n",
      "Save model 1145\n",
      "Train Epoch:1146 learning rate:1.0000e-05, Loss_tot:0.3792,\n",
      "Save model 1146\n",
      "Train Epoch:1147 learning rate:1.0000e-05, Loss_tot:0.3787,\n",
      "save model\n",
      "Save model 1147\n",
      "Train Epoch:1148 learning rate:1.0000e-05, Loss_tot:0.3782,\n",
      "save model\n",
      "Save model 1148\n",
      "Train Epoch:1149 learning rate:1.0000e-05, Loss_tot:0.3760,\n",
      "save model\n",
      "Save model 1149\n",
      "Train Epoch:1150 learning rate:1.0000e-05, Loss_tot:0.3755,\n",
      "save model\n",
      "Save model 1150\n",
      "Train Epoch:1151 learning rate:1.0000e-05, Loss_tot:0.3750,\n",
      "save model\n",
      "Save model 1151\n",
      "Train Epoch:1152 learning rate:1.0000e-05, Loss_tot:0.3748,\n",
      "save model\n",
      "Save model 1152\n",
      "Train Epoch:1153 learning rate:1.0000e-05, Loss_tot:0.3756,\n",
      "Save model 1153\n",
      "Train Epoch:1154 learning rate:1.0000e-05, Loss_tot:0.3751,\n",
      "Save model 1154\n",
      "Train Epoch:1155 learning rate:1.0000e-05, Loss_tot:0.3746,\n",
      "save model\n",
      "Save model 1155\n",
      "Train Epoch:1156 learning rate:1.0000e-05, Loss_tot:0.3724,\n",
      "save model\n",
      "Save model 1156\n",
      "Train Epoch:1157 learning rate:1.0000e-05, Loss_tot:0.3719,\n",
      "save model\n",
      "Save model 1157\n",
      "Train Epoch:1158 learning rate:1.0000e-05, Loss_tot:0.3714,\n",
      "save model\n",
      "Save model 1158\n",
      "Train Epoch:1159 learning rate:1.0000e-05, Loss_tot:0.3726,\n",
      "Save model 1159\n",
      "Train Epoch:1160 learning rate:1.0000e-05, Loss_tot:0.3720,\n",
      "Save model 1160\n",
      "Train Epoch:1161 learning rate:1.0000e-05, Loss_tot:0.3720,\n",
      "Save model 1161\n",
      "Train Epoch:1162 learning rate:1.0000e-05, Loss_tot:0.3695,\n",
      "save model\n",
      "Save model 1162\n",
      "Train Epoch:1163 learning rate:1.0000e-05, Loss_tot:0.3721,\n",
      "Save model 1163\n",
      "Train Epoch:1164 learning rate:1.0000e-05, Loss_tot:0.3703,\n",
      "Save model 1164\n",
      "Train Epoch:1165 learning rate:1.0000e-05, Loss_tot:0.3703,\n",
      "Save model 1165\n",
      "Train Epoch:1166 learning rate:1.0000e-05, Loss_tot:0.3718,\n",
      "Save model 1166\n",
      "Train Epoch:1167 learning rate:1.0000e-05, Loss_tot:0.3717,\n",
      "Save model 1167\n",
      "Train Epoch:1168 learning rate:1.0000e-05, Loss_tot:0.3698,\n",
      "Save model 1168\n",
      "Train Epoch:1169 learning rate:1.0000e-05, Loss_tot:0.3681,\n",
      "save model\n",
      "Save model 1169\n",
      "Train Epoch:1170 learning rate:1.0000e-05, Loss_tot:0.3710,\n",
      "Save model 1170\n",
      "Train Epoch:1171 learning rate:1.0000e-05, Loss_tot:0.3707,\n",
      "Save model 1171\n",
      "Train Epoch:1172 learning rate:1.0000e-05, Loss_tot:0.3704,\n",
      "Save model 1172\n",
      "Train Epoch:1173 learning rate:1.0000e-05, Loss_tot:0.3701,\n",
      "Save model 1173\n",
      "Train Epoch:1174 learning rate:1.0000e-05, Loss_tot:0.3680,\n",
      "save model\n",
      "Save model 1174\n",
      "Train Epoch:1175 learning rate:1.0000e-05, Loss_tot:0.3678,\n",
      "save model\n",
      "Save model 1175\n",
      "Train Epoch:1176 learning rate:1.0000e-05, Loss_tot:0.3686,\n",
      "Save model 1176\n",
      "Train Epoch:1177 learning rate:1.0000e-05, Loss_tot:0.3685,\n",
      "Save model 1177\n",
      "Train Epoch:1178 learning rate:1.0000e-05, Loss_tot:0.3676,\n",
      "save model\n",
      "Save model 1178\n",
      "Train Epoch:1179 learning rate:1.0000e-05, Loss_tot:0.3691,\n",
      "Save model 1179\n",
      "Train Epoch:1180 learning rate:1.0000e-05, Loss_tot:0.3676,\n",
      "Save model 1180\n",
      "Train Epoch:1181 learning rate:1.0000e-05, Loss_tot:0.3686,\n",
      "Save model 1181\n",
      "Train Epoch:1182 learning rate:1.0000e-05, Loss_tot:0.3666,\n",
      "save model\n",
      "Save model 1182\n",
      "Train Epoch:1183 learning rate:1.0000e-05, Loss_tot:0.3666,\n",
      "save model\n",
      "Save model 1183\n",
      "Train Epoch:1184 learning rate:1.0000e-05, Loss_tot:0.3664,\n",
      "save model\n",
      "Save model 1184\n",
      "Train Epoch:1185 learning rate:1.0000e-05, Loss_tot:0.3680,\n",
      "Save model 1185\n",
      "Train Epoch:1186 learning rate:1.0000e-05, Loss_tot:0.3678,\n",
      "Save model 1186\n",
      "Train Epoch:1187 learning rate:1.0000e-05, Loss_tot:0.3676,\n",
      "Save model 1187\n",
      "Train Epoch:1188 learning rate:1.0000e-05, Loss_tot:0.3657,\n",
      "save model\n",
      "Save model 1188\n",
      "Train Epoch:1189 learning rate:1.0000e-05, Loss_tot:0.3656,\n",
      "save model\n",
      "Save model 1189\n",
      "Train Epoch:1190 learning rate:1.0000e-05, Loss_tot:0.3654,\n",
      "save model\n",
      "Save model 1190\n",
      "Train Epoch:1191 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "save model\n",
      "Save model 1191\n",
      "Train Epoch:1192 learning rate:1.0000e-05, Loss_tot:0.3655,\n",
      "Save model 1192\n",
      "Train Epoch:1193 learning rate:1.0000e-05, Loss_tot:0.3666,\n",
      "Save model 1193\n",
      "Train Epoch:1194 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "save model\n",
      "Save model 1194\n",
      "Train Epoch:1195 learning rate:1.0000e-05, Loss_tot:0.3644,\n",
      "Save model 1195\n",
      "Train Epoch:1196 learning rate:1.0000e-05, Loss_tot:0.3641,\n",
      "Save model 1196\n",
      "Train Epoch:1197 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "save model\n",
      "Save model 1197\n",
      "Train Epoch:1198 learning rate:1.0000e-05, Loss_tot:0.3668,\n",
      "Save model 1198\n",
      "Train Epoch:1199 learning rate:1.0000e-05, Loss_tot:0.3667,\n",
      "Save model 1199\n",
      "Train Epoch:1200 learning rate:1.0000e-05, Loss_tot:0.3648,\n",
      "Save model 1200\n",
      "Train Epoch:1201 learning rate:1.0000e-05, Loss_tot:0.3647,\n",
      "Save model 1201\n",
      "Train Epoch:1202 learning rate:1.0000e-05, Loss_tot:0.3646,\n",
      "Save model 1202\n",
      "Train Epoch:1203 learning rate:1.0000e-05, Loss_tot:0.3662,\n",
      "Save model 1203\n",
      "Train Epoch:1204 learning rate:1.0000e-05, Loss_tot:0.3643,\n",
      "Save model 1204\n",
      "Train Epoch:1205 learning rate:1.0000e-05, Loss_tot:0.3660,\n",
      "Save model 1205\n",
      "Train Epoch:1206 learning rate:1.0000e-05, Loss_tot:0.3641,\n",
      "Save model 1206\n",
      "Train Epoch:1207 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "Save model 1207\n",
      "Train Epoch:1208 learning rate:1.0000e-05, Loss_tot:0.3638,\n",
      "save model\n",
      "Save model 1208\n",
      "Train Epoch:1209 learning rate:1.0000e-05, Loss_tot:0.3654,\n",
      "Save model 1209\n",
      "Train Epoch:1210 learning rate:1.0000e-05, Loss_tot:0.3653,\n",
      "Save model 1210\n",
      "Train Epoch:1211 learning rate:1.0000e-05, Loss_tot:0.3635,\n",
      "save model\n",
      "Save model 1211\n",
      "Train Epoch:1212 learning rate:1.0000e-05, Loss_tot:0.3634,\n",
      "save model\n",
      "Save model 1212\n",
      "Train Epoch:1213 learning rate:1.0000e-05, Loss_tot:0.3632,\n",
      "save model\n",
      "Save model 1213\n",
      "Train Epoch:1214 learning rate:1.0000e-05, Loss_tot:0.3630,\n",
      "save model\n",
      "Save model 1214\n",
      "Train Epoch:1215 learning rate:1.0000e-05, Loss_tot:0.3630,\n",
      "save model\n",
      "Save model 1215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1216 learning rate:1.0000e-05, Loss_tot:0.3614,\n",
      "save model\n",
      "Save model 1216\n",
      "Train Epoch:1217 learning rate:1.0000e-05, Loss_tot:0.3627,\n",
      "Save model 1217\n",
      "Train Epoch:1218 learning rate:1.0000e-05, Loss_tot:0.3626,\n",
      "Save model 1218\n",
      "Train Epoch:1219 learning rate:1.0000e-05, Loss_tot:0.3609,\n",
      "save model\n",
      "Save model 1219\n",
      "Train Epoch:1220 learning rate:1.0000e-05, Loss_tot:0.3623,\n",
      "Save model 1220\n",
      "Train Epoch:1221 learning rate:1.0000e-05, Loss_tot:0.3631,\n",
      "Save model 1221\n",
      "Train Epoch:1222 learning rate:1.0000e-05, Loss_tot:0.3621,\n",
      "Save model 1222\n",
      "Train Epoch:1223 learning rate:1.0000e-05, Loss_tot:0.3620,\n",
      "Save model 1223\n",
      "Train Epoch:1224 learning rate:1.0000e-05, Loss_tot:0.3618,\n",
      "Save model 1224\n",
      "Train Epoch:1225 learning rate:1.0000e-05, Loss_tot:0.3604,\n",
      "save model\n",
      "Save model 1225\n",
      "Train Epoch:1226 learning rate:1.0000e-05, Loss_tot:0.3603,\n",
      "save model\n",
      "Save model 1226\n",
      "Train Epoch:1227 learning rate:1.0000e-05, Loss_tot:0.3615,\n",
      "Save model 1227\n",
      "Train Epoch:1228 learning rate:1.0000e-05, Loss_tot:0.3613,\n",
      "Save model 1228\n",
      "Train Epoch:1229 learning rate:1.0000e-05, Loss_tot:0.3600,\n",
      "save model\n",
      "Save model 1229\n",
      "Train Epoch:1230 learning rate:1.0000e-05, Loss_tot:0.3598,\n",
      "save model\n",
      "Save model 1230\n",
      "Train Epoch:1231 learning rate:1.0000e-05, Loss_tot:0.3597,\n",
      "save model\n",
      "Save model 1231\n",
      "Train Epoch:1232 learning rate:1.0000e-05, Loss_tot:0.3596,\n",
      "save model\n",
      "Save model 1232\n",
      "Train Epoch:1233 learning rate:1.0000e-05, Loss_tot:0.3595,\n",
      "save model\n",
      "Save model 1233\n",
      "Train Epoch:1234 learning rate:1.0000e-05, Loss_tot:0.3593,\n",
      "save model\n",
      "Save model 1234\n",
      "Train Epoch:1235 learning rate:1.0000e-05, Loss_tot:0.3592,\n",
      "save model\n",
      "Save model 1235\n",
      "Train Epoch:1236 learning rate:1.0000e-05, Loss_tot:0.3591,\n",
      "save model\n",
      "Save model 1236\n",
      "Train Epoch:1237 learning rate:1.0000e-05, Loss_tot:0.3590,\n",
      "save model\n",
      "Save model 1237\n",
      "Train Epoch:1238 learning rate:1.0000e-05, Loss_tot:0.3589,\n",
      "save model\n",
      "Save model 1238\n",
      "Train Epoch:1239 learning rate:1.0000e-05, Loss_tot:0.3587,\n",
      "save model\n",
      "Save model 1239\n",
      "Train Epoch:1240 learning rate:1.0000e-05, Loss_tot:0.3587,\n",
      "save model\n",
      "Save model 1240\n",
      "Train Epoch:1241 learning rate:1.0000e-05, Loss_tot:0.3586,\n",
      "save model\n",
      "Save model 1241\n",
      "Train Epoch:1242 learning rate:1.0000e-05, Loss_tot:0.3597,\n",
      "Save model 1242\n",
      "Train Epoch:1243 learning rate:1.0000e-05, Loss_tot:0.3604,\n",
      "Save model 1243\n",
      "Train Epoch:1244 learning rate:1.0000e-05, Loss_tot:0.3594,\n",
      "Save model 1244\n",
      "Train Epoch:1245 learning rate:1.0000e-05, Loss_tot:0.3593,\n",
      "Save model 1245\n",
      "Train Epoch:1246 learning rate:1.0000e-05, Loss_tot:0.3579,\n",
      "save model\n",
      "Save model 1246\n",
      "Train Epoch:1247 learning rate:1.0000e-05, Loss_tot:0.3578,\n",
      "save model\n",
      "Save model 1247\n",
      "Train Epoch:1248 learning rate:1.0000e-05, Loss_tot:0.3560,\n",
      "save model\n",
      "Save model 1248\n",
      "Train Epoch:1249 learning rate:1.0000e-05, Loss_tot:0.3575,\n",
      "Save model 1249\n",
      "Train Epoch:1250 learning rate:1.0000e-05, Loss_tot:0.3574,\n",
      "Save model 1250\n",
      "Train Epoch:1251 learning rate:1.0000e-05, Loss_tot:0.3574,\n",
      "Save model 1251\n",
      "Train Epoch:1252 learning rate:1.0000e-05, Loss_tot:0.3572,\n",
      "Save model 1252\n",
      "Train Epoch:1253 learning rate:1.0000e-05, Loss_tot:0.3571,\n",
      "Save model 1253\n",
      "Train Epoch:1254 learning rate:1.0000e-05, Loss_tot:0.3569,\n",
      "Save model 1254\n",
      "Train Epoch:1255 learning rate:1.0000e-05, Loss_tot:0.3569,\n",
      "Save model 1255\n",
      "Train Epoch:1256 learning rate:1.0000e-05, Loss_tot:0.3568,\n",
      "Save model 1256\n",
      "Train Epoch:1257 learning rate:1.0000e-05, Loss_tot:0.3566,\n",
      "Save model 1257\n",
      "Train Epoch:1258 learning rate:1.0000e-05, Loss_tot:0.3565,\n",
      "Save model 1258\n",
      "Train Epoch:1259 learning rate:1.0000e-05, Loss_tot:0.3550,\n",
      "save model\n",
      "Save model 1259\n",
      "Train Epoch:1260 learning rate:1.0000e-05, Loss_tot:0.3563,\n",
      "Save model 1260\n",
      "Train Epoch:1261 learning rate:1.0000e-05, Loss_tot:0.3562,\n",
      "Save model 1261\n",
      "Train Epoch:1262 learning rate:1.0000e-05, Loss_tot:0.3561,\n",
      "Save model 1262\n",
      "Train Epoch:1263 learning rate:1.0000e-05, Loss_tot:0.3560,\n",
      "Save model 1263\n",
      "Train Epoch:1264 learning rate:1.0000e-05, Loss_tot:0.3558,\n",
      "Save model 1264\n",
      "Train Epoch:1265 learning rate:1.0000e-05, Loss_tot:0.3569,\n",
      "Save model 1265\n",
      "Train Epoch:1266 learning rate:1.0000e-05, Loss_tot:0.3556,\n",
      "Save model 1266\n",
      "Train Epoch:1267 learning rate:1.0000e-05, Loss_tot:0.3554,\n",
      "Save model 1267\n",
      "Train Epoch:1268 learning rate:1.0000e-05, Loss_tot:0.3562,\n",
      "Save model 1268\n",
      "Train Epoch:1269 learning rate:1.0000e-05, Loss_tot:0.3552,\n",
      "Save model 1269\n",
      "Train Epoch:1270 learning rate:1.0000e-05, Loss_tot:0.3550,\n",
      "Save model 1270\n",
      "Train Epoch:1271 learning rate:1.0000e-05, Loss_tot:0.3549,\n",
      "save model\n",
      "Save model 1271\n",
      "Train Epoch:1272 learning rate:1.0000e-05, Loss_tot:0.3548,\n",
      "save model\n",
      "Save model 1272\n",
      "Train Epoch:1273 learning rate:1.0000e-05, Loss_tot:0.3547,\n",
      "save model\n",
      "Save model 1273\n",
      "Train Epoch:1274 learning rate:1.0000e-05, Loss_tot:0.3546,\n",
      "save model\n",
      "Save model 1274\n",
      "Train Epoch:1275 learning rate:1.0000e-05, Loss_tot:0.3545,\n",
      "save model\n",
      "Save model 1275\n",
      "Train Epoch:1276 learning rate:1.0000e-05, Loss_tot:0.3543,\n",
      "save model\n",
      "Save model 1276\n",
      "Train Epoch:1277 learning rate:1.0000e-05, Loss_tot:0.3527,\n",
      "save model\n",
      "Save model 1277\n",
      "Train Epoch:1278 learning rate:1.0000e-05, Loss_tot:0.3542,\n",
      "Save model 1278\n",
      "Train Epoch:1279 learning rate:1.0000e-05, Loss_tot:0.3540,\n",
      "Save model 1279\n",
      "Train Epoch:1280 learning rate:1.0000e-05, Loss_tot:0.3538,\n",
      "Save model 1280\n",
      "Train Epoch:1281 learning rate:1.0000e-05, Loss_tot:0.3538,\n",
      "Save model 1281\n",
      "Train Epoch:1282 learning rate:1.0000e-05, Loss_tot:0.3549,\n",
      "Save model 1282\n",
      "Train Epoch:1283 learning rate:1.0000e-05, Loss_tot:0.3535,\n",
      "Save model 1283\n",
      "Train Epoch:1284 learning rate:1.0000e-05, Loss_tot:0.3534,\n",
      "Save model 1284\n",
      "Train Epoch:1285 learning rate:1.0000e-05, Loss_tot:0.3533,\n",
      "Save model 1285\n",
      "Train Epoch:1286 learning rate:1.0000e-05, Loss_tot:0.3531,\n",
      "Save model 1286\n",
      "Train Epoch:1287 learning rate:1.0000e-05, Loss_tot:0.3531,\n",
      "Save model 1287\n",
      "Train Epoch:1288 learning rate:1.0000e-05, Loss_tot:0.3530,\n",
      "Save model 1288\n",
      "Train Epoch:1289 learning rate:1.0000e-05, Loss_tot:0.3529,\n",
      "Save model 1289\n",
      "Train Epoch:1290 learning rate:1.0000e-05, Loss_tot:0.3527,\n",
      "Save model 1290\n",
      "Train Epoch:1291 learning rate:1.0000e-05, Loss_tot:0.3526,\n",
      "save model\n",
      "Save model 1291\n",
      "Train Epoch:1292 learning rate:1.0000e-05, Loss_tot:0.3524,\n",
      "save model\n",
      "Save model 1292\n",
      "Train Epoch:1293 learning rate:1.0000e-05, Loss_tot:0.3533,\n",
      "Save model 1293\n",
      "Train Epoch:1294 learning rate:1.0000e-05, Loss_tot:0.3522,\n",
      "save model\n",
      "Save model 1294\n",
      "Train Epoch:1295 learning rate:1.0000e-05, Loss_tot:0.3521,\n",
      "save model\n",
      "Save model 1295\n",
      "Train Epoch:1296 learning rate:1.0000e-05, Loss_tot:0.3520,\n",
      "save model\n",
      "Save model 1296\n",
      "Train Epoch:1297 learning rate:1.0000e-05, Loss_tot:0.3519,\n",
      "save model\n",
      "Save model 1297\n",
      "Train Epoch:1298 learning rate:1.0000e-05, Loss_tot:0.3518,\n",
      "save model\n",
      "Save model 1298\n",
      "Train Epoch:1299 learning rate:1.0000e-05, Loss_tot:0.3529,\n",
      "Save model 1299\n",
      "Train Epoch:1300 learning rate:1.0000e-05, Loss_tot:0.3501,\n",
      "save model\n",
      "Save model 1300\n",
      "Train Epoch:1301 learning rate:1.0000e-05, Loss_tot:0.3514,\n",
      "Save model 1301\n",
      "Train Epoch:1302 learning rate:1.0000e-05, Loss_tot:0.3513,\n",
      "Save model 1302\n",
      "Train Epoch:1303 learning rate:1.0000e-05, Loss_tot:0.3511,\n",
      "Save model 1303\n",
      "Train Epoch:1304 learning rate:1.0000e-05, Loss_tot:0.3495,\n",
      "save model\n",
      "Save model 1304\n",
      "Train Epoch:1305 learning rate:1.0000e-05, Loss_tot:0.3509,\n",
      "Save model 1305\n",
      "Train Epoch:1306 learning rate:1.0000e-05, Loss_tot:0.3508,\n",
      "Save model 1306\n",
      "Train Epoch:1307 learning rate:1.0000e-05, Loss_tot:0.3507,\n",
      "Save model 1307\n",
      "Train Epoch:1308 learning rate:1.0000e-05, Loss_tot:0.3506,\n",
      "Save model 1308\n",
      "Train Epoch:1309 learning rate:1.0000e-05, Loss_tot:0.3505,\n",
      "Save model 1309\n",
      "Train Epoch:1310 learning rate:1.0000e-05, Loss_tot:0.3504,\n",
      "Save model 1310\n",
      "Train Epoch:1311 learning rate:1.0000e-05, Loss_tot:0.3502,\n",
      "Save model 1311\n",
      "Train Epoch:1312 learning rate:1.0000e-05, Loss_tot:0.3501,\n",
      "Save model 1312\n",
      "Train Epoch:1313 learning rate:1.0000e-05, Loss_tot:0.3500,\n",
      "Save model 1313\n",
      "Train Epoch:1314 learning rate:1.0000e-05, Loss_tot:0.3499,\n",
      "Save model 1314\n",
      "Train Epoch:1315 learning rate:1.0000e-05, Loss_tot:0.3507,\n",
      "Save model 1315\n",
      "Train Epoch:1316 learning rate:1.0000e-05, Loss_tot:0.3496,\n",
      "Save model 1316\n",
      "Train Epoch:1317 learning rate:1.0000e-05, Loss_tot:0.3495,\n",
      "Save model 1317\n",
      "Train Epoch:1318 learning rate:1.0000e-05, Loss_tot:0.3494,\n",
      "save model\n",
      "Save model 1318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1319 learning rate:1.0000e-05, Loss_tot:0.3506,\n",
      "Save model 1319\n",
      "Train Epoch:1320 learning rate:1.0000e-05, Loss_tot:0.3492,\n",
      "save model\n",
      "Save model 1320\n",
      "Train Epoch:1321 learning rate:1.0000e-05, Loss_tot:0.3491,\n",
      "save model\n",
      "Save model 1321\n",
      "Train Epoch:1322 learning rate:1.0000e-05, Loss_tot:0.3490,\n",
      "save model\n",
      "Save model 1322\n",
      "Train Epoch:1323 learning rate:1.0000e-05, Loss_tot:0.3489,\n",
      "save model\n",
      "Save model 1323\n",
      "Train Epoch:1324 learning rate:1.0000e-05, Loss_tot:0.3487,\n",
      "save model\n",
      "Save model 1324\n",
      "Train Epoch:1325 learning rate:1.0000e-05, Loss_tot:0.3486,\n",
      "save model\n",
      "Save model 1325\n",
      "Train Epoch:1326 learning rate:1.0000e-05, Loss_tot:0.3485,\n",
      "save model\n",
      "Save model 1326\n",
      "Train Epoch:1327 learning rate:1.0000e-05, Loss_tot:0.3484,\n",
      "save model\n",
      "Save model 1327\n",
      "Train Epoch:1328 learning rate:1.0000e-05, Loss_tot:0.3482,\n",
      "save model\n",
      "Save model 1328\n",
      "Train Epoch:1329 learning rate:1.0000e-05, Loss_tot:0.3481,\n",
      "save model\n",
      "Save model 1329\n",
      "Train Epoch:1330 learning rate:1.0000e-05, Loss_tot:0.3822,\n",
      "Save model 1330\n",
      "Train Epoch:1331 learning rate:1.0000e-05, Loss_tot:0.3797,\n",
      "Save model 1331\n",
      "Train Epoch:1332 learning rate:1.0000e-05, Loss_tot:0.3821,\n",
      "Save model 1332\n",
      "Train Epoch:1333 learning rate:1.0000e-05, Loss_tot:0.3813,\n",
      "Save model 1333\n",
      "Train Epoch:1334 learning rate:1.0000e-05, Loss_tot:0.3819,\n",
      "Save model 1334\n",
      "Train Epoch:1335 learning rate:1.0000e-05, Loss_tot:0.3819,\n",
      "Save model 1335\n",
      "Train Epoch:1336 learning rate:1.0000e-05, Loss_tot:0.3806,\n",
      "Save model 1336\n",
      "Train Epoch:1337 learning rate:1.0000e-05, Loss_tot:0.3818,\n",
      "Save model 1337\n",
      "Train Epoch:1338 learning rate:1.0000e-05, Loss_tot:0.3817,\n",
      "Save model 1338\n",
      "Train Epoch:1339 learning rate:1.0000e-05, Loss_tot:0.3817,\n",
      "Save model 1339\n",
      "Train Epoch:1340 learning rate:1.0000e-05, Loss_tot:0.3816,\n",
      "Save model 1340\n",
      "Train Epoch:1341 learning rate:1.0000e-05, Loss_tot:0.3815,\n",
      "Save model 1341\n",
      "Train Epoch:1342 learning rate:1.0000e-05, Loss_tot:0.3815,\n",
      "Save model 1342\n",
      "Train Epoch:1343 learning rate:1.0000e-05, Loss_tot:0.3814,\n",
      "Save model 1343\n",
      "Train Epoch:1344 learning rate:1.0000e-05, Loss_tot:0.3813,\n",
      "Save model 1344\n",
      "Train Epoch:1345 learning rate:1.0000e-05, Loss_tot:0.3813,\n",
      "Save model 1345\n",
      "Train Epoch:1346 learning rate:1.0000e-05, Loss_tot:0.3812,\n",
      "Save model 1346\n",
      "Train Epoch:1347 learning rate:1.0000e-05, Loss_tot:0.3812,\n",
      "Save model 1347\n",
      "Train Epoch:1348 learning rate:1.0000e-05, Loss_tot:0.3811,\n",
      "Save model 1348\n",
      "Train Epoch:1349 learning rate:1.0000e-05, Loss_tot:0.3809,\n",
      "Save model 1349\n",
      "Train Epoch:1350 learning rate:1.0000e-05, Loss_tot:0.3810,\n",
      "Save model 1350\n",
      "Train Epoch:1351 learning rate:1.0000e-05, Loss_tot:0.3809,\n",
      "Save model 1351\n",
      "Train Epoch:1352 learning rate:1.0000e-05, Loss_tot:0.3808,\n",
      "Save model 1352\n",
      "Train Epoch:1353 learning rate:1.0000e-05, Loss_tot:0.3807,\n",
      "Save model 1353\n",
      "Train Epoch:1354 learning rate:1.0000e-05, Loss_tot:0.3806,\n",
      "Save model 1354\n",
      "Train Epoch:1355 learning rate:1.0000e-05, Loss_tot:0.3806,\n",
      "Save model 1355\n",
      "Train Epoch:1356 learning rate:1.0000e-05, Loss_tot:0.3806,\n",
      "Save model 1356\n",
      "Train Epoch:1357 learning rate:1.0000e-05, Loss_tot:0.3805,\n",
      "Save model 1357\n",
      "Train Epoch:1358 learning rate:1.0000e-05, Loss_tot:0.3804,\n",
      "Save model 1358\n",
      "Train Epoch:1359 learning rate:1.0000e-05, Loss_tot:0.3800,\n",
      "Save model 1359\n",
      "Train Epoch:1360 learning rate:1.0000e-05, Loss_tot:0.3803,\n",
      "Save model 1360\n",
      "Train Epoch:1361 learning rate:1.0000e-05, Loss_tot:0.3802,\n",
      "Save model 1361\n",
      "Train Epoch:1362 learning rate:1.0000e-05, Loss_tot:0.3791,\n",
      "Save model 1362\n",
      "Train Epoch:1363 learning rate:1.0000e-05, Loss_tot:0.3800,\n",
      "Save model 1363\n",
      "Train Epoch:1364 learning rate:1.0000e-05, Loss_tot:0.3776,\n",
      "Save model 1364\n",
      "Train Epoch:1365 learning rate:1.0000e-05, Loss_tot:0.3799,\n",
      "Save model 1365\n",
      "Train Epoch:1366 learning rate:1.0000e-05, Loss_tot:0.3799,\n",
      "Save model 1366\n",
      "Train Epoch:1367 learning rate:1.0000e-05, Loss_tot:0.3798,\n",
      "Save model 1367\n",
      "Train Epoch:1368 learning rate:1.0000e-05, Loss_tot:0.3797,\n",
      "Save model 1368\n",
      "Train Epoch:1369 learning rate:1.0000e-05, Loss_tot:0.3796,\n",
      "Save model 1369\n",
      "Train Epoch:1370 learning rate:1.0000e-05, Loss_tot:0.3795,\n",
      "Save model 1370\n",
      "Train Epoch:1371 learning rate:1.0000e-05, Loss_tot:0.3794,\n",
      "Save model 1371\n",
      "Train Epoch:1372 learning rate:1.0000e-05, Loss_tot:0.3794,\n",
      "Save model 1372\n",
      "Train Epoch:1373 learning rate:1.0000e-05, Loss_tot:0.3794,\n",
      "Save model 1373\n",
      "Train Epoch:1374 learning rate:1.0000e-05, Loss_tot:0.3787,\n",
      "Save model 1374\n",
      "Train Epoch:1375 learning rate:1.0000e-05, Loss_tot:0.3792,\n",
      "Save model 1375\n",
      "Train Epoch:1376 learning rate:1.0000e-05, Loss_tot:0.3791,\n",
      "Save model 1376\n",
      "Train Epoch:1377 learning rate:1.0000e-05, Loss_tot:0.3790,\n",
      "Save model 1377\n",
      "Train Epoch:1378 learning rate:1.0000e-05, Loss_tot:0.3790,\n",
      "Save model 1378\n",
      "Train Epoch:1379 learning rate:1.0000e-05, Loss_tot:0.3789,\n",
      "Save model 1379\n",
      "Train Epoch:1380 learning rate:1.0000e-05, Loss_tot:0.3788,\n",
      "Save model 1380\n",
      "Train Epoch:1381 learning rate:1.0000e-05, Loss_tot:0.3785,\n",
      "Save model 1381\n",
      "Train Epoch:1382 learning rate:1.0000e-05, Loss_tot:0.3787,\n",
      "Save model 1382\n",
      "Train Epoch:1383 learning rate:1.0000e-05, Loss_tot:0.3786,\n",
      "Save model 1383\n",
      "Train Epoch:1384 learning rate:1.0000e-05, Loss_tot:0.3786,\n",
      "Save model 1384\n",
      "Train Epoch:1385 learning rate:1.0000e-05, Loss_tot:0.3774,\n",
      "Save model 1385\n",
      "Train Epoch:1386 learning rate:1.0000e-05, Loss_tot:0.3784,\n",
      "Save model 1386\n",
      "Train Epoch:1387 learning rate:1.0000e-05, Loss_tot:0.3783,\n",
      "Save model 1387\n",
      "Train Epoch:1388 learning rate:1.0000e-05, Loss_tot:0.3783,\n",
      "Save model 1388\n",
      "Train Epoch:1389 learning rate:1.0000e-05, Loss_tot:0.3782,\n",
      "Save model 1389\n",
      "Train Epoch:1390 learning rate:1.0000e-05, Loss_tot:0.3781,\n",
      "Save model 1390\n",
      "Train Epoch:1391 learning rate:1.0000e-05, Loss_tot:0.3781,\n",
      "Save model 1391\n",
      "Train Epoch:1392 learning rate:1.0000e-05, Loss_tot:0.3780,\n",
      "Save model 1392\n",
      "Train Epoch:1393 learning rate:1.0000e-05, Loss_tot:0.3779,\n",
      "Save model 1393\n",
      "Train Epoch:1394 learning rate:1.0000e-05, Loss_tot:0.3756,\n",
      "Save model 1394\n",
      "Train Epoch:1395 learning rate:1.0000e-05, Loss_tot:0.3778,\n",
      "Save model 1395\n",
      "Train Epoch:1396 learning rate:1.0000e-05, Loss_tot:0.3777,\n",
      "Save model 1396\n",
      "Train Epoch:1397 learning rate:1.0000e-05, Loss_tot:0.3777,\n",
      "Save model 1397\n",
      "Train Epoch:1398 learning rate:1.0000e-05, Loss_tot:0.3776,\n",
      "Save model 1398\n",
      "Train Epoch:1399 learning rate:1.0000e-05, Loss_tot:0.3776,\n",
      "Save model 1399\n",
      "Train Epoch:1400 learning rate:1.0000e-05, Loss_tot:0.3775,\n",
      "Save model 1400\n",
      "Train Epoch:1401 learning rate:1.0000e-05, Loss_tot:0.3774,\n",
      "Save model 1401\n",
      "Train Epoch:1402 learning rate:1.0000e-05, Loss_tot:0.3773,\n",
      "Save model 1402\n",
      "Train Epoch:1403 learning rate:1.0000e-05, Loss_tot:0.3772,\n",
      "Save model 1403\n",
      "Train Epoch:1404 learning rate:1.0000e-05, Loss_tot:0.3770,\n",
      "Save model 1404\n",
      "Train Epoch:1405 learning rate:1.0000e-05, Loss_tot:0.3772,\n",
      "Save model 1405\n",
      "Train Epoch:1406 learning rate:1.0000e-05, Loss_tot:0.3771,\n",
      "Save model 1406\n",
      "Train Epoch:1407 learning rate:1.0000e-05, Loss_tot:0.3770,\n",
      "Save model 1407\n",
      "Train Epoch:1408 learning rate:1.0000e-05, Loss_tot:0.3769,\n",
      "Save model 1408\n",
      "Train Epoch:1409 learning rate:1.0000e-05, Loss_tot:0.3768,\n",
      "Save model 1409\n",
      "Train Epoch:1410 learning rate:1.0000e-05, Loss_tot:0.3768,\n",
      "Save model 1410\n",
      "Train Epoch:1411 learning rate:1.0000e-05, Loss_tot:0.3756,\n",
      "Save model 1411\n",
      "Train Epoch:1412 learning rate:1.0000e-05, Loss_tot:0.3766,\n",
      "Save model 1412\n",
      "Train Epoch:1413 learning rate:1.0000e-05, Loss_tot:0.3759,\n",
      "Save model 1413\n",
      "Train Epoch:1414 learning rate:1.0000e-05, Loss_tot:0.3765,\n",
      "Save model 1414\n",
      "Train Epoch:1415 learning rate:1.0000e-05, Loss_tot:0.3764,\n",
      "Save model 1415\n",
      "Train Epoch:1416 learning rate:1.0000e-05, Loss_tot:0.3763,\n",
      "Save model 1416\n",
      "Train Epoch:1417 learning rate:1.0000e-05, Loss_tot:0.3763,\n",
      "Save model 1417\n",
      "Train Epoch:1418 learning rate:1.0000e-05, Loss_tot:0.3762,\n",
      "Save model 1418\n",
      "Train Epoch:1419 learning rate:1.0000e-05, Loss_tot:0.3761,\n",
      "Save model 1419\n",
      "Train Epoch:1420 learning rate:1.0000e-05, Loss_tot:0.3761,\n",
      "Save model 1420\n",
      "Train Epoch:1421 learning rate:1.0000e-05, Loss_tot:0.3760,\n",
      "Save model 1421\n",
      "Train Epoch:1422 learning rate:1.0000e-05, Loss_tot:0.3759,\n",
      "Save model 1422\n",
      "Train Epoch:1423 learning rate:1.0000e-05, Loss_tot:0.3737,\n",
      "Save model 1423\n",
      "Train Epoch:1424 learning rate:1.0000e-05, Loss_tot:0.3758,\n",
      "Save model 1424\n",
      "Train Epoch:1425 learning rate:1.0000e-05, Loss_tot:0.3757,\n",
      "Save model 1425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1426 learning rate:1.0000e-05, Loss_tot:0.3756,\n",
      "Save model 1426\n",
      "Train Epoch:1427 learning rate:1.0000e-05, Loss_tot:0.3755,\n",
      "Save model 1427\n",
      "Train Epoch:1428 learning rate:1.0000e-05, Loss_tot:0.3753,\n",
      "Save model 1428\n",
      "Train Epoch:1429 learning rate:1.0000e-05, Loss_tot:0.3754,\n",
      "Save model 1429\n",
      "Train Epoch:1430 learning rate:1.0000e-05, Loss_tot:0.3753,\n",
      "Save model 1430\n",
      "Train Epoch:1431 learning rate:1.0000e-05, Loss_tot:0.3753,\n",
      "Save model 1431\n",
      "Train Epoch:1432 learning rate:1.0000e-05, Loss_tot:0.3752,\n",
      "Save model 1432\n",
      "Train Epoch:1433 learning rate:1.0000e-05, Loss_tot:0.3740,\n",
      "Save model 1433\n",
      "Train Epoch:1434 learning rate:1.0000e-05, Loss_tot:0.3751,\n",
      "Save model 1434\n",
      "Train Epoch:1435 learning rate:1.0000e-05, Loss_tot:0.3750,\n",
      "Save model 1435\n",
      "Train Epoch:1436 learning rate:1.0000e-05, Loss_tot:0.3749,\n",
      "Save model 1436\n",
      "Train Epoch:1437 learning rate:1.0000e-05, Loss_tot:0.3748,\n",
      "Save model 1437\n",
      "Train Epoch:1438 learning rate:1.0000e-05, Loss_tot:0.3748,\n",
      "Save model 1438\n",
      "Train Epoch:1439 learning rate:1.0000e-05, Loss_tot:0.3747,\n",
      "Save model 1439\n",
      "Train Epoch:1440 learning rate:1.0000e-05, Loss_tot:0.3746,\n",
      "Save model 1440\n",
      "Train Epoch:1441 learning rate:1.0000e-05, Loss_tot:0.3746,\n",
      "Save model 1441\n",
      "Train Epoch:1442 learning rate:1.0000e-05, Loss_tot:0.3745,\n",
      "Save model 1442\n",
      "Train Epoch:1443 learning rate:1.0000e-05, Loss_tot:0.3744,\n",
      "Save model 1443\n",
      "Train Epoch:1444 learning rate:1.0000e-05, Loss_tot:0.3738,\n",
      "Save model 1444\n",
      "Train Epoch:1445 learning rate:1.0000e-05, Loss_tot:0.3743,\n",
      "Save model 1445\n",
      "Train Epoch:1446 learning rate:1.0000e-05, Loss_tot:0.3742,\n",
      "Save model 1446\n",
      "Train Epoch:1447 learning rate:1.0000e-05, Loss_tot:0.3741,\n",
      "Save model 1447\n",
      "Train Epoch:1448 learning rate:1.0000e-05, Loss_tot:0.3741,\n",
      "Save model 1448\n",
      "Train Epoch:1449 learning rate:1.0000e-05, Loss_tot:0.3740,\n",
      "Save model 1449\n",
      "Train Epoch:1450 learning rate:1.0000e-05, Loss_tot:0.3731,\n",
      "Save model 1450\n",
      "Train Epoch:1451 learning rate:1.0000e-05, Loss_tot:0.3738,\n",
      "Save model 1451\n",
      "Train Epoch:1452 learning rate:1.0000e-05, Loss_tot:0.3718,\n",
      "Save model 1452\n",
      "Train Epoch:1453 learning rate:1.0000e-05, Loss_tot:0.3737,\n",
      "Save model 1453\n",
      "Train Epoch:1454 learning rate:1.0000e-05, Loss_tot:0.3736,\n",
      "Save model 1454\n",
      "Train Epoch:1455 learning rate:1.0000e-05, Loss_tot:0.3735,\n",
      "Save model 1455\n",
      "Train Epoch:1456 learning rate:1.0000e-05, Loss_tot:0.3735,\n",
      "Save model 1456\n",
      "Train Epoch:1457 learning rate:1.0000e-05, Loss_tot:0.3734,\n",
      "Save model 1457\n",
      "Train Epoch:1458 learning rate:1.0000e-05, Loss_tot:0.3734,\n",
      "Save model 1458\n",
      "Train Epoch:1459 learning rate:1.0000e-05, Loss_tot:0.3722,\n",
      "Save model 1459\n",
      "Train Epoch:1460 learning rate:1.0000e-05, Loss_tot:0.3732,\n",
      "Save model 1460\n",
      "Train Epoch:1461 learning rate:1.0000e-05, Loss_tot:0.3731,\n",
      "Save model 1461\n",
      "Train Epoch:1462 learning rate:1.0000e-05, Loss_tot:0.3731,\n",
      "Save model 1462\n",
      "Train Epoch:1463 learning rate:1.0000e-05, Loss_tot:0.3730,\n",
      "Save model 1463\n",
      "Train Epoch:1464 learning rate:1.0000e-05, Loss_tot:0.3729,\n",
      "Save model 1464\n",
      "Train Epoch:1465 learning rate:1.0000e-05, Loss_tot:0.3728,\n",
      "Save model 1465\n",
      "Train Epoch:1466 learning rate:1.0000e-05, Loss_tot:0.3728,\n",
      "Save model 1466\n",
      "Train Epoch:1467 learning rate:1.0000e-05, Loss_tot:0.3727,\n",
      "Save model 1467\n",
      "Train Epoch:1468 learning rate:1.0000e-05, Loss_tot:0.3726,\n",
      "Save model 1468\n",
      "Train Epoch:1469 learning rate:1.0000e-05, Loss_tot:0.3726,\n",
      "Save model 1469\n",
      "Train Epoch:1470 learning rate:1.0000e-05, Loss_tot:0.3725,\n",
      "Save model 1470\n",
      "Train Epoch:1471 learning rate:1.0000e-05, Loss_tot:0.3722,\n",
      "Save model 1471\n",
      "Train Epoch:1472 learning rate:1.0000e-05, Loss_tot:0.3723,\n",
      "Save model 1472\n",
      "Train Epoch:1473 learning rate:1.0000e-05, Loss_tot:0.3723,\n",
      "Save model 1473\n",
      "Train Epoch:1474 learning rate:1.0000e-05, Loss_tot:0.3722,\n",
      "Save model 1474\n",
      "Train Epoch:1475 learning rate:1.0000e-05, Loss_tot:0.3701,\n",
      "Save model 1475\n",
      "Train Epoch:1476 learning rate:1.0000e-05, Loss_tot:0.3720,\n",
      "Save model 1476\n",
      "Train Epoch:1477 learning rate:1.0000e-05, Loss_tot:0.3719,\n",
      "Save model 1477\n",
      "Train Epoch:1478 learning rate:1.0000e-05, Loss_tot:0.3719,\n",
      "Save model 1478\n",
      "Train Epoch:1479 learning rate:1.0000e-05, Loss_tot:0.3718,\n",
      "Save model 1479\n",
      "Train Epoch:1480 learning rate:1.0000e-05, Loss_tot:0.3718,\n",
      "Save model 1480\n",
      "Train Epoch:1481 learning rate:1.0000e-05, Loss_tot:0.3717,\n",
      "Save model 1481\n",
      "Train Epoch:1482 learning rate:1.0000e-05, Loss_tot:0.3706,\n",
      "Save model 1482\n",
      "Train Epoch:1483 learning rate:1.0000e-05, Loss_tot:0.3716,\n",
      "Save model 1483\n",
      "Train Epoch:1484 learning rate:1.0000e-05, Loss_tot:0.3715,\n",
      "Save model 1484\n",
      "Train Epoch:1485 learning rate:1.0000e-05, Loss_tot:0.3714,\n",
      "Save model 1485\n",
      "Train Epoch:1486 learning rate:1.0000e-05, Loss_tot:0.3713,\n",
      "Save model 1486\n",
      "Train Epoch:1487 learning rate:1.0000e-05, Loss_tot:0.3707,\n",
      "Save model 1487\n",
      "Train Epoch:1488 learning rate:1.0000e-05, Loss_tot:0.3712,\n",
      "Save model 1488\n",
      "Train Epoch:1489 learning rate:1.0000e-05, Loss_tot:0.3711,\n",
      "Save model 1489\n",
      "Train Epoch:1490 learning rate:1.0000e-05, Loss_tot:0.3710,\n",
      "Save model 1490\n",
      "Train Epoch:1491 learning rate:1.0000e-05, Loss_tot:0.3709,\n",
      "Save model 1491\n",
      "Train Epoch:1492 learning rate:1.0000e-05, Loss_tot:0.3706,\n",
      "Save model 1492\n",
      "Train Epoch:1493 learning rate:1.0000e-05, Loss_tot:0.3708,\n",
      "Save model 1493\n",
      "Train Epoch:1494 learning rate:1.0000e-05, Loss_tot:0.3707,\n",
      "Save model 1494\n",
      "Train Epoch:1495 learning rate:1.0000e-05, Loss_tot:0.3707,\n",
      "Save model 1495\n",
      "Train Epoch:1496 learning rate:1.0000e-05, Loss_tot:0.3706,\n",
      "Save model 1496\n",
      "Train Epoch:1497 learning rate:1.0000e-05, Loss_tot:0.3705,\n",
      "Save model 1497\n",
      "Train Epoch:1498 learning rate:1.0000e-05, Loss_tot:0.3705,\n",
      "Save model 1498\n",
      "Train Epoch:1499 learning rate:1.0000e-05, Loss_tot:0.3704,\n",
      "Save model 1499\n",
      "Train Epoch:1500 learning rate:1.0000e-05, Loss_tot:0.3703,\n",
      "Save model 1500\n",
      "Train Epoch:1501 learning rate:1.0000e-05, Loss_tot:0.3702,\n",
      "Save model 1501\n",
      "Train Epoch:1502 learning rate:1.0000e-05, Loss_tot:0.3684,\n",
      "Save model 1502\n",
      "Train Epoch:1503 learning rate:1.0000e-05, Loss_tot:0.3701,\n",
      "Save model 1503\n",
      "Train Epoch:1504 learning rate:1.0000e-05, Loss_tot:0.3700,\n",
      "Save model 1504\n",
      "Train Epoch:1505 learning rate:1.0000e-05, Loss_tot:0.3699,\n",
      "Save model 1505\n",
      "Train Epoch:1506 learning rate:1.0000e-05, Loss_tot:0.3688,\n",
      "Save model 1506\n",
      "Train Epoch:1507 learning rate:1.0000e-05, Loss_tot:0.3698,\n",
      "Save model 1507\n",
      "Train Epoch:1508 learning rate:1.0000e-05, Loss_tot:0.3697,\n",
      "Save model 1508\n",
      "Train Epoch:1509 learning rate:1.0000e-05, Loss_tot:0.3696,\n",
      "Save model 1509\n",
      "Train Epoch:1510 learning rate:1.0000e-05, Loss_tot:0.3695,\n",
      "Save model 1510\n",
      "Train Epoch:1511 learning rate:1.0000e-05, Loss_tot:0.3695,\n",
      "Save model 1511\n",
      "Train Epoch:1512 learning rate:1.0000e-05, Loss_tot:0.3694,\n",
      "Save model 1512\n",
      "Train Epoch:1513 learning rate:1.0000e-05, Loss_tot:0.3691,\n",
      "Save model 1513\n",
      "Train Epoch:1514 learning rate:1.0000e-05, Loss_tot:0.3692,\n",
      "Save model 1514\n",
      "Train Epoch:1515 learning rate:1.0000e-05, Loss_tot:0.3691,\n",
      "Save model 1515\n",
      "Train Epoch:1516 learning rate:1.0000e-05, Loss_tot:0.3691,\n",
      "Save model 1516\n",
      "Train Epoch:1517 learning rate:1.0000e-05, Loss_tot:0.3690,\n",
      "Save model 1517\n",
      "Train Epoch:1518 learning rate:1.0000e-05, Loss_tot:0.3689,\n",
      "Save model 1518\n",
      "Train Epoch:1519 learning rate:1.0000e-05, Loss_tot:0.3683,\n",
      "Save model 1519\n",
      "Train Epoch:1520 learning rate:1.0000e-05, Loss_tot:0.3687,\n",
      "Save model 1520\n",
      "Train Epoch:1521 learning rate:1.0000e-05, Loss_tot:0.3687,\n",
      "Save model 1521\n",
      "Train Epoch:1522 learning rate:1.0000e-05, Loss_tot:0.3686,\n",
      "Save model 1522\n",
      "Train Epoch:1523 learning rate:1.0000e-05, Loss_tot:0.3685,\n",
      "Save model 1523\n",
      "Train Epoch:1524 learning rate:1.0000e-05, Loss_tot:0.3684,\n",
      "Save model 1524\n",
      "Train Epoch:1525 learning rate:1.0000e-05, Loss_tot:0.3667,\n",
      "Save model 1525\n",
      "Train Epoch:1526 learning rate:1.0000e-05, Loss_tot:0.3683,\n",
      "Save model 1526\n",
      "Train Epoch:1527 learning rate:1.0000e-05, Loss_tot:0.3682,\n",
      "Save model 1527\n",
      "Train Epoch:1528 learning rate:1.0000e-05, Loss_tot:0.3670,\n",
      "Save model 1528\n",
      "Train Epoch:1529 learning rate:1.0000e-05, Loss_tot:0.3681,\n",
      "Save model 1529\n",
      "Train Epoch:1530 learning rate:1.0000e-05, Loss_tot:0.3680,\n",
      "Save model 1530\n",
      "Train Epoch:1531 learning rate:1.0000e-05, Loss_tot:0.3679,\n",
      "Save model 1531\n",
      "Train Epoch:1532 learning rate:1.0000e-05, Loss_tot:0.3679,\n",
      "Save model 1532\n",
      "Train Epoch:1533 learning rate:1.0000e-05, Loss_tot:0.3676,\n",
      "Save model 1533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1534 learning rate:1.0000e-05, Loss_tot:0.3678,\n",
      "Save model 1534\n",
      "Train Epoch:1535 learning rate:1.0000e-05, Loss_tot:0.3677,\n",
      "Save model 1535\n",
      "Train Epoch:1536 learning rate:1.0000e-05, Loss_tot:0.3675,\n",
      "Save model 1536\n",
      "Train Epoch:1537 learning rate:1.0000e-05, Loss_tot:0.3674,\n",
      "Save model 1537\n",
      "Train Epoch:1538 learning rate:1.0000e-05, Loss_tot:0.3674,\n",
      "Save model 1538\n",
      "Train Epoch:1539 learning rate:1.0000e-05, Loss_tot:0.3673,\n",
      "Save model 1539\n",
      "Train Epoch:1540 learning rate:1.0000e-05, Loss_tot:0.3672,\n",
      "Save model 1540\n",
      "Train Epoch:1541 learning rate:1.0000e-05, Loss_tot:0.3672,\n",
      "Save model 1541\n",
      "Train Epoch:1542 learning rate:1.0000e-05, Loss_tot:0.3671,\n",
      "Save model 1542\n",
      "Train Epoch:1543 learning rate:1.0000e-05, Loss_tot:0.3670,\n",
      "Save model 1543\n",
      "Train Epoch:1544 learning rate:1.0000e-05, Loss_tot:0.3669,\n",
      "Save model 1544\n",
      "Train Epoch:1545 learning rate:1.0000e-05, Loss_tot:0.3668,\n",
      "Save model 1545\n",
      "Train Epoch:1546 learning rate:1.0000e-05, Loss_tot:0.3651,\n",
      "Save model 1546\n",
      "Train Epoch:1547 learning rate:1.0000e-05, Loss_tot:0.3666,\n",
      "Save model 1547\n",
      "Train Epoch:1548 learning rate:1.0000e-05, Loss_tot:0.3666,\n",
      "Save model 1548\n",
      "Train Epoch:1549 learning rate:1.0000e-05, Loss_tot:0.3665,\n",
      "Save model 1549\n",
      "Train Epoch:1550 learning rate:1.0000e-05, Loss_tot:0.3664,\n",
      "Save model 1550\n",
      "Train Epoch:1551 learning rate:1.0000e-05, Loss_tot:0.3659,\n",
      "Save model 1551\n",
      "Train Epoch:1552 learning rate:1.0000e-05, Loss_tot:0.3662,\n",
      "Save model 1552\n",
      "Train Epoch:1553 learning rate:1.0000e-05, Loss_tot:0.3650,\n",
      "Save model 1553\n",
      "Train Epoch:1554 learning rate:1.0000e-05, Loss_tot:0.3661,\n",
      "Save model 1554\n",
      "Train Epoch:1555 learning rate:1.0000e-05, Loss_tot:0.3660,\n",
      "Save model 1555\n",
      "Train Epoch:1556 learning rate:1.0000e-05, Loss_tot:0.3657,\n",
      "Save model 1556\n",
      "Train Epoch:1557 learning rate:1.0000e-05, Loss_tot:0.3658,\n",
      "Save model 1557\n",
      "Train Epoch:1558 learning rate:1.0000e-05, Loss_tot:0.3657,\n",
      "Save model 1558\n",
      "Train Epoch:1559 learning rate:1.0000e-05, Loss_tot:0.3657,\n",
      "Save model 1559\n",
      "Train Epoch:1560 learning rate:1.0000e-05, Loss_tot:0.3656,\n",
      "Save model 1560\n",
      "Train Epoch:1561 learning rate:1.0000e-05, Loss_tot:0.3655,\n",
      "Save model 1561\n",
      "Train Epoch:1562 learning rate:1.0000e-05, Loss_tot:0.3654,\n",
      "Save model 1562\n",
      "Train Epoch:1563 learning rate:1.0000e-05, Loss_tot:0.3653,\n",
      "Save model 1563\n",
      "Train Epoch:1564 learning rate:1.0000e-05, Loss_tot:0.3637,\n",
      "Save model 1564\n",
      "Train Epoch:1565 learning rate:1.0000e-05, Loss_tot:0.3652,\n",
      "Save model 1565\n",
      "Train Epoch:1566 learning rate:1.0000e-05, Loss_tot:0.3652,\n",
      "Save model 1566\n",
      "Train Epoch:1567 learning rate:1.0000e-05, Loss_tot:0.3651,\n",
      "Save model 1567\n",
      "Train Epoch:1568 learning rate:1.0000e-05, Loss_tot:0.3650,\n",
      "Save model 1568\n",
      "Train Epoch:1569 learning rate:1.0000e-05, Loss_tot:0.3649,\n",
      "Save model 1569\n",
      "Train Epoch:1570 learning rate:1.0000e-05, Loss_tot:0.3648,\n",
      "Save model 1570\n",
      "Train Epoch:1571 learning rate:1.0000e-05, Loss_tot:0.3647,\n",
      "Save model 1571\n",
      "Train Epoch:1572 learning rate:1.0000e-05, Loss_tot:0.3646,\n",
      "Save model 1572\n",
      "Train Epoch:1573 learning rate:1.0000e-05, Loss_tot:0.3635,\n",
      "Save model 1573\n",
      "Train Epoch:1574 learning rate:1.0000e-05, Loss_tot:0.3645,\n",
      "Save model 1574\n",
      "Train Epoch:1575 learning rate:1.0000e-05, Loss_tot:0.3644,\n",
      "Save model 1575\n",
      "Train Epoch:1576 learning rate:1.0000e-05, Loss_tot:0.3643,\n",
      "Save model 1576\n",
      "Train Epoch:1577 learning rate:1.0000e-05, Loss_tot:0.3642,\n",
      "Save model 1577\n",
      "Train Epoch:1578 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "Save model 1578\n",
      "Train Epoch:1579 learning rate:1.0000e-05, Loss_tot:0.3640,\n",
      "Save model 1579\n",
      "Train Epoch:1580 learning rate:1.0000e-05, Loss_tot:0.3640,\n",
      "Save model 1580\n",
      "Train Epoch:1581 learning rate:1.0000e-05, Loss_tot:0.3639,\n",
      "Save model 1581\n",
      "Train Epoch:1582 learning rate:1.0000e-05, Loss_tot:0.3638,\n",
      "Save model 1582\n",
      "Train Epoch:1583 learning rate:1.0000e-05, Loss_tot:0.3637,\n",
      "Save model 1583\n",
      "Train Epoch:1584 learning rate:1.0000e-05, Loss_tot:0.3636,\n",
      "Save model 1584\n",
      "Train Epoch:1585 learning rate:1.0000e-05, Loss_tot:0.3617,\n",
      "Save model 1585\n",
      "Train Epoch:1586 learning rate:1.0000e-05, Loss_tot:0.3634,\n",
      "Save model 1586\n",
      "Train Epoch:1587 learning rate:1.0000e-05, Loss_tot:0.3634,\n",
      "Save model 1587\n",
      "Train Epoch:1588 learning rate:1.0000e-05, Loss_tot:0.3634,\n",
      "Save model 1588\n",
      "Train Epoch:1589 learning rate:1.0000e-05, Loss_tot:0.3633,\n",
      "Save model 1589\n",
      "Train Epoch:1590 learning rate:1.0000e-05, Loss_tot:0.3632,\n",
      "Save model 1590\n",
      "Train Epoch:1591 learning rate:1.0000e-05, Loss_tot:0.3631,\n",
      "Save model 1591\n",
      "Train Epoch:1592 learning rate:1.0000e-05, Loss_tot:0.3629,\n",
      "Save model 1592\n",
      "Train Epoch:1593 learning rate:1.0000e-05, Loss_tot:0.3629,\n",
      "Save model 1593\n",
      "Train Epoch:1594 learning rate:1.0000e-05, Loss_tot:0.3628,\n",
      "Save model 1594\n",
      "Train Epoch:1595 learning rate:1.0000e-05, Loss_tot:0.3627,\n",
      "Save model 1595\n",
      "Train Epoch:1596 learning rate:1.0000e-05, Loss_tot:0.3615,\n",
      "Save model 1596\n",
      "Train Epoch:1597 learning rate:1.0000e-05, Loss_tot:0.3625,\n",
      "Save model 1597\n",
      "Train Epoch:1598 learning rate:1.0000e-05, Loss_tot:0.3624,\n",
      "Save model 1598\n",
      "Train Epoch:1599 learning rate:1.0000e-05, Loss_tot:0.3621,\n",
      "Save model 1599\n",
      "Train Epoch:1600 learning rate:1.0000e-05, Loss_tot:0.3609,\n",
      "Save model 1600\n",
      "Train Epoch:1601 learning rate:1.0000e-05, Loss_tot:0.3622,\n",
      "Save model 1601\n",
      "Train Epoch:1602 learning rate:1.0000e-05, Loss_tot:0.3617,\n",
      "Save model 1602\n",
      "Train Epoch:1603 learning rate:1.0000e-05, Loss_tot:0.3620,\n",
      "Save model 1603\n",
      "Train Epoch:1604 learning rate:1.0000e-05, Loss_tot:0.3620,\n",
      "Save model 1604\n",
      "Train Epoch:1605 learning rate:1.0000e-05, Loss_tot:0.3619,\n",
      "Save model 1605\n",
      "Train Epoch:1606 learning rate:1.0000e-05, Loss_tot:0.3618,\n",
      "Save model 1606\n",
      "Train Epoch:1607 learning rate:1.0000e-05, Loss_tot:0.3617,\n",
      "Save model 1607\n",
      "Train Epoch:1608 learning rate:1.0000e-05, Loss_tot:0.3616,\n",
      "Save model 1608\n",
      "Train Epoch:1609 learning rate:1.0000e-05, Loss_tot:0.3616,\n",
      "Save model 1609\n",
      "Train Epoch:1610 learning rate:1.0000e-05, Loss_tot:0.3614,\n",
      "Save model 1610\n",
      "Train Epoch:1611 learning rate:1.0000e-05, Loss_tot:0.3610,\n",
      "Save model 1611\n",
      "Train Epoch:1612 learning rate:1.0000e-05, Loss_tot:0.3613,\n",
      "Save model 1612\n",
      "Train Epoch:1613 learning rate:1.0000e-05, Loss_tot:0.3612,\n",
      "Save model 1613\n",
      "Train Epoch:1614 learning rate:1.0000e-05, Loss_tot:0.3611,\n",
      "Save model 1614\n",
      "Train Epoch:1615 learning rate:1.0000e-05, Loss_tot:0.3610,\n",
      "Save model 1615\n",
      "Train Epoch:1616 learning rate:1.0000e-05, Loss_tot:0.3609,\n",
      "Save model 1616\n",
      "Train Epoch:1617 learning rate:1.0000e-05, Loss_tot:0.3582,\n",
      "Save model 1617\n",
      "Train Epoch:1618 learning rate:1.0000e-05, Loss_tot:0.3607,\n",
      "Save model 1618\n",
      "Train Epoch:1619 learning rate:1.0000e-05, Loss_tot:0.3606,\n",
      "Save model 1619\n",
      "Train Epoch:1620 learning rate:1.0000e-05, Loss_tot:0.3605,\n",
      "Save model 1620\n",
      "Train Epoch:1621 learning rate:1.0000e-05, Loss_tot:0.3602,\n",
      "Save model 1621\n",
      "Train Epoch:1622 learning rate:1.0000e-05, Loss_tot:0.3604,\n",
      "Save model 1622\n",
      "Train Epoch:1623 learning rate:1.0000e-05, Loss_tot:0.3603,\n",
      "Save model 1623\n",
      "Train Epoch:1624 learning rate:1.0000e-05, Loss_tot:0.3602,\n",
      "Save model 1624\n",
      "Train Epoch:1625 learning rate:1.0000e-05, Loss_tot:0.3601,\n",
      "Save model 1625\n",
      "Train Epoch:1626 learning rate:1.0000e-05, Loss_tot:0.3601,\n",
      "Save model 1626\n",
      "Train Epoch:1627 learning rate:1.0000e-05, Loss_tot:0.3600,\n",
      "Save model 1627\n",
      "Train Epoch:1628 learning rate:1.0000e-05, Loss_tot:0.3599,\n",
      "Save model 1628\n",
      "Train Epoch:1629 learning rate:1.0000e-05, Loss_tot:0.3597,\n",
      "Save model 1629\n",
      "Train Epoch:1630 learning rate:1.0000e-05, Loss_tot:0.3585,\n",
      "Save model 1630\n",
      "Train Epoch:1631 learning rate:1.0000e-05, Loss_tot:0.3597,\n",
      "Save model 1631\n",
      "Train Epoch:1632 learning rate:1.0000e-05, Loss_tot:0.3596,\n",
      "Save model 1632\n",
      "Train Epoch:1633 learning rate:1.0000e-05, Loss_tot:0.3594,\n",
      "Save model 1633\n",
      "Train Epoch:1634 learning rate:1.0000e-05, Loss_tot:0.3594,\n",
      "Save model 1634\n",
      "Train Epoch:1635 learning rate:1.0000e-05, Loss_tot:0.3594,\n",
      "Save model 1635\n",
      "Train Epoch:1636 learning rate:1.0000e-05, Loss_tot:0.3593,\n",
      "Save model 1636\n",
      "Train Epoch:1637 learning rate:1.0000e-05, Loss_tot:0.3592,\n",
      "Save model 1637\n",
      "Train Epoch:1638 learning rate:1.0000e-05, Loss_tot:0.3591,\n",
      "Save model 1638\n",
      "Train Epoch:1639 learning rate:1.0000e-05, Loss_tot:0.3590,\n",
      "Save model 1639\n",
      "Train Epoch:1640 learning rate:1.0000e-05, Loss_tot:0.3588,\n",
      "Save model 1640\n",
      "Train Epoch:1641 learning rate:1.0000e-05, Loss_tot:0.3587,\n",
      "Save model 1641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1642 learning rate:1.0000e-05, Loss_tot:0.3575,\n",
      "Save model 1642\n",
      "Train Epoch:1643 learning rate:1.0000e-05, Loss_tot:0.3575,\n",
      "Save model 1643\n",
      "Train Epoch:1644 learning rate:1.0000e-05, Loss_tot:0.3580,\n",
      "Save model 1644\n",
      "Train Epoch:1645 learning rate:1.0000e-05, Loss_tot:0.3573,\n",
      "Save model 1645\n",
      "Train Epoch:1646 learning rate:1.0000e-05, Loss_tot:0.3583,\n",
      "Save model 1646\n",
      "Train Epoch:1647 learning rate:1.0000e-05, Loss_tot:0.3583,\n",
      "Save model 1647\n",
      "Train Epoch:1648 learning rate:1.0000e-05, Loss_tot:0.3582,\n",
      "Save model 1648\n",
      "Train Epoch:1649 learning rate:1.0000e-05, Loss_tot:0.3581,\n",
      "Save model 1649\n",
      "Train Epoch:1650 learning rate:1.0000e-05, Loss_tot:0.3580,\n",
      "Save model 1650\n",
      "Train Epoch:1651 learning rate:1.0000e-05, Loss_tot:0.3578,\n",
      "Save model 1651\n",
      "Train Epoch:1652 learning rate:1.0000e-05, Loss_tot:0.3578,\n",
      "Save model 1652\n",
      "Train Epoch:1653 learning rate:1.0000e-05, Loss_tot:0.3577,\n",
      "Save model 1653\n",
      "Train Epoch:1654 learning rate:1.0000e-05, Loss_tot:0.3576,\n",
      "Save model 1654\n",
      "Train Epoch:1655 learning rate:1.0000e-05, Loss_tot:0.3574,\n",
      "Save model 1655\n",
      "Train Epoch:1656 learning rate:1.0000e-05, Loss_tot:0.3574,\n",
      "Save model 1656\n",
      "Train Epoch:1657 learning rate:1.0000e-05, Loss_tot:0.3573,\n",
      "Save model 1657\n",
      "Train Epoch:1658 learning rate:1.0000e-05, Loss_tot:0.3572,\n",
      "Save model 1658\n",
      "Train Epoch:1659 learning rate:1.0000e-05, Loss_tot:0.3561,\n",
      "Save model 1659\n",
      "Train Epoch:1660 learning rate:1.0000e-05, Loss_tot:0.3570,\n",
      "Save model 1660\n",
      "Train Epoch:1661 learning rate:1.0000e-05, Loss_tot:0.3569,\n",
      "Save model 1661\n",
      "Train Epoch:1662 learning rate:1.0000e-05, Loss_tot:0.3565,\n",
      "Save model 1662\n",
      "Train Epoch:1663 learning rate:1.0000e-05, Loss_tot:0.3556,\n",
      "Save model 1663\n",
      "Train Epoch:1664 learning rate:1.0000e-05, Loss_tot:0.3566,\n",
      "Save model 1664\n",
      "Train Epoch:1665 learning rate:1.0000e-05, Loss_tot:0.3565,\n",
      "Save model 1665\n",
      "Train Epoch:1666 learning rate:1.0000e-05, Loss_tot:0.3564,\n",
      "Save model 1666\n",
      "Train Epoch:1667 learning rate:1.0000e-05, Loss_tot:0.3564,\n",
      "Save model 1667\n",
      "Train Epoch:1668 learning rate:1.0000e-05, Loss_tot:0.3563,\n",
      "Save model 1668\n",
      "Train Epoch:1669 learning rate:1.0000e-05, Loss_tot:0.3561,\n",
      "Save model 1669\n",
      "Train Epoch:1670 learning rate:1.0000e-05, Loss_tot:0.3561,\n",
      "Save model 1670\n",
      "Train Epoch:1671 learning rate:1.0000e-05, Loss_tot:0.3560,\n",
      "Save model 1671\n",
      "Train Epoch:1672 learning rate:1.0000e-05, Loss_tot:0.3555,\n",
      "Save model 1672\n",
      "Train Epoch:1673 learning rate:1.0000e-05, Loss_tot:0.3547,\n",
      "Save model 1673\n",
      "Train Epoch:1674 learning rate:1.0000e-05, Loss_tot:0.3557,\n",
      "Save model 1674\n",
      "Train Epoch:1675 learning rate:1.0000e-05, Loss_tot:0.3556,\n",
      "Save model 1675\n",
      "Train Epoch:1676 learning rate:1.0000e-05, Loss_tot:0.3555,\n",
      "Save model 1676\n",
      "Train Epoch:1677 learning rate:1.0000e-05, Loss_tot:0.3554,\n",
      "Save model 1677\n",
      "Train Epoch:1678 learning rate:1.0000e-05, Loss_tot:0.3553,\n",
      "Save model 1678\n",
      "Train Epoch:1679 learning rate:1.0000e-05, Loss_tot:0.3552,\n",
      "Save model 1679\n",
      "Train Epoch:1680 learning rate:1.0000e-05, Loss_tot:0.3551,\n",
      "Save model 1680\n",
      "Train Epoch:1681 learning rate:1.0000e-05, Loss_tot:0.3550,\n",
      "Save model 1681\n",
      "Train Epoch:1682 learning rate:1.0000e-05, Loss_tot:0.3550,\n",
      "Save model 1682\n",
      "Train Epoch:1683 learning rate:1.0000e-05, Loss_tot:0.3549,\n",
      "Save model 1683\n",
      "Train Epoch:1684 learning rate:1.0000e-05, Loss_tot:0.3544,\n",
      "Save model 1684\n",
      "Train Epoch:1685 learning rate:1.0000e-05, Loss_tot:0.3546,\n",
      "Save model 1685\n",
      "Train Epoch:1686 learning rate:1.0000e-05, Loss_tot:0.3535,\n",
      "Save model 1686\n",
      "Train Epoch:1687 learning rate:1.0000e-05, Loss_tot:0.3532,\n",
      "Save model 1687\n",
      "Train Epoch:1688 learning rate:1.0000e-05, Loss_tot:0.3543,\n",
      "Save model 1688\n",
      "Train Epoch:1689 learning rate:1.0000e-05, Loss_tot:0.3542,\n",
      "Save model 1689\n",
      "Train Epoch:1690 learning rate:1.0000e-05, Loss_tot:0.3541,\n",
      "Save model 1690\n",
      "Train Epoch:1691 learning rate:1.0000e-05, Loss_tot:0.3538,\n",
      "Save model 1691\n",
      "Train Epoch:1692 learning rate:1.0000e-05, Loss_tot:0.3539,\n",
      "Save model 1692\n",
      "Train Epoch:1693 learning rate:1.0000e-05, Loss_tot:0.3538,\n",
      "Save model 1693\n",
      "Train Epoch:1694 learning rate:1.0000e-05, Loss_tot:0.3537,\n",
      "Save model 1694\n",
      "Train Epoch:1695 learning rate:1.0000e-05, Loss_tot:0.3536,\n",
      "Save model 1695\n",
      "Train Epoch:1696 learning rate:1.0000e-05, Loss_tot:0.3526,\n",
      "Save model 1696\n",
      "Train Epoch:1697 learning rate:1.0000e-05, Loss_tot:0.3534,\n",
      "Save model 1697\n",
      "Train Epoch:1698 learning rate:1.0000e-05, Loss_tot:0.3533,\n",
      "Save model 1698\n",
      "Train Epoch:1699 learning rate:1.0000e-05, Loss_tot:0.3533,\n",
      "Save model 1699\n",
      "Train Epoch:1700 learning rate:1.0000e-05, Loss_tot:0.3529,\n",
      "Save model 1700\n",
      "Train Epoch:1701 learning rate:1.0000e-05, Loss_tot:0.3530,\n",
      "Save model 1701\n",
      "Train Epoch:1702 learning rate:1.0000e-05, Loss_tot:0.3529,\n",
      "Save model 1702\n",
      "Train Epoch:1703 learning rate:1.0000e-05, Loss_tot:0.3528,\n",
      "Save model 1703\n",
      "Train Epoch:1704 learning rate:1.0000e-05, Loss_tot:0.3527,\n",
      "Save model 1704\n",
      "Train Epoch:1705 learning rate:1.0000e-05, Loss_tot:0.3526,\n",
      "Save model 1705\n",
      "Train Epoch:1706 learning rate:1.0000e-05, Loss_tot:0.3525,\n",
      "Save model 1706\n",
      "Train Epoch:1707 learning rate:1.0000e-05, Loss_tot:0.3524,\n",
      "Save model 1707\n",
      "Train Epoch:1708 learning rate:1.0000e-05, Loss_tot:0.3512,\n",
      "Save model 1708\n",
      "Train Epoch:1709 learning rate:1.0000e-05, Loss_tot:0.3510,\n",
      "Save model 1709\n",
      "Train Epoch:1710 learning rate:1.0000e-05, Loss_tot:0.3521,\n",
      "Save model 1710\n",
      "Train Epoch:1711 learning rate:1.0000e-05, Loss_tot:0.3520,\n",
      "Save model 1711\n",
      "Train Epoch:1712 learning rate:1.0000e-05, Loss_tot:0.3519,\n",
      "Save model 1712\n",
      "Train Epoch:1713 learning rate:1.0000e-05, Loss_tot:0.3518,\n",
      "Save model 1713\n",
      "Train Epoch:1714 learning rate:1.0000e-05, Loss_tot:0.3515,\n",
      "Save model 1714\n",
      "Train Epoch:1715 learning rate:1.0000e-05, Loss_tot:0.3513,\n",
      "Save model 1715\n",
      "Train Epoch:1716 learning rate:1.0000e-05, Loss_tot:0.3507,\n",
      "Save model 1716\n",
      "Train Epoch:1717 learning rate:1.0000e-05, Loss_tot:0.3514,\n",
      "Save model 1717\n",
      "Train Epoch:1718 learning rate:1.0000e-05, Loss_tot:0.3513,\n",
      "Save model 1718\n",
      "Train Epoch:1719 learning rate:1.0000e-05, Loss_tot:0.3507,\n",
      "Save model 1719\n",
      "Train Epoch:1720 learning rate:1.0000e-05, Loss_tot:0.3511,\n",
      "Save model 1720\n",
      "Train Epoch:1721 learning rate:1.0000e-05, Loss_tot:0.3510,\n",
      "Save model 1721\n",
      "Train Epoch:1722 learning rate:1.0000e-05, Loss_tot:0.3509,\n",
      "Save model 1722\n",
      "Train Epoch:1723 learning rate:1.0000e-05, Loss_tot:0.3508,\n",
      "Save model 1723\n",
      "Train Epoch:1724 learning rate:1.0000e-05, Loss_tot:0.3507,\n",
      "Save model 1724\n",
      "Train Epoch:1725 learning rate:1.0000e-05, Loss_tot:0.3506,\n",
      "Save model 1725\n",
      "Train Epoch:1726 learning rate:1.0000e-05, Loss_tot:0.3504,\n",
      "Save model 1726\n",
      "Train Epoch:1727 learning rate:1.0000e-05, Loss_tot:0.3503,\n",
      "Save model 1727\n",
      "Train Epoch:1728 learning rate:1.0000e-05, Loss_tot:0.3495,\n",
      "Save model 1728\n",
      "Train Epoch:1729 learning rate:1.0000e-05, Loss_tot:0.3502,\n",
      "Save model 1729\n",
      "Train Epoch:1730 learning rate:1.0000e-05, Loss_tot:0.3500,\n",
      "Save model 1730\n",
      "Train Epoch:1731 learning rate:1.0000e-05, Loss_tot:0.3499,\n",
      "Save model 1731\n",
      "Train Epoch:1732 learning rate:1.0000e-05, Loss_tot:0.3498,\n",
      "Save model 1732\n",
      "Train Epoch:1733 learning rate:1.0000e-05, Loss_tot:0.3494,\n",
      "Save model 1733\n",
      "Train Epoch:1734 learning rate:1.0000e-05, Loss_tot:0.3484,\n",
      "Save model 1734\n",
      "Train Epoch:1735 learning rate:1.0000e-05, Loss_tot:0.3495,\n",
      "Save model 1735\n",
      "Train Epoch:1736 learning rate:1.0000e-05, Loss_tot:0.3494,\n",
      "Save model 1736\n",
      "Train Epoch:1737 learning rate:1.0000e-05, Loss_tot:0.3485,\n",
      "Save model 1737\n",
      "Train Epoch:1738 learning rate:1.0000e-05, Loss_tot:0.3491,\n",
      "Save model 1738\n",
      "Train Epoch:1739 learning rate:1.0000e-05, Loss_tot:0.3490,\n",
      "Save model 1739\n",
      "Train Epoch:1740 learning rate:1.0000e-05, Loss_tot:0.3489,\n",
      "Save model 1740\n",
      "Train Epoch:1741 learning rate:1.0000e-05, Loss_tot:0.3486,\n",
      "Save model 1741\n",
      "Train Epoch:1742 learning rate:1.0000e-05, Loss_tot:0.3485,\n",
      "Save model 1742\n",
      "Train Epoch:1743 learning rate:1.0000e-05, Loss_tot:0.3486,\n",
      "Save model 1743\n",
      "Train Epoch:1744 learning rate:1.0000e-05, Loss_tot:0.3485,\n",
      "Save model 1744\n",
      "Train Epoch:1745 learning rate:1.0000e-05, Loss_tot:0.3483,\n",
      "Save model 1745\n",
      "Train Epoch:1746 learning rate:1.0000e-05, Loss_tot:0.3482,\n",
      "Save model 1746\n",
      "Train Epoch:1747 learning rate:1.0000e-05, Loss_tot:0.3475,\n",
      "save model\n",
      "Save model 1747\n",
      "Train Epoch:1748 learning rate:1.0000e-05, Loss_tot:0.3479,\n",
      "Save model 1748\n",
      "Train Epoch:1749 learning rate:1.0000e-05, Loss_tot:0.3478,\n",
      "Save model 1749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1750 learning rate:1.0000e-05, Loss_tot:0.3477,\n",
      "Save model 1750\n",
      "Train Epoch:1751 learning rate:1.0000e-05, Loss_tot:0.3476,\n",
      "Save model 1751\n",
      "Train Epoch:1752 learning rate:1.0000e-05, Loss_tot:0.3475,\n",
      "save model\n",
      "Save model 1752\n",
      "Train Epoch:1753 learning rate:1.0000e-05, Loss_tot:0.3473,\n",
      "save model\n",
      "Save model 1753\n",
      "Train Epoch:1754 learning rate:1.0000e-05, Loss_tot:0.3466,\n",
      "save model\n",
      "Save model 1754\n",
      "Train Epoch:1755 learning rate:1.0000e-05, Loss_tot:0.3470,\n",
      "Save model 1755\n",
      "Train Epoch:1756 learning rate:1.0000e-05, Loss_tot:0.3469,\n",
      "Save model 1756\n",
      "Train Epoch:1757 learning rate:1.0000e-05, Loss_tot:0.3467,\n",
      "Save model 1757\n",
      "Train Epoch:1758 learning rate:1.0000e-05, Loss_tot:0.3460,\n",
      "save model\n",
      "Save model 1758\n",
      "Train Epoch:1759 learning rate:1.0000e-05, Loss_tot:0.3449,\n",
      "save model\n",
      "Save model 1759\n",
      "Train Epoch:1760 learning rate:1.0000e-05, Loss_tot:0.3463,\n",
      "Save model 1760\n",
      "Train Epoch:1761 learning rate:1.0000e-05, Loss_tot:0.3461,\n",
      "Save model 1761\n",
      "Train Epoch:1762 learning rate:1.0000e-05, Loss_tot:0.3460,\n",
      "Save model 1762\n",
      "Train Epoch:1763 learning rate:1.0000e-05, Loss_tot:0.3454,\n",
      "Save model 1763\n",
      "Train Epoch:1764 learning rate:1.0000e-05, Loss_tot:0.3452,\n",
      "Save model 1764\n",
      "Train Epoch:1765 learning rate:1.0000e-05, Loss_tot:0.3450,\n",
      "Save model 1765\n",
      "Train Epoch:1766 learning rate:1.0000e-05, Loss_tot:0.3448,\n",
      "save model\n",
      "Save model 1766\n",
      "Train Epoch:1767 learning rate:1.0000e-05, Loss_tot:0.3446,\n",
      "save model\n",
      "Save model 1767\n",
      "Train Epoch:1768 learning rate:1.0000e-05, Loss_tot:0.3471,\n",
      "Save model 1768\n",
      "Train Epoch:1769 learning rate:1.0000e-05, Loss_tot:0.3470,\n",
      "Save model 1769\n",
      "Train Epoch:1770 learning rate:1.0000e-05, Loss_tot:0.3469,\n",
      "Save model 1770\n",
      "Train Epoch:1771 learning rate:1.0000e-05, Loss_tot:0.3469,\n",
      "Save model 1771\n",
      "Train Epoch:1772 learning rate:1.0000e-05, Loss_tot:0.3468,\n",
      "Save model 1772\n",
      "Train Epoch:1773 learning rate:1.0000e-05, Loss_tot:0.3466,\n",
      "Save model 1773\n",
      "Train Epoch:1774 learning rate:1.0000e-05, Loss_tot:0.3409,\n",
      "save model\n",
      "Save model 1774\n",
      "Train Epoch:1775 learning rate:1.0000e-05, Loss_tot:0.3464,\n",
      "Save model 1775\n",
      "Train Epoch:1776 learning rate:1.0000e-05, Loss_tot:0.3463,\n",
      "Save model 1776\n",
      "Train Epoch:1777 learning rate:1.0000e-05, Loss_tot:0.3462,\n",
      "Save model 1777\n",
      "Train Epoch:1778 learning rate:1.0000e-05, Loss_tot:0.3461,\n",
      "Save model 1778\n",
      "Train Epoch:1779 learning rate:1.0000e-05, Loss_tot:0.3460,\n",
      "Save model 1779\n",
      "Train Epoch:1780 learning rate:1.0000e-05, Loss_tot:0.3459,\n",
      "Save model 1780\n",
      "Train Epoch:1781 learning rate:1.0000e-05, Loss_tot:0.3458,\n",
      "Save model 1781\n",
      "Train Epoch:1782 learning rate:1.0000e-05, Loss_tot:0.3456,\n",
      "Save model 1782\n",
      "Train Epoch:1783 learning rate:1.0000e-05, Loss_tot:0.3455,\n",
      "Save model 1783\n",
      "Train Epoch:1784 learning rate:1.0000e-05, Loss_tot:0.3451,\n",
      "Save model 1784\n",
      "Train Epoch:1785 learning rate:1.0000e-05, Loss_tot:0.3440,\n",
      "Save model 1785\n",
      "Train Epoch:1786 learning rate:1.0000e-05, Loss_tot:0.3453,\n",
      "Save model 1786\n",
      "Train Epoch:1787 learning rate:1.0000e-05, Loss_tot:0.3452,\n",
      "Save model 1787\n",
      "Train Epoch:1788 learning rate:1.0000e-05, Loss_tot:0.3451,\n",
      "Save model 1788\n",
      "Train Epoch:1789 learning rate:1.0000e-05, Loss_tot:0.3450,\n",
      "Save model 1789\n",
      "Train Epoch:1790 learning rate:1.0000e-05, Loss_tot:0.3449,\n",
      "Save model 1790\n",
      "Train Epoch:1791 learning rate:1.0000e-05, Loss_tot:0.3448,\n",
      "Save model 1791\n",
      "Train Epoch:1792 learning rate:1.0000e-05, Loss_tot:0.3447,\n",
      "Save model 1792\n",
      "Train Epoch:1793 learning rate:1.0000e-05, Loss_tot:0.3445,\n",
      "Save model 1793\n",
      "Train Epoch:1794 learning rate:1.0000e-05, Loss_tot:0.3445,\n",
      "Save model 1794\n",
      "Train Epoch:1795 learning rate:1.0000e-05, Loss_tot:0.3444,\n",
      "Save model 1795\n",
      "Train Epoch:1796 learning rate:1.0000e-05, Loss_tot:0.3443,\n",
      "Save model 1796\n",
      "Train Epoch:1797 learning rate:1.0000e-05, Loss_tot:0.3442,\n",
      "Save model 1797\n",
      "Train Epoch:1798 learning rate:1.0000e-05, Loss_tot:0.3441,\n",
      "Save model 1798\n",
      "Train Epoch:1799 learning rate:1.0000e-05, Loss_tot:0.3440,\n",
      "Save model 1799\n",
      "Train Epoch:1800 learning rate:1.0000e-05, Loss_tot:0.3439,\n",
      "Save model 1800\n",
      "Train Epoch:1801 learning rate:1.0000e-05, Loss_tot:0.3437,\n",
      "Save model 1801\n",
      "Train Epoch:1802 learning rate:1.0000e-05, Loss_tot:0.3437,\n",
      "Save model 1802\n",
      "Train Epoch:1803 learning rate:1.0000e-05, Loss_tot:0.3436,\n",
      "Save model 1803\n",
      "Train Epoch:1804 learning rate:1.0000e-05, Loss_tot:0.3435,\n",
      "Save model 1804\n",
      "Train Epoch:1805 learning rate:1.0000e-05, Loss_tot:0.3434,\n",
      "Save model 1805\n",
      "Train Epoch:1806 learning rate:1.0000e-05, Loss_tot:0.3433,\n",
      "Save model 1806\n",
      "Train Epoch:1807 learning rate:1.0000e-05, Loss_tot:0.3432,\n",
      "Save model 1807\n",
      "Train Epoch:1808 learning rate:1.0000e-05, Loss_tot:0.3422,\n",
      "Save model 1808\n",
      "Train Epoch:1809 learning rate:1.0000e-05, Loss_tot:0.3429,\n",
      "Save model 1809\n",
      "Train Epoch:1810 learning rate:1.0000e-05, Loss_tot:0.3428,\n",
      "Save model 1810\n",
      "Train Epoch:1811 learning rate:1.0000e-05, Loss_tot:0.3427,\n",
      "Save model 1811\n",
      "Train Epoch:1812 learning rate:1.0000e-05, Loss_tot:0.3412,\n",
      "Save model 1812\n",
      "Train Epoch:1813 learning rate:1.0000e-05, Loss_tot:0.3425,\n",
      "Save model 1813\n",
      "Train Epoch:1814 learning rate:1.0000e-05, Loss_tot:0.3425,\n",
      "Save model 1814\n",
      "Train Epoch:1815 learning rate:1.0000e-05, Loss_tot:0.3423,\n",
      "Save model 1815\n",
      "Train Epoch:1816 learning rate:1.0000e-05, Loss_tot:0.3419,\n",
      "Save model 1816\n",
      "Train Epoch:1817 learning rate:1.0000e-05, Loss_tot:0.3422,\n",
      "Save model 1817\n",
      "Train Epoch:1818 learning rate:1.0000e-05, Loss_tot:0.3421,\n",
      "Save model 1818\n",
      "Train Epoch:1819 learning rate:1.0000e-05, Loss_tot:0.3420,\n",
      "Save model 1819\n",
      "Train Epoch:1820 learning rate:1.0000e-05, Loss_tot:0.3419,\n",
      "Save model 1820\n",
      "Train Epoch:1821 learning rate:1.0000e-05, Loss_tot:0.3418,\n",
      "Save model 1821\n",
      "Train Epoch:1822 learning rate:1.0000e-05, Loss_tot:0.3417,\n",
      "Save model 1822\n",
      "Train Epoch:1823 learning rate:1.0000e-05, Loss_tot:0.3415,\n",
      "Save model 1823\n",
      "Train Epoch:1824 learning rate:1.0000e-05, Loss_tot:0.3415,\n",
      "Save model 1824\n",
      "Train Epoch:1825 learning rate:1.0000e-05, Loss_tot:0.3414,\n",
      "Save model 1825\n",
      "Train Epoch:1826 learning rate:1.0000e-05, Loss_tot:0.3413,\n",
      "Save model 1826\n",
      "Train Epoch:1827 learning rate:1.0000e-05, Loss_tot:0.3412,\n",
      "Save model 1827\n",
      "Train Epoch:1828 learning rate:1.0000e-05, Loss_tot:0.3411,\n",
      "Save model 1828\n",
      "Train Epoch:1829 learning rate:1.0000e-05, Loss_tot:0.3409,\n",
      "save model\n",
      "Save model 1829\n",
      "Train Epoch:1830 learning rate:1.0000e-05, Loss_tot:0.3409,\n",
      "save model\n",
      "Save model 1830\n",
      "Train Epoch:1831 learning rate:1.0000e-05, Loss_tot:0.3408,\n",
      "save model\n",
      "Save model 1831\n",
      "Train Epoch:1832 learning rate:1.0000e-05, Loss_tot:0.3406,\n",
      "save model\n",
      "Save model 1832\n",
      "Train Epoch:1833 learning rate:1.0000e-05, Loss_tot:0.3405,\n",
      "save model\n",
      "Save model 1833\n",
      "Train Epoch:1834 learning rate:1.0000e-05, Loss_tot:0.3404,\n",
      "save model\n",
      "Save model 1834\n",
      "Train Epoch:1835 learning rate:1.0000e-05, Loss_tot:0.3403,\n",
      "save model\n",
      "Save model 1835\n",
      "Train Epoch:1836 learning rate:1.0000e-05, Loss_tot:0.3402,\n",
      "save model\n",
      "Save model 1836\n",
      "Train Epoch:1837 learning rate:1.0000e-05, Loss_tot:0.3400,\n",
      "save model\n",
      "Save model 1837\n",
      "Train Epoch:1838 learning rate:1.0000e-05, Loss_tot:0.3399,\n",
      "save model\n",
      "Save model 1838\n",
      "Train Epoch:1839 learning rate:1.0000e-05, Loss_tot:0.3390,\n",
      "save model\n",
      "Save model 1839\n",
      "Train Epoch:1840 learning rate:1.0000e-05, Loss_tot:0.3397,\n",
      "Save model 1840\n",
      "Train Epoch:1841 learning rate:1.0000e-05, Loss_tot:0.3396,\n",
      "Save model 1841\n",
      "Train Epoch:1842 learning rate:1.0000e-05, Loss_tot:0.3395,\n",
      "Save model 1842\n",
      "Train Epoch:1843 learning rate:1.0000e-05, Loss_tot:0.3395,\n",
      "Save model 1843\n",
      "Train Epoch:1844 learning rate:1.0000e-05, Loss_tot:0.3378,\n",
      "save model\n",
      "Save model 1844\n",
      "Train Epoch:1845 learning rate:1.0000e-05, Loss_tot:0.3392,\n",
      "Save model 1845\n",
      "Train Epoch:1846 learning rate:1.0000e-05, Loss_tot:0.3391,\n",
      "Save model 1846\n",
      "Train Epoch:1847 learning rate:1.0000e-05, Loss_tot:0.3390,\n",
      "Save model 1847\n",
      "Train Epoch:1848 learning rate:1.0000e-05, Loss_tot:0.3389,\n",
      "Save model 1848\n",
      "Train Epoch:1849 learning rate:1.0000e-05, Loss_tot:0.3388,\n",
      "Save model 1849\n",
      "Train Epoch:1850 learning rate:1.0000e-05, Loss_tot:0.3383,\n",
      "Save model 1850\n",
      "Train Epoch:1851 learning rate:1.0000e-05, Loss_tot:0.3386,\n",
      "Save model 1851\n",
      "Train Epoch:1852 learning rate:1.0000e-05, Loss_tot:0.3385,\n",
      "Save model 1852\n",
      "Train Epoch:1853 learning rate:1.0000e-05, Loss_tot:0.3384,\n",
      "Save model 1853\n",
      "Train Epoch:1854 learning rate:1.0000e-05, Loss_tot:0.3382,\n",
      "Save model 1854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1855 learning rate:1.0000e-05, Loss_tot:0.3381,\n",
      "Save model 1855\n",
      "Train Epoch:1856 learning rate:1.0000e-05, Loss_tot:0.3380,\n",
      "Save model 1856\n",
      "Train Epoch:1857 learning rate:1.0000e-05, Loss_tot:0.3379,\n",
      "Save model 1857\n",
      "Train Epoch:1858 learning rate:1.0000e-05, Loss_tot:0.3378,\n",
      "save model\n",
      "Save model 1858\n",
      "Train Epoch:1859 learning rate:1.0000e-05, Loss_tot:0.3377,\n",
      "save model\n",
      "Save model 1859\n",
      "Train Epoch:1860 learning rate:1.0000e-05, Loss_tot:0.3376,\n",
      "save model\n",
      "Save model 1860\n",
      "Train Epoch:1861 learning rate:1.0000e-05, Loss_tot:0.3375,\n",
      "save model\n",
      "Save model 1861\n",
      "Train Epoch:1862 learning rate:1.0000e-05, Loss_tot:0.3374,\n",
      "save model\n",
      "Save model 1862\n",
      "Train Epoch:1863 learning rate:1.0000e-05, Loss_tot:0.3360,\n",
      "save model\n",
      "Save model 1863\n",
      "Train Epoch:1864 learning rate:1.0000e-05, Loss_tot:0.3372,\n",
      "Save model 1864\n",
      "Train Epoch:1865 learning rate:1.0000e-05, Loss_tot:0.3371,\n",
      "Save model 1865\n",
      "Train Epoch:1866 learning rate:1.0000e-05, Loss_tot:0.3369,\n",
      "Save model 1866\n",
      "Train Epoch:1867 learning rate:1.0000e-05, Loss_tot:0.3369,\n",
      "Save model 1867\n",
      "Train Epoch:1868 learning rate:1.0000e-05, Loss_tot:0.3368,\n",
      "Save model 1868\n",
      "Train Epoch:1869 learning rate:1.0000e-05, Loss_tot:0.3367,\n",
      "Save model 1869\n",
      "Train Epoch:1870 learning rate:1.0000e-05, Loss_tot:0.3366,\n",
      "Save model 1870\n",
      "Train Epoch:1871 learning rate:1.0000e-05, Loss_tot:0.3365,\n",
      "Save model 1871\n",
      "Train Epoch:1872 learning rate:1.0000e-05, Loss_tot:0.3363,\n",
      "Save model 1872\n",
      "Train Epoch:1873 learning rate:1.0000e-05, Loss_tot:0.3347,\n",
      "save model\n",
      "Save model 1873\n",
      "Train Epoch:1874 learning rate:1.0000e-05, Loss_tot:0.3361,\n",
      "Save model 1874\n",
      "Train Epoch:1875 learning rate:1.0000e-05, Loss_tot:0.3360,\n",
      "Save model 1875\n",
      "Train Epoch:1876 learning rate:1.0000e-05, Loss_tot:0.3359,\n",
      "Save model 1876\n",
      "Train Epoch:1877 learning rate:1.0000e-05, Loss_tot:0.3358,\n",
      "Save model 1877\n",
      "Train Epoch:1878 learning rate:1.0000e-05, Loss_tot:0.3357,\n",
      "Save model 1878\n",
      "Train Epoch:1879 learning rate:1.0000e-05, Loss_tot:0.3356,\n",
      "Save model 1879\n",
      "Train Epoch:1880 learning rate:1.0000e-05, Loss_tot:0.3354,\n",
      "Save model 1880\n",
      "Train Epoch:1881 learning rate:1.0000e-05, Loss_tot:0.3353,\n",
      "Save model 1881\n",
      "Train Epoch:1882 learning rate:1.0000e-05, Loss_tot:0.3352,\n",
      "Save model 1882\n",
      "Train Epoch:1883 learning rate:1.0000e-05, Loss_tot:0.3346,\n",
      "save model\n",
      "Save model 1883\n",
      "Train Epoch:1884 learning rate:1.0000e-05, Loss_tot:0.3350,\n",
      "Save model 1884\n",
      "Train Epoch:1885 learning rate:1.0000e-05, Loss_tot:0.3291,\n",
      "save model\n",
      "Save model 1885\n",
      "Train Epoch:1886 learning rate:1.0000e-05, Loss_tot:0.3347,\n",
      "Save model 1886\n",
      "Train Epoch:1887 learning rate:1.0000e-05, Loss_tot:0.3346,\n",
      "Save model 1887\n",
      "Train Epoch:1888 learning rate:1.0000e-05, Loss_tot:0.3345,\n",
      "Save model 1888\n",
      "Train Epoch:1889 learning rate:1.0000e-05, Loss_tot:0.3344,\n",
      "Save model 1889\n",
      "Train Epoch:1890 learning rate:1.0000e-05, Loss_tot:0.3343,\n",
      "Save model 1890\n",
      "Train Epoch:1891 learning rate:1.0000e-05, Loss_tot:0.3342,\n",
      "Save model 1891\n",
      "Train Epoch:1892 learning rate:1.0000e-05, Loss_tot:0.3341,\n",
      "Save model 1892\n",
      "Train Epoch:1893 learning rate:1.0000e-05, Loss_tot:0.3340,\n",
      "Save model 1893\n",
      "Train Epoch:1894 learning rate:1.0000e-05, Loss_tot:0.3339,\n",
      "Save model 1894\n",
      "Train Epoch:1895 learning rate:1.0000e-05, Loss_tot:0.3337,\n",
      "Save model 1895\n",
      "Train Epoch:1896 learning rate:1.0000e-05, Loss_tot:0.3336,\n",
      "Save model 1896\n",
      "Train Epoch:1897 learning rate:1.0000e-05, Loss_tot:0.3335,\n",
      "Save model 1897\n",
      "Train Epoch:1898 learning rate:1.0000e-05, Loss_tot:0.3335,\n",
      "Save model 1898\n",
      "Train Epoch:1899 learning rate:1.0000e-05, Loss_tot:0.3333,\n",
      "Save model 1899\n",
      "Train Epoch:1900 learning rate:1.0000e-05, Loss_tot:0.3317,\n",
      "Save model 1900\n",
      "Train Epoch:1901 learning rate:1.0000e-05, Loss_tot:0.3330,\n",
      "Save model 1901\n",
      "Train Epoch:1902 learning rate:1.0000e-05, Loss_tot:0.3329,\n",
      "Save model 1902\n",
      "Train Epoch:1903 learning rate:1.0000e-05, Loss_tot:0.3328,\n",
      "Save model 1903\n",
      "Train Epoch:1904 learning rate:1.0000e-05, Loss_tot:0.3327,\n",
      "Save model 1904\n",
      "Train Epoch:1905 learning rate:1.0000e-05, Loss_tot:0.3326,\n",
      "Save model 1905\n",
      "Train Epoch:1906 learning rate:1.0000e-05, Loss_tot:0.3325,\n",
      "Save model 1906\n",
      "Train Epoch:1907 learning rate:1.0000e-05, Loss_tot:0.3324,\n",
      "Save model 1907\n",
      "Train Epoch:1908 learning rate:1.0000e-05, Loss_tot:0.3323,\n",
      "Save model 1908\n",
      "Train Epoch:1909 learning rate:1.0000e-05, Loss_tot:0.3322,\n",
      "Save model 1909\n",
      "Train Epoch:1910 learning rate:1.0000e-05, Loss_tot:0.3320,\n",
      "Save model 1910\n",
      "Train Epoch:1911 learning rate:1.0000e-05, Loss_tot:0.3320,\n",
      "Save model 1911\n",
      "Train Epoch:1912 learning rate:1.0000e-05, Loss_tot:0.3319,\n",
      "Save model 1912\n",
      "Train Epoch:1913 learning rate:1.0000e-05, Loss_tot:0.3318,\n",
      "Save model 1913\n",
      "Train Epoch:1914 learning rate:1.0000e-05, Loss_tot:0.3316,\n",
      "Save model 1914\n",
      "Train Epoch:1915 learning rate:1.0000e-05, Loss_tot:0.3315,\n",
      "Save model 1915\n",
      "Train Epoch:1916 learning rate:1.0000e-05, Loss_tot:0.3314,\n",
      "Save model 1916\n",
      "Train Epoch:1917 learning rate:1.0000e-05, Loss_tot:0.3313,\n",
      "Save model 1917\n",
      "Train Epoch:1918 learning rate:1.0000e-05, Loss_tot:0.3311,\n",
      "Save model 1918\n",
      "Train Epoch:1919 learning rate:1.0000e-05, Loss_tot:0.3310,\n",
      "Save model 1919\n",
      "Train Epoch:1920 learning rate:1.0000e-05, Loss_tot:0.3310,\n",
      "Save model 1920\n",
      "Train Epoch:1921 learning rate:1.0000e-05, Loss_tot:0.3309,\n",
      "Save model 1921\n",
      "Train Epoch:1922 learning rate:1.0000e-05, Loss_tot:0.3307,\n",
      "Save model 1922\n",
      "Train Epoch:1923 learning rate:1.0000e-05, Loss_tot:0.3306,\n",
      "Save model 1923\n",
      "Train Epoch:1924 learning rate:1.0000e-05, Loss_tot:0.3304,\n",
      "Save model 1924\n",
      "Train Epoch:1925 learning rate:1.0000e-05, Loss_tot:0.3298,\n",
      "Save model 1925\n",
      "Train Epoch:1926 learning rate:1.0000e-05, Loss_tot:0.3302,\n",
      "Save model 1926\n",
      "Train Epoch:1927 learning rate:1.0000e-05, Loss_tot:0.3301,\n",
      "Save model 1927\n",
      "Train Epoch:1928 learning rate:1.0000e-05, Loss_tot:0.3300,\n",
      "Save model 1928\n",
      "Train Epoch:1929 learning rate:1.0000e-05, Loss_tot:0.3299,\n",
      "Save model 1929\n",
      "Train Epoch:1930 learning rate:1.0000e-05, Loss_tot:0.3298,\n",
      "Save model 1930\n",
      "Train Epoch:1931 learning rate:1.0000e-05, Loss_tot:0.3297,\n",
      "Save model 1931\n",
      "Train Epoch:1932 learning rate:1.0000e-05, Loss_tot:0.3280,\n",
      "save model\n",
      "Save model 1932\n",
      "Train Epoch:1933 learning rate:1.0000e-05, Loss_tot:0.3293,\n",
      "Save model 1933\n",
      "Train Epoch:1934 learning rate:1.0000e-05, Loss_tot:0.3292,\n",
      "Save model 1934\n",
      "Train Epoch:1935 learning rate:1.0000e-05, Loss_tot:0.3291,\n",
      "Save model 1935\n",
      "Train Epoch:1936 learning rate:1.0000e-05, Loss_tot:0.3290,\n",
      "Save model 1936\n",
      "Train Epoch:1937 learning rate:1.0000e-05, Loss_tot:0.3245,\n",
      "save model\n",
      "Save model 1937\n",
      "Train Epoch:1938 learning rate:1.0000e-05, Loss_tot:0.3288,\n",
      "Save model 1938\n",
      "Train Epoch:1939 learning rate:1.0000e-05, Loss_tot:0.3287,\n",
      "Save model 1939\n",
      "Train Epoch:1940 learning rate:1.0000e-05, Loss_tot:0.3286,\n",
      "Save model 1940\n",
      "Train Epoch:1941 learning rate:1.0000e-05, Loss_tot:0.3284,\n",
      "Save model 1941\n",
      "Train Epoch:1942 learning rate:1.0000e-05, Loss_tot:0.3283,\n",
      "Save model 1942\n",
      "Train Epoch:1943 learning rate:1.0000e-05, Loss_tot:0.3282,\n",
      "Save model 1943\n",
      "Train Epoch:1944 learning rate:1.0000e-05, Loss_tot:0.3281,\n",
      "Save model 1944\n",
      "Train Epoch:1945 learning rate:1.0000e-05, Loss_tot:0.3279,\n",
      "Save model 1945\n",
      "Train Epoch:1946 learning rate:1.0000e-05, Loss_tot:0.3268,\n",
      "Save model 1946\n",
      "Train Epoch:1947 learning rate:1.0000e-05, Loss_tot:0.3277,\n",
      "Save model 1947\n",
      "Train Epoch:1948 learning rate:1.0000e-05, Loss_tot:0.3276,\n",
      "Save model 1948\n",
      "Train Epoch:1949 learning rate:1.0000e-05, Loss_tot:0.3265,\n",
      "Save model 1949\n",
      "Train Epoch:1950 learning rate:1.0000e-05, Loss_tot:0.3274,\n",
      "Save model 1950\n",
      "Train Epoch:1951 learning rate:1.0000e-05, Loss_tot:0.3273,\n",
      "Save model 1951\n",
      "Train Epoch:1952 learning rate:1.0000e-05, Loss_tot:0.3272,\n",
      "Save model 1952\n",
      "Train Epoch:1953 learning rate:1.0000e-05, Loss_tot:0.3270,\n",
      "Save model 1953\n",
      "Train Epoch:1954 learning rate:1.0000e-05, Loss_tot:0.3269,\n",
      "Save model 1954\n",
      "Train Epoch:1955 learning rate:1.0000e-05, Loss_tot:0.3268,\n",
      "Save model 1955\n",
      "Train Epoch:1956 learning rate:1.0000e-05, Loss_tot:0.3266,\n",
      "Save model 1956\n",
      "Train Epoch:1957 learning rate:1.0000e-05, Loss_tot:0.3265,\n",
      "Save model 1957\n",
      "Train Epoch:1958 learning rate:1.0000e-05, Loss_tot:0.3264,\n",
      "Save model 1958\n",
      "Train Epoch:1959 learning rate:1.0000e-05, Loss_tot:0.3263,\n",
      "Save model 1959\n",
      "Train Epoch:1960 learning rate:1.0000e-05, Loss_tot:0.3262,\n",
      "Save model 1960\n",
      "Train Epoch:1961 learning rate:1.0000e-05, Loss_tot:0.3260,\n",
      "Save model 1961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1962 learning rate:1.0000e-05, Loss_tot:0.3259,\n",
      "Save model 1962\n",
      "Train Epoch:1963 learning rate:1.0000e-05, Loss_tot:0.3242,\n",
      "save model\n",
      "Save model 1963\n",
      "Train Epoch:1964 learning rate:1.0000e-05, Loss_tot:0.3257,\n",
      "Save model 1964\n",
      "Train Epoch:1965 learning rate:1.0000e-05, Loss_tot:0.3256,\n",
      "Save model 1965\n",
      "Train Epoch:1966 learning rate:1.0000e-05, Loss_tot:0.3254,\n",
      "Save model 1966\n",
      "Train Epoch:1967 learning rate:1.0000e-05, Loss_tot:0.3253,\n",
      "Save model 1967\n",
      "Train Epoch:1968 learning rate:1.0000e-05, Loss_tot:0.3252,\n",
      "Save model 1968\n",
      "Train Epoch:1969 learning rate:1.0000e-05, Loss_tot:0.3251,\n",
      "Save model 1969\n",
      "Train Epoch:1970 learning rate:1.0000e-05, Loss_tot:0.3250,\n",
      "Save model 1970\n",
      "Train Epoch:1971 learning rate:1.0000e-05, Loss_tot:0.3249,\n",
      "Save model 1971\n",
      "Train Epoch:1972 learning rate:1.0000e-05, Loss_tot:0.3242,\n",
      "save model\n",
      "Save model 1972\n",
      "Train Epoch:1973 learning rate:1.0000e-05, Loss_tot:0.3246,\n",
      "Save model 1973\n",
      "Train Epoch:1974 learning rate:1.0000e-05, Loss_tot:0.3244,\n",
      "Save model 1974\n",
      "Train Epoch:1975 learning rate:1.0000e-05, Loss_tot:0.3244,\n",
      "Save model 1975\n",
      "Train Epoch:1976 learning rate:1.0000e-05, Loss_tot:0.3243,\n",
      "Save model 1976\n",
      "Train Epoch:1977 learning rate:1.0000e-05, Loss_tot:0.3241,\n",
      "save model\n",
      "Save model 1977\n",
      "Train Epoch:1978 learning rate:1.0000e-05, Loss_tot:0.3240,\n",
      "save model\n",
      "Save model 1978\n",
      "Train Epoch:1979 learning rate:1.0000e-05, Loss_tot:0.3239,\n",
      "save model\n",
      "Save model 1979\n",
      "Train Epoch:1980 learning rate:1.0000e-05, Loss_tot:0.3238,\n",
      "save model\n",
      "Save model 1980\n",
      "Train Epoch:1981 learning rate:1.0000e-05, Loss_tot:0.3237,\n",
      "save model\n",
      "Save model 1981\n",
      "Train Epoch:1982 learning rate:1.0000e-05, Loss_tot:0.3235,\n",
      "save model\n",
      "Save model 1982\n",
      "Train Epoch:1983 learning rate:1.0000e-05, Loss_tot:0.3233,\n",
      "save model\n",
      "Save model 1983\n",
      "Train Epoch:1984 learning rate:1.0000e-05, Loss_tot:0.3233,\n",
      "save model\n",
      "Save model 1984\n",
      "Train Epoch:1985 learning rate:1.0000e-05, Loss_tot:0.3232,\n",
      "save model\n",
      "Save model 1985\n",
      "Train Epoch:1986 learning rate:1.0000e-05, Loss_tot:0.3230,\n",
      "save model\n",
      "Save model 1986\n",
      "Train Epoch:1987 learning rate:1.0000e-05, Loss_tot:0.3229,\n",
      "save model\n",
      "Save model 1987\n",
      "Train Epoch:1988 learning rate:1.0000e-05, Loss_tot:0.3228,\n",
      "save model\n",
      "Save model 1988\n",
      "Train Epoch:1989 learning rate:1.0000e-05, Loss_tot:0.3227,\n",
      "save model\n",
      "Save model 1989\n",
      "Train Epoch:1990 learning rate:1.0000e-05, Loss_tot:0.3225,\n",
      "save model\n",
      "Save model 1990\n",
      "Train Epoch:1991 learning rate:1.0000e-05, Loss_tot:0.3223,\n",
      "save model\n",
      "Save model 1991\n",
      "Train Epoch:1992 learning rate:1.0000e-05, Loss_tot:0.3222,\n",
      "save model\n",
      "Save model 1992\n",
      "Train Epoch:1993 learning rate:1.0000e-05, Loss_tot:0.3222,\n",
      "save model\n",
      "Save model 1993\n",
      "Train Epoch:1994 learning rate:1.0000e-05, Loss_tot:0.3221,\n",
      "save model\n",
      "Save model 1994\n",
      "Train Epoch:1995 learning rate:1.0000e-05, Loss_tot:0.3219,\n",
      "save model\n",
      "Save model 1995\n",
      "Train Epoch:1996 learning rate:1.0000e-05, Loss_tot:0.3217,\n",
      "save model\n",
      "Save model 1996\n",
      "Train Epoch:1997 learning rate:1.0000e-05, Loss_tot:0.3201,\n",
      "save model\n",
      "Save model 1997\n",
      "Train Epoch:1998 learning rate:1.0000e-05, Loss_tot:0.3215,\n",
      "Save model 1998\n",
      "Train Epoch:1999 learning rate:1.0000e-05, Loss_tot:0.3214,\n",
      "Save model 1999\n",
      "Train Epoch:2000 learning rate:1.0000e-05, Loss_tot:0.3212,\n",
      "Save model 2000\n",
      "Train Epoch:2001 learning rate:1.0000e-05, Loss_tot:0.3211,\n",
      "Save model 2001\n",
      "Train Epoch:2002 learning rate:1.0000e-05, Loss_tot:0.3150,\n",
      "save model\n",
      "Save model 2002\n",
      "Train Epoch:2003 learning rate:1.0000e-05, Loss_tot:0.3208,\n",
      "Save model 2003\n",
      "Train Epoch:2004 learning rate:1.0000e-05, Loss_tot:0.3207,\n",
      "Save model 2004\n",
      "Train Epoch:2005 learning rate:1.0000e-05, Loss_tot:0.3206,\n",
      "Save model 2005\n",
      "Train Epoch:2006 learning rate:1.0000e-05, Loss_tot:0.3204,\n",
      "Save model 2006\n",
      "Train Epoch:2007 learning rate:1.0000e-05, Loss_tot:0.3198,\n",
      "Save model 2007\n",
      "Train Epoch:2008 learning rate:1.0000e-05, Loss_tot:0.3202,\n",
      "Save model 2008\n",
      "Train Epoch:2009 learning rate:1.0000e-05, Loss_tot:0.3200,\n",
      "Save model 2009\n",
      "Train Epoch:2010 learning rate:1.0000e-05, Loss_tot:0.3199,\n",
      "Save model 2010\n",
      "Train Epoch:2011 learning rate:1.0000e-05, Loss_tot:0.3198,\n",
      "Save model 2011\n",
      "Train Epoch:2012 learning rate:1.0000e-05, Loss_tot:0.3196,\n",
      "Save model 2012\n",
      "Train Epoch:2013 learning rate:1.0000e-05, Loss_tot:0.3196,\n",
      "Save model 2013\n",
      "Train Epoch:2014 learning rate:1.0000e-05, Loss_tot:0.3194,\n",
      "Save model 2014\n",
      "Train Epoch:2015 learning rate:1.0000e-05, Loss_tot:0.3193,\n",
      "Save model 2015\n",
      "Train Epoch:2016 learning rate:1.0000e-05, Loss_tot:0.3191,\n",
      "Save model 2016\n",
      "Train Epoch:2017 learning rate:1.0000e-05, Loss_tot:0.3190,\n",
      "Save model 2017\n",
      "Train Epoch:2018 learning rate:1.0000e-05, Loss_tot:0.3174,\n",
      "Save model 2018\n",
      "Train Epoch:2019 learning rate:1.0000e-05, Loss_tot:0.3187,\n",
      "Save model 2019\n",
      "Train Epoch:2020 learning rate:1.0000e-05, Loss_tot:0.3186,\n",
      "Save model 2020\n",
      "Train Epoch:2021 learning rate:1.0000e-05, Loss_tot:0.3185,\n",
      "Save model 2021\n",
      "Train Epoch:2022 learning rate:1.0000e-05, Loss_tot:0.3183,\n",
      "Save model 2022\n",
      "Train Epoch:2023 learning rate:1.0000e-05, Loss_tot:0.3182,\n",
      "Save model 2023\n",
      "Train Epoch:2024 learning rate:1.0000e-05, Loss_tot:0.3181,\n",
      "Save model 2024\n",
      "Train Epoch:2025 learning rate:1.0000e-05, Loss_tot:0.3180,\n",
      "Save model 2025\n",
      "Train Epoch:2026 learning rate:1.0000e-05, Loss_tot:0.3178,\n",
      "Save model 2026\n",
      "Train Epoch:2027 learning rate:1.0000e-05, Loss_tot:0.3177,\n",
      "Save model 2027\n",
      "Train Epoch:2028 learning rate:1.0000e-05, Loss_tot:0.3176,\n",
      "Save model 2028\n",
      "Train Epoch:2029 learning rate:1.0000e-05, Loss_tot:0.3174,\n",
      "Save model 2029\n",
      "Train Epoch:2030 learning rate:1.0000e-05, Loss_tot:0.3158,\n",
      "Save model 2030\n",
      "Train Epoch:2031 learning rate:1.0000e-05, Loss_tot:0.3172,\n",
      "Save model 2031\n",
      "Train Epoch:2032 learning rate:1.0000e-05, Loss_tot:0.3170,\n",
      "Save model 2032\n",
      "Train Epoch:2033 learning rate:1.0000e-05, Loss_tot:0.3169,\n",
      "Save model 2033\n",
      "Train Epoch:2034 learning rate:1.0000e-05, Loss_tot:0.3168,\n",
      "Save model 2034\n",
      "Train Epoch:2035 learning rate:1.0000e-05, Loss_tot:0.3167,\n",
      "Save model 2035\n",
      "Train Epoch:2036 learning rate:1.0000e-05, Loss_tot:0.3165,\n",
      "Save model 2036\n",
      "Train Epoch:2037 learning rate:1.0000e-05, Loss_tot:0.3164,\n",
      "Save model 2037\n",
      "Train Epoch:2038 learning rate:1.0000e-05, Loss_tot:0.3163,\n",
      "Save model 2038\n",
      "Train Epoch:2039 learning rate:1.0000e-05, Loss_tot:0.3161,\n",
      "Save model 2039\n",
      "Train Epoch:2040 learning rate:1.0000e-05, Loss_tot:0.3160,\n",
      "Save model 2040\n",
      "Train Epoch:2041 learning rate:1.0000e-05, Loss_tot:0.3159,\n",
      "Save model 2041\n",
      "Train Epoch:2042 learning rate:1.0000e-05, Loss_tot:0.3158,\n",
      "Save model 2042\n",
      "Train Epoch:2043 learning rate:1.0000e-05, Loss_tot:0.3156,\n",
      "Save model 2043\n",
      "Train Epoch:2044 learning rate:1.0000e-05, Loss_tot:0.3154,\n",
      "Save model 2044\n",
      "Train Epoch:2045 learning rate:1.0000e-05, Loss_tot:0.3153,\n",
      "Save model 2045\n",
      "Train Epoch:2046 learning rate:1.0000e-05, Loss_tot:0.3141,\n",
      "save model\n",
      "Save model 2046\n",
      "Train Epoch:2047 learning rate:1.0000e-05, Loss_tot:0.2776,\n",
      "save model\n",
      "Save model 2047\n",
      "Train Epoch:2048 learning rate:1.0000e-05, Loss_tot:0.3149,\n",
      "Save model 2048\n",
      "Train Epoch:2049 learning rate:1.0000e-05, Loss_tot:0.3148,\n",
      "Save model 2049\n",
      "Train Epoch:2050 learning rate:1.0000e-05, Loss_tot:0.3147,\n",
      "Save model 2050\n",
      "Train Epoch:2051 learning rate:1.0000e-05, Loss_tot:0.3145,\n",
      "Save model 2051\n",
      "Train Epoch:2052 learning rate:1.0000e-05, Loss_tot:0.3144,\n",
      "Save model 2052\n",
      "Train Epoch:2053 learning rate:1.0000e-05, Loss_tot:0.3143,\n",
      "Save model 2053\n",
      "Train Epoch:2054 learning rate:1.0000e-05, Loss_tot:0.3142,\n",
      "Save model 2054\n",
      "Train Epoch:2055 learning rate:1.0000e-05, Loss_tot:0.3140,\n",
      "Save model 2055\n",
      "Train Epoch:2056 learning rate:1.0000e-05, Loss_tot:0.3139,\n",
      "Save model 2056\n",
      "Train Epoch:2057 learning rate:1.0000e-05, Loss_tot:0.3138,\n",
      "Save model 2057\n",
      "Train Epoch:2058 learning rate:1.0000e-05, Loss_tot:0.3137,\n",
      "Save model 2058\n",
      "Train Epoch:2059 learning rate:1.0000e-05, Loss_tot:0.3135,\n",
      "Save model 2059\n",
      "Train Epoch:2060 learning rate:1.0000e-05, Loss_tot:0.3134,\n",
      "Save model 2060\n",
      "Train Epoch:2061 learning rate:1.0000e-05, Loss_tot:0.3133,\n",
      "Save model 2061\n",
      "Train Epoch:2062 learning rate:1.0000e-05, Loss_tot:0.3131,\n",
      "Save model 2062\n",
      "Train Epoch:2063 learning rate:1.0000e-05, Loss_tot:0.3130,\n",
      "Save model 2063\n",
      "Train Epoch:2064 learning rate:1.0000e-05, Loss_tot:0.3113,\n",
      "Save model 2064\n",
      "Train Epoch:2065 learning rate:1.0000e-05, Loss_tot:0.3127,\n",
      "Save model 2065\n",
      "Train Epoch:2066 learning rate:1.0000e-05, Loss_tot:0.3119,\n",
      "Save model 2066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2067 learning rate:1.0000e-05, Loss_tot:0.3124,\n",
      "Save model 2067\n",
      "Train Epoch:2068 learning rate:1.0000e-05, Loss_tot:0.3122,\n",
      "Save model 2068\n",
      "Train Epoch:2069 learning rate:1.0000e-05, Loss_tot:0.3121,\n",
      "Save model 2069\n",
      "Train Epoch:2070 learning rate:1.0000e-05, Loss_tot:0.3120,\n",
      "Save model 2070\n",
      "Train Epoch:2071 learning rate:1.0000e-05, Loss_tot:0.3118,\n",
      "Save model 2071\n",
      "Train Epoch:2072 learning rate:1.0000e-05, Loss_tot:0.3117,\n",
      "Save model 2072\n",
      "Train Epoch:2073 learning rate:1.0000e-05, Loss_tot:0.3116,\n",
      "Save model 2073\n",
      "Train Epoch:2074 learning rate:1.0000e-05, Loss_tot:0.3115,\n",
      "Save model 2074\n",
      "Train Epoch:2075 learning rate:1.0000e-05, Loss_tot:0.3113,\n",
      "Save model 2075\n",
      "Train Epoch:2076 learning rate:1.0000e-05, Loss_tot:0.3112,\n",
      "Save model 2076\n",
      "Train Epoch:2077 learning rate:1.0000e-05, Loss_tot:0.3110,\n",
      "Save model 2077\n",
      "Train Epoch:2078 learning rate:1.0000e-05, Loss_tot:0.3109,\n",
      "Save model 2078\n",
      "Train Epoch:2079 learning rate:1.0000e-05, Loss_tot:0.3108,\n",
      "Save model 2079\n",
      "Train Epoch:2080 learning rate:1.0000e-05, Loss_tot:0.3106,\n",
      "Save model 2080\n",
      "Train Epoch:2081 learning rate:1.0000e-05, Loss_tot:0.3104,\n",
      "Save model 2081\n",
      "Train Epoch:2082 learning rate:1.0000e-05, Loss_tot:0.3103,\n",
      "Save model 2082\n",
      "Train Epoch:2083 learning rate:1.0000e-05, Loss_tot:0.3102,\n",
      "Save model 2083\n",
      "Train Epoch:2084 learning rate:1.0000e-05, Loss_tot:0.3100,\n",
      "Save model 2084\n",
      "Train Epoch:2085 learning rate:1.0000e-05, Loss_tot:0.3088,\n",
      "Save model 2085\n",
      "Train Epoch:2086 learning rate:1.0000e-05, Loss_tot:0.3097,\n",
      "Save model 2086\n",
      "Train Epoch:2087 learning rate:1.0000e-05, Loss_tot:0.3096,\n",
      "Save model 2087\n",
      "Train Epoch:2088 learning rate:1.0000e-05, Loss_tot:0.3094,\n",
      "Save model 2088\n",
      "Train Epoch:2089 learning rate:1.0000e-05, Loss_tot:0.3093,\n",
      "Save model 2089\n",
      "Train Epoch:2090 learning rate:1.0000e-05, Loss_tot:0.3092,\n",
      "Save model 2090\n",
      "Train Epoch:2091 learning rate:1.0000e-05, Loss_tot:0.3090,\n",
      "Save model 2091\n",
      "Train Epoch:2092 learning rate:1.0000e-05, Loss_tot:0.3089,\n",
      "Save model 2092\n",
      "Train Epoch:2093 learning rate:1.0000e-05, Loss_tot:0.3087,\n",
      "Save model 2093\n",
      "Train Epoch:2094 learning rate:1.0000e-05, Loss_tot:0.3086,\n",
      "Save model 2094\n",
      "Train Epoch:2095 learning rate:1.0000e-05, Loss_tot:0.3085,\n",
      "Save model 2095\n",
      "Train Epoch:2096 learning rate:1.0000e-05, Loss_tot:0.3083,\n",
      "Save model 2096\n",
      "Train Epoch:2097 learning rate:1.0000e-05, Loss_tot:0.3082,\n",
      "Save model 2097\n",
      "Train Epoch:2098 learning rate:1.0000e-05, Loss_tot:0.3081,\n",
      "Save model 2098\n",
      "Train Epoch:2099 learning rate:1.0000e-05, Loss_tot:0.3064,\n",
      "Save model 2099\n",
      "Train Epoch:2100 learning rate:1.0000e-05, Loss_tot:0.3078,\n",
      "Save model 2100\n",
      "Train Epoch:2101 learning rate:1.0000e-05, Loss_tot:0.3015,\n",
      "Save model 2101\n",
      "Train Epoch:2102 learning rate:1.0000e-05, Loss_tot:0.3075,\n",
      "Save model 2102\n",
      "Train Epoch:2103 learning rate:1.0000e-05, Loss_tot:0.3068,\n",
      "Save model 2103\n",
      "Train Epoch:2104 learning rate:1.0000e-05, Loss_tot:0.3072,\n",
      "Save model 2104\n",
      "Train Epoch:2105 learning rate:1.0000e-05, Loss_tot:0.3071,\n",
      "Save model 2105\n",
      "Train Epoch:2106 learning rate:1.0000e-05, Loss_tot:0.3069,\n",
      "Save model 2106\n",
      "Train Epoch:2107 learning rate:1.0000e-05, Loss_tot:0.3068,\n",
      "Save model 2107\n",
      "Train Epoch:2108 learning rate:1.0000e-05, Loss_tot:0.3066,\n",
      "Save model 2108\n",
      "Train Epoch:2109 learning rate:1.0000e-05, Loss_tot:0.3065,\n",
      "Save model 2109\n",
      "Train Epoch:2110 learning rate:1.0000e-05, Loss_tot:0.3063,\n",
      "Save model 2110\n",
      "Train Epoch:2111 learning rate:1.0000e-05, Loss_tot:0.3062,\n",
      "Save model 2111\n",
      "Train Epoch:2112 learning rate:1.0000e-05, Loss_tot:0.3061,\n",
      "Save model 2112\n",
      "Train Epoch:2113 learning rate:1.0000e-05, Loss_tot:0.3059,\n",
      "Save model 2113\n",
      "Train Epoch:2114 learning rate:1.0000e-05, Loss_tot:0.2695,\n",
      "save model\n",
      "Save model 2114\n",
      "Train Epoch:2115 learning rate:1.0000e-05, Loss_tot:0.3056,\n",
      "Save model 2115\n",
      "Train Epoch:2116 learning rate:1.0000e-05, Loss_tot:0.3055,\n",
      "Save model 2116\n",
      "Train Epoch:2117 learning rate:1.0000e-05, Loss_tot:0.3054,\n",
      "Save model 2117\n",
      "Train Epoch:2118 learning rate:1.0000e-05, Loss_tot:0.3052,\n",
      "Save model 2118\n",
      "Train Epoch:2119 learning rate:1.0000e-05, Loss_tot:0.3001,\n",
      "Save model 2119\n",
      "Train Epoch:2120 learning rate:1.0000e-05, Loss_tot:0.3049,\n",
      "Save model 2120\n",
      "Train Epoch:2121 learning rate:1.0000e-05, Loss_tot:0.3048,\n",
      "Save model 2121\n",
      "Train Epoch:2122 learning rate:1.0000e-05, Loss_tot:0.3046,\n",
      "Save model 2122\n",
      "Train Epoch:2123 learning rate:1.0000e-05, Loss_tot:0.3045,\n",
      "Save model 2123\n",
      "Train Epoch:2124 learning rate:1.0000e-05, Loss_tot:0.3044,\n",
      "Save model 2124\n",
      "Train Epoch:2125 learning rate:1.0000e-05, Loss_tot:0.3042,\n",
      "Save model 2125\n",
      "Train Epoch:2126 learning rate:1.0000e-05, Loss_tot:0.3040,\n",
      "Save model 2126\n",
      "Train Epoch:2127 learning rate:1.0000e-05, Loss_tot:0.3040,\n",
      "Save model 2127\n",
      "Train Epoch:2128 learning rate:1.0000e-05, Loss_tot:0.3038,\n",
      "Save model 2128\n",
      "Train Epoch:2129 learning rate:1.0000e-05, Loss_tot:0.3036,\n",
      "Save model 2129\n",
      "Train Epoch:2130 learning rate:1.0000e-05, Loss_tot:0.3035,\n",
      "Save model 2130\n",
      "Train Epoch:2131 learning rate:1.0000e-05, Loss_tot:0.3034,\n",
      "Save model 2131\n",
      "Train Epoch:2132 learning rate:1.0000e-05, Loss_tot:0.3032,\n",
      "Save model 2132\n",
      "Train Epoch:2133 learning rate:1.0000e-05, Loss_tot:0.3031,\n",
      "Save model 2133\n",
      "Train Epoch:2134 learning rate:1.0000e-05, Loss_tot:0.3029,\n",
      "Save model 2134\n",
      "Train Epoch:2135 learning rate:1.0000e-05, Loss_tot:0.3027,\n",
      "Save model 2135\n",
      "Train Epoch:2136 learning rate:1.0000e-05, Loss_tot:0.3010,\n",
      "Save model 2136\n",
      "Train Epoch:2137 learning rate:1.0000e-05, Loss_tot:0.3025,\n",
      "Save model 2137\n",
      "Train Epoch:2138 learning rate:1.0000e-05, Loss_tot:0.3023,\n",
      "Save model 2138\n",
      "Train Epoch:2139 learning rate:1.0000e-05, Loss_tot:0.3021,\n",
      "Save model 2139\n",
      "Train Epoch:2140 learning rate:1.0000e-05, Loss_tot:0.3020,\n",
      "Save model 2140\n",
      "Train Epoch:2141 learning rate:1.0000e-05, Loss_tot:0.2989,\n",
      "Save model 2141\n",
      "Train Epoch:2142 learning rate:1.0000e-05, Loss_tot:0.3017,\n",
      "Save model 2142\n",
      "Train Epoch:2143 learning rate:1.0000e-05, Loss_tot:0.3016,\n",
      "Save model 2143\n",
      "Train Epoch:2144 learning rate:1.0000e-05, Loss_tot:0.3015,\n",
      "Save model 2144\n",
      "Train Epoch:2145 learning rate:1.0000e-05, Loss_tot:0.3013,\n",
      "Save model 2145\n",
      "Train Epoch:2146 learning rate:1.0000e-05, Loss_tot:0.3010,\n",
      "Save model 2146\n",
      "Train Epoch:2147 learning rate:1.0000e-05, Loss_tot:0.3010,\n",
      "Save model 2147\n",
      "Train Epoch:2148 learning rate:1.0000e-05, Loss_tot:0.3008,\n",
      "Save model 2148\n",
      "Train Epoch:2149 learning rate:1.0000e-05, Loss_tot:0.3007,\n",
      "Save model 2149\n",
      "Train Epoch:2150 learning rate:1.0000e-05, Loss_tot:0.2652,\n",
      "save model\n",
      "Save model 2150\n",
      "Train Epoch:2151 learning rate:1.0000e-05, Loss_tot:0.3003,\n",
      "Save model 2151\n",
      "Train Epoch:2152 learning rate:1.0000e-05, Loss_tot:0.3002,\n",
      "Save model 2152\n",
      "Train Epoch:2153 learning rate:1.0000e-05, Loss_tot:0.3001,\n",
      "Save model 2153\n",
      "Train Epoch:2154 learning rate:1.0000e-05, Loss_tot:0.3000,\n",
      "Save model 2154\n",
      "Train Epoch:2155 learning rate:1.0000e-05, Loss_tot:0.2999,\n",
      "Save model 2155\n",
      "Train Epoch:2156 learning rate:1.0000e-05, Loss_tot:0.2997,\n",
      "Save model 2156\n",
      "Train Epoch:2157 learning rate:1.0000e-05, Loss_tot:0.2995,\n",
      "Save model 2157\n",
      "Train Epoch:2158 learning rate:1.0000e-05, Loss_tot:0.2993,\n",
      "Save model 2158\n",
      "Train Epoch:2159 learning rate:1.0000e-05, Loss_tot:0.2992,\n",
      "Save model 2159\n",
      "Train Epoch:2160 learning rate:1.0000e-05, Loss_tot:0.2990,\n",
      "Save model 2160\n",
      "Train Epoch:2161 learning rate:1.0000e-05, Loss_tot:0.2989,\n",
      "Save model 2161\n",
      "Train Epoch:2162 learning rate:1.0000e-05, Loss_tot:0.2987,\n",
      "Save model 2162\n",
      "Train Epoch:2163 learning rate:1.0000e-05, Loss_tot:0.2986,\n",
      "Save model 2163\n",
      "Train Epoch:2164 learning rate:1.0000e-05, Loss_tot:0.2984,\n",
      "Save model 2164\n",
      "Train Epoch:2165 learning rate:1.0000e-05, Loss_tot:0.2983,\n",
      "Save model 2165\n",
      "Train Epoch:2166 learning rate:1.0000e-05, Loss_tot:0.2981,\n",
      "Save model 2166\n",
      "Train Epoch:2167 learning rate:1.0000e-05, Loss_tot:0.2980,\n",
      "Save model 2167\n",
      "Train Epoch:2168 learning rate:1.0000e-05, Loss_tot:0.2978,\n",
      "Save model 2168\n",
      "Train Epoch:2169 learning rate:1.0000e-05, Loss_tot:0.2976,\n",
      "Save model 2169\n",
      "Train Epoch:2170 learning rate:1.0000e-05, Loss_tot:0.2959,\n",
      "Save model 2170\n",
      "Train Epoch:2171 learning rate:1.0000e-05, Loss_tot:0.2974,\n",
      "Save model 2171\n",
      "Train Epoch:2172 learning rate:1.0000e-05, Loss_tot:0.2972,\n",
      "Save model 2172\n",
      "Train Epoch:2173 learning rate:1.0000e-05, Loss_tot:0.2970,\n",
      "Save model 2173\n",
      "Train Epoch:2174 learning rate:1.0000e-05, Loss_tot:0.2969,\n",
      "Save model 2174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2175 learning rate:1.0000e-05, Loss_tot:0.2959,\n",
      "Save model 2175\n",
      "Train Epoch:2176 learning rate:1.0000e-05, Loss_tot:0.2965,\n",
      "Save model 2176\n",
      "Train Epoch:2177 learning rate:1.0000e-05, Loss_tot:0.2964,\n",
      "Save model 2177\n",
      "Train Epoch:2178 learning rate:1.0000e-05, Loss_tot:0.2963,\n",
      "Save model 2178\n",
      "Train Epoch:2179 learning rate:1.0000e-05, Loss_tot:0.2961,\n",
      "Save model 2179\n",
      "Train Epoch:2180 learning rate:1.0000e-05, Loss_tot:0.2959,\n",
      "Save model 2180\n",
      "Train Epoch:2181 learning rate:1.0000e-05, Loss_tot:0.2958,\n",
      "Save model 2181\n",
      "Train Epoch:2182 learning rate:1.0000e-05, Loss_tot:0.2956,\n",
      "Save model 2182\n",
      "Train Epoch:2183 learning rate:1.0000e-05, Loss_tot:0.2955,\n",
      "Save model 2183\n",
      "Train Epoch:2184 learning rate:1.0000e-05, Loss_tot:0.2610,\n",
      "save model\n",
      "Save model 2184\n",
      "Train Epoch:2185 learning rate:1.0000e-05, Loss_tot:0.2952,\n",
      "Save model 2185\n",
      "Train Epoch:2186 learning rate:1.0000e-05, Loss_tot:0.2950,\n",
      "Save model 2186\n",
      "Train Epoch:2187 learning rate:1.0000e-05, Loss_tot:0.2949,\n",
      "Save model 2187\n",
      "Train Epoch:2188 learning rate:1.0000e-05, Loss_tot:0.2947,\n",
      "Save model 2188\n",
      "Train Epoch:2189 learning rate:1.0000e-05, Loss_tot:0.2946,\n",
      "Save model 2189\n",
      "Train Epoch:2190 learning rate:1.0000e-05, Loss_tot:0.2944,\n",
      "Save model 2190\n",
      "Train Epoch:2191 learning rate:1.0000e-05, Loss_tot:0.2943,\n",
      "Save model 2191\n",
      "Train Epoch:2192 learning rate:1.0000e-05, Loss_tot:0.2941,\n",
      "Save model 2192\n",
      "Train Epoch:2193 learning rate:1.0000e-05, Loss_tot:0.2940,\n",
      "Save model 2193\n",
      "Train Epoch:2194 learning rate:1.0000e-05, Loss_tot:0.2938,\n",
      "Save model 2194\n",
      "Train Epoch:2195 learning rate:1.0000e-05, Loss_tot:0.2930,\n",
      "Save model 2195\n",
      "Train Epoch:2196 learning rate:1.0000e-05, Loss_tot:0.2935,\n",
      "Save model 2196\n",
      "Train Epoch:2197 learning rate:1.0000e-05, Loss_tot:0.2933,\n",
      "Save model 2197\n",
      "Train Epoch:2198 learning rate:1.0000e-05, Loss_tot:0.2932,\n",
      "Save model 2198\n",
      "Train Epoch:2199 learning rate:1.0000e-05, Loss_tot:0.2931,\n",
      "Save model 2199\n",
      "Train Epoch:2200 learning rate:1.0000e-05, Loss_tot:0.2929,\n",
      "Save model 2200\n",
      "Train Epoch:2201 learning rate:1.0000e-05, Loss_tot:0.2927,\n",
      "Save model 2201\n",
      "Train Epoch:2202 learning rate:1.0000e-05, Loss_tot:0.2926,\n",
      "Save model 2202\n",
      "Train Epoch:2203 learning rate:1.0000e-05, Loss_tot:0.2925,\n",
      "Save model 2203\n",
      "Train Epoch:2204 learning rate:1.0000e-05, Loss_tot:0.2923,\n",
      "Save model 2204\n",
      "Train Epoch:2205 learning rate:1.0000e-05, Loss_tot:0.2581,\n",
      "save model\n",
      "Save model 2205\n",
      "Train Epoch:2206 learning rate:1.0000e-05, Loss_tot:0.2920,\n",
      "Save model 2206\n",
      "Train Epoch:2207 learning rate:1.0000e-05, Loss_tot:0.2917,\n",
      "Save model 2207\n",
      "Train Epoch:2208 learning rate:1.0000e-05, Loss_tot:0.2905,\n",
      "Save model 2208\n",
      "Train Epoch:2209 learning rate:1.0000e-05, Loss_tot:0.2916,\n",
      "Save model 2209\n",
      "Train Epoch:2210 learning rate:1.0000e-05, Loss_tot:0.2897,\n",
      "Save model 2210\n",
      "Train Epoch:2211 learning rate:1.0000e-05, Loss_tot:0.2911,\n",
      "Save model 2211\n",
      "Train Epoch:2212 learning rate:1.0000e-05, Loss_tot:0.2910,\n",
      "Save model 2212\n",
      "Train Epoch:2213 learning rate:1.0000e-05, Loss_tot:0.2909,\n",
      "Save model 2213\n",
      "Train Epoch:2214 learning rate:1.0000e-05, Loss_tot:0.2907,\n",
      "Save model 2214\n",
      "Train Epoch:2215 learning rate:1.0000e-05, Loss_tot:0.2905,\n",
      "Save model 2215\n",
      "Train Epoch:2216 learning rate:1.0000e-05, Loss_tot:0.2903,\n",
      "Save model 2216\n",
      "Train Epoch:2217 learning rate:1.0000e-05, Loss_tot:0.2903,\n",
      "Save model 2217\n",
      "Train Epoch:2218 learning rate:1.0000e-05, Loss_tot:0.2901,\n",
      "Save model 2218\n",
      "Train Epoch:2219 learning rate:1.0000e-05, Loss_tot:0.2899,\n",
      "Save model 2219\n",
      "Train Epoch:2220 learning rate:1.0000e-05, Loss_tot:0.2897,\n",
      "Save model 2220\n",
      "Train Epoch:2221 learning rate:1.0000e-05, Loss_tot:0.2896,\n",
      "Save model 2221\n",
      "Train Epoch:2222 learning rate:1.0000e-05, Loss_tot:0.2895,\n",
      "Save model 2222\n",
      "Train Epoch:2223 learning rate:1.0000e-05, Loss_tot:0.2893,\n",
      "Save model 2223\n",
      "Train Epoch:2224 learning rate:1.0000e-05, Loss_tot:0.2891,\n",
      "Save model 2224\n",
      "Train Epoch:2225 learning rate:1.0000e-05, Loss_tot:0.2889,\n",
      "Save model 2225\n",
      "Train Epoch:2226 learning rate:1.0000e-05, Loss_tot:0.2888,\n",
      "Save model 2226\n",
      "Train Epoch:2227 learning rate:1.0000e-05, Loss_tot:0.2873,\n",
      "Save model 2227\n",
      "Train Epoch:2228 learning rate:1.0000e-05, Loss_tot:0.2884,\n",
      "Save model 2228\n",
      "Train Epoch:2229 learning rate:1.0000e-05, Loss_tot:0.2883,\n",
      "Save model 2229\n",
      "Train Epoch:2230 learning rate:1.0000e-05, Loss_tot:0.2881,\n",
      "Save model 2230\n",
      "Train Epoch:2231 learning rate:1.0000e-05, Loss_tot:0.2879,\n",
      "Save model 2231\n",
      "Train Epoch:2232 learning rate:1.0000e-05, Loss_tot:0.2816,\n",
      "Save model 2232\n",
      "Train Epoch:2233 learning rate:1.0000e-05, Loss_tot:0.2876,\n",
      "Save model 2233\n",
      "Train Epoch:2234 learning rate:1.0000e-05, Loss_tot:0.2875,\n",
      "Save model 2234\n",
      "Train Epoch:2235 learning rate:1.0000e-05, Loss_tot:0.2873,\n",
      "Save model 2235\n",
      "Train Epoch:2236 learning rate:1.0000e-05, Loss_tot:0.2871,\n",
      "Save model 2236\n",
      "Train Epoch:2237 learning rate:1.0000e-05, Loss_tot:0.2869,\n",
      "Save model 2237\n",
      "Train Epoch:2238 learning rate:1.0000e-05, Loss_tot:0.2868,\n",
      "Save model 2238\n",
      "Train Epoch:2239 learning rate:1.0000e-05, Loss_tot:0.2866,\n",
      "Save model 2239\n",
      "Train Epoch:2240 learning rate:1.0000e-05, Loss_tot:0.2865,\n",
      "Save model 2240\n",
      "Train Epoch:2241 learning rate:1.0000e-05, Loss_tot:0.2863,\n",
      "Save model 2241\n",
      "Train Epoch:2242 learning rate:1.0000e-05, Loss_tot:0.2541,\n",
      "save model\n",
      "Save model 2242\n",
      "Train Epoch:2243 learning rate:1.0000e-05, Loss_tot:0.2860,\n",
      "Save model 2243\n",
      "Train Epoch:2244 learning rate:1.0000e-05, Loss_tot:0.2859,\n",
      "Save model 2244\n",
      "Train Epoch:2245 learning rate:1.0000e-05, Loss_tot:0.2857,\n",
      "Save model 2245\n",
      "Train Epoch:2246 learning rate:1.0000e-05, Loss_tot:0.2855,\n",
      "Save model 2246\n",
      "Train Epoch:2247 learning rate:1.0000e-05, Loss_tot:0.2853,\n",
      "Save model 2247\n",
      "Train Epoch:2248 learning rate:1.0000e-05, Loss_tot:0.2852,\n",
      "Save model 2248\n",
      "Train Epoch:2249 learning rate:1.0000e-05, Loss_tot:0.2533,\n",
      "save model\n",
      "Save model 2249\n",
      "Train Epoch:2250 learning rate:1.0000e-05, Loss_tot:0.2849,\n",
      "Save model 2250\n",
      "Train Epoch:2251 learning rate:1.0000e-05, Loss_tot:0.2848,\n",
      "Save model 2251\n",
      "Train Epoch:2252 learning rate:1.0000e-05, Loss_tot:0.2846,\n",
      "Save model 2252\n",
      "Train Epoch:2253 learning rate:1.0000e-05, Loss_tot:0.2845,\n",
      "Save model 2253\n",
      "Train Epoch:2254 learning rate:1.0000e-05, Loss_tot:0.2827,\n",
      "Save model 2254\n",
      "Train Epoch:2255 learning rate:1.0000e-05, Loss_tot:0.2842,\n",
      "Save model 2255\n",
      "Train Epoch:2256 learning rate:1.0000e-05, Loss_tot:0.2821,\n",
      "Save model 2256\n",
      "Train Epoch:2257 learning rate:1.0000e-05, Loss_tot:0.2838,\n",
      "Save model 2257\n",
      "Train Epoch:2258 learning rate:1.0000e-05, Loss_tot:0.2836,\n",
      "Save model 2258\n",
      "Train Epoch:2259 learning rate:1.0000e-05, Loss_tot:0.2834,\n",
      "Save model 2259\n",
      "Train Epoch:2260 learning rate:1.0000e-05, Loss_tot:0.2832,\n",
      "Save model 2260\n",
      "Train Epoch:2261 learning rate:1.0000e-05, Loss_tot:0.2831,\n",
      "Save model 2261\n",
      "Train Epoch:2262 learning rate:1.0000e-05, Loss_tot:0.2829,\n",
      "Save model 2262\n",
      "Train Epoch:2263 learning rate:1.0000e-05, Loss_tot:0.2828,\n",
      "Save model 2263\n",
      "Train Epoch:2264 learning rate:1.0000e-05, Loss_tot:0.2826,\n",
      "Save model 2264\n",
      "Train Epoch:2265 learning rate:1.0000e-05, Loss_tot:0.2824,\n",
      "Save model 2265\n",
      "Train Epoch:2266 learning rate:1.0000e-05, Loss_tot:0.2822,\n",
      "Save model 2266\n",
      "Train Epoch:2267 learning rate:1.0000e-05, Loss_tot:0.2821,\n",
      "Save model 2267\n",
      "Train Epoch:2268 learning rate:1.0000e-05, Loss_tot:0.2819,\n",
      "Save model 2268\n",
      "Train Epoch:2269 learning rate:1.0000e-05, Loss_tot:0.2818,\n",
      "Save model 2269\n",
      "Train Epoch:2270 learning rate:1.0000e-05, Loss_tot:0.2816,\n",
      "Save model 2270\n",
      "Train Epoch:2271 learning rate:1.0000e-05, Loss_tot:0.2814,\n",
      "Save model 2271\n",
      "Train Epoch:2272 learning rate:1.0000e-05, Loss_tot:0.2506,\n",
      "save model\n",
      "Save model 2272\n",
      "Train Epoch:2273 learning rate:1.0000e-05, Loss_tot:0.2760,\n",
      "Save model 2273\n",
      "Train Epoch:2274 learning rate:1.0000e-05, Loss_tot:0.2810,\n",
      "Save model 2274\n",
      "Train Epoch:2275 learning rate:1.0000e-05, Loss_tot:0.2807,\n",
      "Save model 2275\n",
      "Train Epoch:2276 learning rate:1.0000e-05, Loss_tot:0.2806,\n",
      "Save model 2276\n",
      "Train Epoch:2277 learning rate:1.0000e-05, Loss_tot:0.2795,\n",
      "Save model 2277\n",
      "Train Epoch:2278 learning rate:1.0000e-05, Loss_tot:0.2803,\n",
      "Save model 2278\n",
      "Train Epoch:2279 learning rate:1.0000e-05, Loss_tot:0.2799,\n",
      "Save model 2279\n",
      "Train Epoch:2280 learning rate:1.0000e-05, Loss_tot:0.2799,\n",
      "Save model 2280\n",
      "Train Epoch:2281 learning rate:1.0000e-05, Loss_tot:0.2798,\n",
      "Save model 2281\n",
      "Train Epoch:2282 learning rate:1.0000e-05, Loss_tot:0.2797,\n",
      "Save model 2282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2283 learning rate:1.0000e-05, Loss_tot:0.2795,\n",
      "Save model 2283\n",
      "Train Epoch:2284 learning rate:1.0000e-05, Loss_tot:0.2794,\n",
      "Save model 2284\n",
      "Train Epoch:2285 learning rate:1.0000e-05, Loss_tot:0.2792,\n",
      "Save model 2285\n",
      "Train Epoch:2286 learning rate:1.0000e-05, Loss_tot:0.2790,\n",
      "Save model 2286\n",
      "Train Epoch:2287 learning rate:1.0000e-05, Loss_tot:0.2788,\n",
      "Save model 2287\n",
      "Train Epoch:2288 learning rate:1.0000e-05, Loss_tot:0.2787,\n",
      "Save model 2288\n",
      "Train Epoch:2289 learning rate:1.0000e-05, Loss_tot:0.2786,\n",
      "Save model 2289\n",
      "Train Epoch:2290 learning rate:1.0000e-05, Loss_tot:0.2784,\n",
      "Save model 2290\n",
      "Train Epoch:2291 learning rate:1.0000e-05, Loss_tot:0.2782,\n",
      "Save model 2291\n",
      "Train Epoch:2292 learning rate:1.0000e-05, Loss_tot:0.2780,\n",
      "Save model 2292\n",
      "Train Epoch:2293 learning rate:1.0000e-05, Loss_tot:0.2779,\n",
      "Save model 2293\n",
      "Train Epoch:2294 learning rate:1.0000e-05, Loss_tot:0.2777,\n",
      "Save model 2294\n",
      "Train Epoch:2295 learning rate:1.0000e-05, Loss_tot:0.2776,\n",
      "Save model 2295\n",
      "Train Epoch:2296 learning rate:1.0000e-05, Loss_tot:0.2767,\n",
      "Save model 2296\n",
      "Train Epoch:2297 learning rate:1.0000e-05, Loss_tot:0.2772,\n",
      "Save model 2297\n",
      "Train Epoch:2298 learning rate:1.0000e-05, Loss_tot:0.2478,\n",
      "save model\n",
      "Save model 2298\n",
      "Train Epoch:2299 learning rate:1.0000e-05, Loss_tot:0.2478,\n",
      "save model\n",
      "Save model 2299\n",
      "Train Epoch:2300 learning rate:1.0000e-05, Loss_tot:0.2477,\n",
      "save model\n",
      "Save model 2300\n",
      "Train Epoch:2301 learning rate:1.0000e-05, Loss_tot:0.2766,\n",
      "Save model 2301\n",
      "Train Epoch:2302 learning rate:1.0000e-05, Loss_tot:0.2765,\n",
      "Save model 2302\n",
      "Train Epoch:2303 learning rate:1.0000e-05, Loss_tot:0.2764,\n",
      "Save model 2303\n",
      "Train Epoch:2304 learning rate:1.0000e-05, Loss_tot:0.2762,\n",
      "Save model 2304\n",
      "Train Epoch:2305 learning rate:1.0000e-05, Loss_tot:0.2761,\n",
      "Save model 2305\n",
      "Train Epoch:2306 learning rate:1.0000e-05, Loss_tot:0.2759,\n",
      "Save model 2306\n",
      "Train Epoch:2307 learning rate:1.0000e-05, Loss_tot:0.2757,\n",
      "Save model 2307\n",
      "Train Epoch:2308 learning rate:1.0000e-05, Loss_tot:0.2755,\n",
      "Save model 2308\n",
      "Train Epoch:2309 learning rate:1.0000e-05, Loss_tot:0.2739,\n",
      "Save model 2309\n",
      "Train Epoch:2310 learning rate:1.0000e-05, Loss_tot:0.2739,\n",
      "Save model 2310\n",
      "Train Epoch:2311 learning rate:1.0000e-05, Loss_tot:0.2750,\n",
      "Save model 2311\n",
      "Train Epoch:2312 learning rate:1.0000e-05, Loss_tot:0.2748,\n",
      "Save model 2312\n",
      "Train Epoch:2313 learning rate:1.0000e-05, Loss_tot:0.2747,\n",
      "Save model 2313\n",
      "Train Epoch:2314 learning rate:1.0000e-05, Loss_tot:0.2745,\n",
      "Save model 2314\n",
      "Train Epoch:2315 learning rate:1.0000e-05, Loss_tot:0.2744,\n",
      "Save model 2315\n",
      "Train Epoch:2316 learning rate:1.0000e-05, Loss_tot:0.2742,\n",
      "Save model 2316\n",
      "Train Epoch:2317 learning rate:1.0000e-05, Loss_tot:0.2740,\n",
      "Save model 2317\n",
      "Train Epoch:2318 learning rate:1.0000e-05, Loss_tot:0.2737,\n",
      "Save model 2318\n",
      "Train Epoch:2319 learning rate:1.0000e-05, Loss_tot:0.2737,\n",
      "Save model 2319\n",
      "Train Epoch:2320 learning rate:1.0000e-05, Loss_tot:0.2735,\n",
      "Save model 2320\n",
      "Train Epoch:2321 learning rate:1.0000e-05, Loss_tot:0.2733,\n",
      "Save model 2321\n",
      "Train Epoch:2322 learning rate:1.0000e-05, Loss_tot:0.2730,\n",
      "Save model 2322\n",
      "Train Epoch:2323 learning rate:1.0000e-05, Loss_tot:0.2729,\n",
      "Save model 2323\n",
      "Train Epoch:2324 learning rate:1.0000e-05, Loss_tot:0.2710,\n",
      "Save model 2324\n",
      "Train Epoch:2325 learning rate:1.0000e-05, Loss_tot:0.2725,\n",
      "Save model 2325\n",
      "Train Epoch:2326 learning rate:1.0000e-05, Loss_tot:0.2448,\n",
      "save model\n",
      "Save model 2326\n",
      "Train Epoch:2327 learning rate:1.0000e-05, Loss_tot:0.2445,\n",
      "save model\n",
      "Save model 2327\n",
      "Train Epoch:2328 learning rate:1.0000e-05, Loss_tot:0.2444,\n",
      "save model\n",
      "Save model 2328\n",
      "Train Epoch:2329 learning rate:1.0000e-05, Loss_tot:0.2658,\n",
      "Save model 2329\n",
      "Train Epoch:2330 learning rate:1.0000e-05, Loss_tot:0.2716,\n",
      "Save model 2330\n",
      "Train Epoch:2331 learning rate:1.0000e-05, Loss_tot:0.2715,\n",
      "Save model 2331\n",
      "Train Epoch:2332 learning rate:1.0000e-05, Loss_tot:0.2713,\n",
      "Save model 2332\n",
      "Train Epoch:2333 learning rate:1.0000e-05, Loss_tot:0.2711,\n",
      "Save model 2333\n",
      "Train Epoch:2334 learning rate:1.0000e-05, Loss_tot:0.2710,\n",
      "Save model 2334\n",
      "Train Epoch:2335 learning rate:1.0000e-05, Loss_tot:0.2708,\n",
      "Save model 2335\n",
      "Train Epoch:2336 learning rate:1.0000e-05, Loss_tot:0.2706,\n",
      "Save model 2336\n",
      "Train Epoch:2337 learning rate:1.0000e-05, Loss_tot:0.2704,\n",
      "Save model 2337\n",
      "Train Epoch:2338 learning rate:1.0000e-05, Loss_tot:0.2702,\n",
      "Save model 2338\n",
      "Train Epoch:2339 learning rate:1.0000e-05, Loss_tot:0.2701,\n",
      "Save model 2339\n",
      "Train Epoch:2340 learning rate:1.0000e-05, Loss_tot:0.2699,\n",
      "Save model 2340\n",
      "Train Epoch:2341 learning rate:1.0000e-05, Loss_tot:0.2697,\n",
      "Save model 2341\n",
      "Train Epoch:2342 learning rate:1.0000e-05, Loss_tot:0.2695,\n",
      "Save model 2342\n",
      "Train Epoch:2343 learning rate:1.0000e-05, Loss_tot:0.2693,\n",
      "Save model 2343\n",
      "Train Epoch:2344 learning rate:1.0000e-05, Loss_tot:0.2691,\n",
      "Save model 2344\n",
      "Train Epoch:2345 learning rate:1.0000e-05, Loss_tot:0.2689,\n",
      "Save model 2345\n",
      "Train Epoch:2346 learning rate:1.0000e-05, Loss_tot:0.2688,\n",
      "Save model 2346\n",
      "Train Epoch:2347 learning rate:1.0000e-05, Loss_tot:0.2424,\n",
      "save model\n",
      "Save model 2347\n",
      "Train Epoch:2348 learning rate:1.0000e-05, Loss_tot:0.2423,\n",
      "save model\n",
      "Save model 2348\n",
      "Train Epoch:2349 learning rate:1.0000e-05, Loss_tot:0.2683,\n",
      "Save model 2349\n",
      "Train Epoch:2350 learning rate:1.0000e-05, Loss_tot:0.2421,\n",
      "save model\n",
      "Save model 2350\n",
      "Train Epoch:2351 learning rate:1.0000e-05, Loss_tot:0.2679,\n",
      "Save model 2351\n",
      "Train Epoch:2352 learning rate:1.0000e-05, Loss_tot:0.2418,\n",
      "save model\n",
      "Save model 2352\n",
      "Train Epoch:2353 learning rate:1.0000e-05, Loss_tot:0.2676,\n",
      "Save model 2353\n",
      "Train Epoch:2354 learning rate:1.0000e-05, Loss_tot:0.2674,\n",
      "Save model 2354\n",
      "Train Epoch:2355 learning rate:1.0000e-05, Loss_tot:0.2672,\n",
      "Save model 2355\n",
      "Train Epoch:2356 learning rate:1.0000e-05, Loss_tot:0.2671,\n",
      "Save model 2356\n",
      "Train Epoch:2357 learning rate:1.0000e-05, Loss_tot:0.2669,\n",
      "Save model 2357\n",
      "Train Epoch:2358 learning rate:1.0000e-05, Loss_tot:0.2668,\n",
      "Save model 2358\n",
      "Train Epoch:2359 learning rate:1.0000e-05, Loss_tot:0.2666,\n",
      "Save model 2359\n",
      "Train Epoch:2360 learning rate:1.0000e-05, Loss_tot:0.2664,\n",
      "Save model 2360\n",
      "Train Epoch:2361 learning rate:1.0000e-05, Loss_tot:0.2662,\n",
      "Save model 2361\n",
      "Train Epoch:2362 learning rate:1.0000e-05, Loss_tot:0.2659,\n",
      "Save model 2362\n",
      "Train Epoch:2363 learning rate:1.0000e-05, Loss_tot:0.2657,\n",
      "Save model 2363\n",
      "Train Epoch:2364 learning rate:1.0000e-05, Loss_tot:0.2656,\n",
      "Save model 2364\n",
      "Train Epoch:2365 learning rate:1.0000e-05, Loss_tot:0.2653,\n",
      "Save model 2365\n",
      "Train Epoch:2366 learning rate:1.0000e-05, Loss_tot:0.2651,\n",
      "Save model 2366\n",
      "Train Epoch:2367 learning rate:1.0000e-05, Loss_tot:0.2402,\n",
      "save model\n",
      "Save model 2367\n",
      "Train Epoch:2368 learning rate:1.0000e-05, Loss_tot:0.2401,\n",
      "save model\n",
      "Save model 2368\n",
      "Train Epoch:2369 learning rate:1.0000e-05, Loss_tot:0.2400,\n",
      "save model\n",
      "Save model 2369\n",
      "Train Epoch:2370 learning rate:1.0000e-05, Loss_tot:0.2399,\n",
      "save model\n",
      "Save model 2370\n",
      "Train Epoch:2371 learning rate:1.0000e-05, Loss_tot:0.2398,\n",
      "save model\n",
      "Save model 2371\n",
      "Train Epoch:2372 learning rate:1.0000e-05, Loss_tot:0.2397,\n",
      "save model\n",
      "Save model 2372\n",
      "Train Epoch:2373 learning rate:1.0000e-05, Loss_tot:0.2396,\n",
      "save model\n",
      "Save model 2373\n",
      "Train Epoch:2374 learning rate:1.0000e-05, Loss_tot:0.2395,\n",
      "save model\n",
      "Save model 2374\n",
      "Train Epoch:2375 learning rate:1.0000e-05, Loss_tot:0.2394,\n",
      "save model\n",
      "Save model 2375\n",
      "Train Epoch:2376 learning rate:1.0000e-05, Loss_tot:0.2393,\n",
      "save model\n",
      "Save model 2376\n",
      "Train Epoch:2377 learning rate:1.0000e-05, Loss_tot:0.2392,\n",
      "save model\n",
      "Save model 2377\n",
      "Train Epoch:2378 learning rate:1.0000e-05, Loss_tot:0.2392,\n",
      "save model\n",
      "Save model 2378\n",
      "Train Epoch:2379 learning rate:1.0000e-05, Loss_tot:0.2591,\n",
      "Save model 2379\n",
      "Train Epoch:2380 learning rate:1.0000e-05, Loss_tot:0.2629,\n",
      "Save model 2380\n",
      "Train Epoch:2381 learning rate:1.0000e-05, Loss_tot:0.2628,\n",
      "Save model 2381\n",
      "Train Epoch:2382 learning rate:1.0000e-05, Loss_tot:0.2626,\n",
      "Save model 2382\n",
      "Train Epoch:2383 learning rate:1.0000e-05, Loss_tot:0.2624,\n",
      "Save model 2383\n",
      "Train Epoch:2384 learning rate:1.0000e-05, Loss_tot:0.2623,\n",
      "Save model 2384\n",
      "Train Epoch:2385 learning rate:1.0000e-05, Loss_tot:0.2621,\n",
      "Save model 2385\n",
      "Train Epoch:2386 learning rate:1.0000e-05, Loss_tot:0.2618,\n",
      "Save model 2386\n",
      "Train Epoch:2387 learning rate:1.0000e-05, Loss_tot:0.2622,\n",
      "Save model 2387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2388 learning rate:1.0000e-05, Loss_tot:0.2620,\n",
      "Save model 2388\n",
      "Train Epoch:2389 learning rate:1.0000e-05, Loss_tot:0.2609,\n",
      "Save model 2389\n",
      "Train Epoch:2390 learning rate:1.0000e-05, Loss_tot:0.2565,\n",
      "Save model 2390\n",
      "Train Epoch:2391 learning rate:1.0000e-05, Loss_tot:0.2615,\n",
      "Save model 2391\n",
      "Train Epoch:2392 learning rate:1.0000e-05, Loss_tot:0.2613,\n",
      "Save model 2392\n",
      "Train Epoch:2393 learning rate:1.0000e-05, Loss_tot:0.2611,\n",
      "Save model 2393\n",
      "Train Epoch:2394 learning rate:1.0000e-05, Loss_tot:0.2609,\n",
      "Save model 2394\n",
      "Train Epoch:2395 learning rate:1.0000e-05, Loss_tot:0.2607,\n",
      "Save model 2395\n",
      "Train Epoch:2396 learning rate:1.0000e-05, Loss_tot:0.2605,\n",
      "Save model 2396\n",
      "Train Epoch:2397 learning rate:1.0000e-05, Loss_tot:0.2603,\n",
      "Save model 2397\n",
      "Train Epoch:2398 learning rate:1.0000e-05, Loss_tot:0.2372,\n",
      "save model\n",
      "Save model 2398\n",
      "Train Epoch:2399 learning rate:1.0000e-05, Loss_tot:0.2371,\n",
      "save model\n",
      "Save model 2399\n",
      "Train Epoch:2400 learning rate:1.0000e-05, Loss_tot:0.2369,\n",
      "save model\n",
      "Save model 2400\n",
      "Train Epoch:2401 learning rate:1.0000e-05, Loss_tot:0.2369,\n",
      "save model\n",
      "Save model 2401\n",
      "Train Epoch:2402 learning rate:1.0000e-05, Loss_tot:0.2368,\n",
      "save model\n",
      "Save model 2402\n",
      "Train Epoch:2403 learning rate:1.0000e-05, Loss_tot:0.2367,\n",
      "save model\n",
      "Save model 2403\n",
      "Train Epoch:2404 learning rate:1.0000e-05, Loss_tot:0.2367,\n",
      "save model\n",
      "Save model 2404\n",
      "Train Epoch:2405 learning rate:1.0000e-05, Loss_tot:0.2367,\n",
      "Save model 2405\n",
      "Train Epoch:2406 learning rate:1.0000e-05, Loss_tot:0.2366,\n",
      "save model\n",
      "Save model 2406\n",
      "Train Epoch:2407 learning rate:1.0000e-05, Loss_tot:0.2363,\n",
      "save model\n",
      "Save model 2407\n",
      "Train Epoch:2408 learning rate:1.0000e-05, Loss_tot:0.2363,\n",
      "save model\n",
      "Save model 2408\n",
      "Train Epoch:2409 learning rate:1.0000e-05, Loss_tot:0.2363,\n",
      "save model\n",
      "Save model 2409\n",
      "Train Epoch:2410 learning rate:1.0000e-05, Loss_tot:0.2363,\n",
      "Save model 2410\n",
      "Train Epoch:2411 learning rate:1.0000e-05, Loss_tot:0.2361,\n",
      "save model\n",
      "Save model 2411\n",
      "Train Epoch:2412 learning rate:1.0000e-05, Loss_tot:0.2361,\n",
      "save model\n",
      "Save model 2412\n",
      "Train Epoch:2413 learning rate:1.0000e-05, Loss_tot:0.2360,\n",
      "save model\n",
      "Save model 2413\n",
      "Train Epoch:2414 learning rate:1.0000e-05, Loss_tot:0.2359,\n",
      "save model\n",
      "Save model 2414\n",
      "Train Epoch:2415 learning rate:1.0000e-05, Loss_tot:0.2358,\n",
      "save model\n",
      "Save model 2415\n",
      "Train Epoch:2416 learning rate:1.0000e-05, Loss_tot:0.2358,\n",
      "save model\n",
      "Save model 2416\n",
      "Train Epoch:2417 learning rate:1.0000e-05, Loss_tot:0.2357,\n",
      "save model\n",
      "Save model 2417\n",
      "Train Epoch:2418 learning rate:1.0000e-05, Loss_tot:0.2356,\n",
      "save model\n",
      "Save model 2418\n",
      "Train Epoch:2419 learning rate:1.0000e-05, Loss_tot:0.2356,\n",
      "save model\n",
      "Save model 2419\n",
      "Train Epoch:2420 learning rate:1.0000e-05, Loss_tot:0.2355,\n",
      "save model\n",
      "Save model 2420\n",
      "Train Epoch:2421 learning rate:1.0000e-05, Loss_tot:0.2354,\n",
      "save model\n",
      "Save model 2421\n",
      "Train Epoch:2422 learning rate:1.0000e-05, Loss_tot:0.2352,\n",
      "save model\n",
      "Save model 2422\n",
      "Train Epoch:2423 learning rate:1.0000e-05, Loss_tot:0.2351,\n",
      "save model\n",
      "Save model 2423\n",
      "Train Epoch:2424 learning rate:1.0000e-05, Loss_tot:0.2350,\n",
      "save model\n",
      "Save model 2424\n",
      "Train Epoch:2425 learning rate:1.0000e-05, Loss_tot:0.2350,\n",
      "save model\n",
      "Save model 2425\n",
      "Train Epoch:2426 learning rate:1.0000e-05, Loss_tot:0.2349,\n",
      "save model\n",
      "Save model 2426\n",
      "Train Epoch:2427 learning rate:1.0000e-05, Loss_tot:0.2348,\n",
      "save model\n",
      "Save model 2427\n",
      "Train Epoch:2428 learning rate:1.0000e-05, Loss_tot:0.2348,\n",
      "save model\n",
      "Save model 2428\n",
      "Train Epoch:2429 learning rate:1.0000e-05, Loss_tot:0.2347,\n",
      "save model\n",
      "Save model 2429\n",
      "Train Epoch:2430 learning rate:1.0000e-05, Loss_tot:0.2346,\n",
      "save model\n",
      "Save model 2430\n",
      "Train Epoch:2431 learning rate:1.0000e-05, Loss_tot:0.2345,\n",
      "save model\n",
      "Save model 2431\n",
      "Train Epoch:2432 learning rate:1.0000e-05, Loss_tot:0.2345,\n",
      "save model\n",
      "Save model 2432\n",
      "Train Epoch:2433 learning rate:1.0000e-05, Loss_tot:0.2344,\n",
      "save model\n",
      "Save model 2433\n",
      "Train Epoch:2434 learning rate:1.0000e-05, Loss_tot:0.2343,\n",
      "save model\n",
      "Save model 2434\n",
      "Train Epoch:2435 learning rate:1.0000e-05, Loss_tot:0.2342,\n",
      "save model\n",
      "Save model 2435\n",
      "Train Epoch:2436 learning rate:1.0000e-05, Loss_tot:0.2342,\n",
      "save model\n",
      "Save model 2436\n",
      "Train Epoch:2437 learning rate:1.0000e-05, Loss_tot:0.2342,\n",
      "save model\n",
      "Save model 2437\n",
      "Train Epoch:2438 learning rate:1.0000e-05, Loss_tot:0.2341,\n",
      "save model\n",
      "Save model 2438\n",
      "Train Epoch:2439 learning rate:1.0000e-05, Loss_tot:0.2340,\n",
      "save model\n",
      "Save model 2439\n",
      "Train Epoch:2440 learning rate:1.0000e-05, Loss_tot:0.2339,\n",
      "save model\n",
      "Save model 2440\n",
      "Train Epoch:2441 learning rate:1.0000e-05, Loss_tot:0.2339,\n",
      "Save model 2441\n",
      "Train Epoch:2442 learning rate:1.0000e-05, Loss_tot:0.2338,\n",
      "save model\n",
      "Save model 2442\n",
      "Train Epoch:2443 learning rate:1.0000e-05, Loss_tot:0.2337,\n",
      "save model\n",
      "Save model 2443\n",
      "Train Epoch:2444 learning rate:1.0000e-05, Loss_tot:0.2337,\n",
      "save model\n",
      "Save model 2444\n",
      "Train Epoch:2445 learning rate:1.0000e-05, Loss_tot:0.2336,\n",
      "save model\n",
      "Save model 2445\n",
      "Train Epoch:2446 learning rate:1.0000e-05, Loss_tot:0.2336,\n",
      "save model\n",
      "Save model 2446\n",
      "Train Epoch:2447 learning rate:1.0000e-05, Loss_tot:0.2335,\n",
      "save model\n",
      "Save model 2447\n",
      "Train Epoch:2448 learning rate:1.0000e-05, Loss_tot:0.2334,\n",
      "save model\n",
      "Save model 2448\n",
      "Train Epoch:2449 learning rate:1.0000e-05, Loss_tot:0.2333,\n",
      "save model\n",
      "Save model 2449\n",
      "Train Epoch:2450 learning rate:1.0000e-05, Loss_tot:0.2331,\n",
      "save model\n",
      "Save model 2450\n",
      "Train Epoch:2451 learning rate:1.0000e-05, Loss_tot:0.2332,\n",
      "Save model 2451\n",
      "Train Epoch:2452 learning rate:1.0000e-05, Loss_tot:0.2332,\n",
      "Save model 2452\n",
      "Train Epoch:2453 learning rate:1.0000e-05, Loss_tot:0.2330,\n",
      "save model\n",
      "Save model 2453\n",
      "Train Epoch:2454 learning rate:1.0000e-05, Loss_tot:0.2329,\n",
      "save model\n",
      "Save model 2454\n",
      "Train Epoch:2455 learning rate:1.0000e-05, Loss_tot:0.2328,\n",
      "save model\n",
      "Save model 2455\n",
      "Train Epoch:2456 learning rate:1.0000e-05, Loss_tot:0.2328,\n",
      "save model\n",
      "Save model 2456\n",
      "Train Epoch:2457 learning rate:1.0000e-05, Loss_tot:0.2327,\n",
      "save model\n",
      "Save model 2457\n",
      "Train Epoch:2458 learning rate:1.0000e-05, Loss_tot:0.2326,\n",
      "save model\n",
      "Save model 2458\n",
      "Train Epoch:2459 learning rate:1.0000e-05, Loss_tot:0.2325,\n",
      "save model\n",
      "Save model 2459\n",
      "Train Epoch:2460 learning rate:1.0000e-05, Loss_tot:0.2325,\n",
      "save model\n",
      "Save model 2460\n",
      "Train Epoch:2461 learning rate:1.0000e-05, Loss_tot:0.2324,\n",
      "save model\n",
      "Save model 2461\n",
      "Train Epoch:2462 learning rate:1.0000e-05, Loss_tot:0.2323,\n",
      "save model\n",
      "Save model 2462\n",
      "Train Epoch:2463 learning rate:1.0000e-05, Loss_tot:0.2323,\n",
      "save model\n",
      "Save model 2463\n",
      "Train Epoch:2464 learning rate:1.0000e-05, Loss_tot:0.2322,\n",
      "save model\n",
      "Save model 2464\n",
      "Train Epoch:2465 learning rate:1.0000e-05, Loss_tot:0.2321,\n",
      "save model\n",
      "Save model 2465\n",
      "Train Epoch:2466 learning rate:1.0000e-05, Loss_tot:0.2320,\n",
      "save model\n",
      "Save model 2466\n",
      "Train Epoch:2467 learning rate:1.0000e-05, Loss_tot:0.2320,\n",
      "save model\n",
      "Save model 2467\n",
      "Train Epoch:2468 learning rate:1.0000e-05, Loss_tot:0.2319,\n",
      "save model\n",
      "Save model 2468\n",
      "Train Epoch:2469 learning rate:1.0000e-05, Loss_tot:0.2318,\n",
      "save model\n",
      "Save model 2469\n",
      "Train Epoch:2470 learning rate:1.0000e-05, Loss_tot:0.2317,\n",
      "save model\n",
      "Save model 2470\n",
      "Train Epoch:2471 learning rate:1.0000e-05, Loss_tot:0.2316,\n",
      "save model\n",
      "Save model 2471\n",
      "Train Epoch:2472 learning rate:1.0000e-05, Loss_tot:0.2317,\n",
      "Save model 2472\n",
      "Train Epoch:2473 learning rate:1.0000e-05, Loss_tot:0.2316,\n",
      "save model\n",
      "Save model 2473\n",
      "Train Epoch:2474 learning rate:1.0000e-05, Loss_tot:0.2315,\n",
      "save model\n",
      "Save model 2474\n",
      "Train Epoch:2475 learning rate:1.0000e-05, Loss_tot:0.2314,\n",
      "save model\n",
      "Save model 2475\n",
      "Train Epoch:2476 learning rate:1.0000e-05, Loss_tot:0.2314,\n",
      "save model\n",
      "Save model 2476\n",
      "Train Epoch:2477 learning rate:1.0000e-05, Loss_tot:0.2313,\n",
      "save model\n",
      "Save model 2477\n",
      "Train Epoch:2478 learning rate:1.0000e-05, Loss_tot:0.2313,\n",
      "save model\n",
      "Save model 2478\n",
      "Train Epoch:2479 learning rate:1.0000e-05, Loss_tot:0.2312,\n",
      "save model\n",
      "Save model 2479\n",
      "Train Epoch:2480 learning rate:1.0000e-05, Loss_tot:0.2311,\n",
      "save model\n",
      "Save model 2480\n",
      "Train Epoch:2481 learning rate:1.0000e-05, Loss_tot:0.2309,\n",
      "save model\n",
      "Save model 2481\n",
      "Train Epoch:2482 learning rate:1.0000e-05, Loss_tot:0.2310,\n",
      "Save model 2482\n",
      "Train Epoch:2483 learning rate:1.0000e-05, Loss_tot:0.2309,\n",
      "Save model 2483\n",
      "Train Epoch:2484 learning rate:1.0000e-05, Loss_tot:0.2308,\n",
      "save model\n",
      "Save model 2484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2485 learning rate:1.0000e-05, Loss_tot:0.2307,\n",
      "save model\n",
      "Save model 2485\n",
      "Train Epoch:2486 learning rate:1.0000e-05, Loss_tot:0.2306,\n",
      "save model\n",
      "Save model 2486\n",
      "Train Epoch:2487 learning rate:1.0000e-05, Loss_tot:0.2306,\n",
      "save model\n",
      "Save model 2487\n",
      "Train Epoch:2488 learning rate:1.0000e-05, Loss_tot:0.2305,\n",
      "save model\n",
      "Save model 2488\n",
      "Train Epoch:2489 learning rate:1.0000e-05, Loss_tot:0.2304,\n",
      "save model\n",
      "Save model 2489\n",
      "Train Epoch:2490 learning rate:1.0000e-05, Loss_tot:0.2303,\n",
      "save model\n",
      "Save model 2490\n",
      "Train Epoch:2491 learning rate:1.0000e-05, Loss_tot:0.2303,\n",
      "save model\n",
      "Save model 2491\n",
      "Train Epoch:2492 learning rate:1.0000e-05, Loss_tot:0.2302,\n",
      "save model\n",
      "Save model 2492\n",
      "Train Epoch:2493 learning rate:1.0000e-05, Loss_tot:0.2301,\n",
      "save model\n",
      "Save model 2493\n",
      "Train Epoch:2494 learning rate:1.0000e-05, Loss_tot:0.2300,\n",
      "save model\n",
      "Save model 2494\n",
      "Train Epoch:2495 learning rate:1.0000e-05, Loss_tot:0.2300,\n",
      "save model\n",
      "Save model 2495\n",
      "Train Epoch:2496 learning rate:1.0000e-05, Loss_tot:0.2299,\n",
      "save model\n",
      "Save model 2496\n",
      "Train Epoch:2497 learning rate:1.0000e-05, Loss_tot:0.2298,\n",
      "save model\n",
      "Save model 2497\n",
      "Train Epoch:2498 learning rate:1.0000e-05, Loss_tot:0.2298,\n",
      "save model\n",
      "Save model 2498\n",
      "Train Epoch:2499 learning rate:1.0000e-05, Loss_tot:0.2297,\n",
      "save model\n",
      "Save model 2499\n",
      "Train Epoch:2500 learning rate:1.0000e-05, Loss_tot:0.2296,\n",
      "save model\n",
      "Save model 2500\n",
      "Train Epoch:2501 learning rate:1.0000e-05, Loss_tot:0.2295,\n",
      "save model\n",
      "Save model 2501\n",
      "Train Epoch:2502 learning rate:1.0000e-05, Loss_tot:0.2294,\n",
      "save model\n",
      "Save model 2502\n",
      "Train Epoch:2503 learning rate:1.0000e-05, Loss_tot:0.2294,\n",
      "save model\n",
      "Save model 2503\n",
      "Train Epoch:2504 learning rate:1.0000e-05, Loss_tot:0.2293,\n",
      "save model\n",
      "Save model 2504\n",
      "Train Epoch:2505 learning rate:1.0000e-05, Loss_tot:0.2293,\n",
      "save model\n",
      "Save model 2505\n",
      "Train Epoch:2506 learning rate:1.0000e-05, Loss_tot:0.2292,\n",
      "save model\n",
      "Save model 2506\n",
      "Train Epoch:2507 learning rate:1.0000e-05, Loss_tot:0.2291,\n",
      "save model\n",
      "Save model 2507\n",
      "Train Epoch:2508 learning rate:1.0000e-05, Loss_tot:0.2291,\n",
      "save model\n",
      "Save model 2508\n",
      "Train Epoch:2509 learning rate:1.0000e-05, Loss_tot:0.2290,\n",
      "save model\n",
      "Save model 2509\n",
      "Train Epoch:2510 learning rate:1.0000e-05, Loss_tot:0.2289,\n",
      "save model\n",
      "Save model 2510\n",
      "Train Epoch:2511 learning rate:1.0000e-05, Loss_tot:0.2289,\n",
      "save model\n",
      "Save model 2511\n",
      "Train Epoch:2512 learning rate:1.0000e-05, Loss_tot:0.2288,\n",
      "save model\n",
      "Save model 2512\n",
      "Train Epoch:2513 learning rate:1.0000e-05, Loss_tot:0.2287,\n",
      "save model\n",
      "Save model 2513\n",
      "Train Epoch:2514 learning rate:1.0000e-05, Loss_tot:0.2285,\n",
      "save model\n",
      "Save model 2514\n",
      "Train Epoch:2515 learning rate:1.0000e-05, Loss_tot:0.2285,\n",
      "save model\n",
      "Save model 2515\n",
      "Train Epoch:2516 learning rate:1.0000e-05, Loss_tot:0.2284,\n",
      "save model\n",
      "Save model 2516\n",
      "Train Epoch:2517 learning rate:1.0000e-05, Loss_tot:0.2283,\n",
      "save model\n",
      "Save model 2517\n",
      "Train Epoch:2518 learning rate:1.0000e-05, Loss_tot:0.2283,\n",
      "save model\n",
      "Save model 2518\n",
      "Train Epoch:2519 learning rate:1.0000e-05, Loss_tot:0.2282,\n",
      "save model\n",
      "Save model 2519\n",
      "Train Epoch:2520 learning rate:1.0000e-05, Loss_tot:0.2281,\n",
      "save model\n",
      "Save model 2520\n",
      "Train Epoch:2521 learning rate:1.0000e-05, Loss_tot:0.2281,\n",
      "Save model 2521\n",
      "Train Epoch:2522 learning rate:1.0000e-05, Loss_tot:0.2280,\n",
      "save model\n",
      "Save model 2522\n",
      "Train Epoch:2523 learning rate:1.0000e-05, Loss_tot:0.2279,\n",
      "save model\n",
      "Save model 2523\n",
      "Train Epoch:2524 learning rate:1.0000e-05, Loss_tot:0.2278,\n",
      "save model\n",
      "Save model 2524\n",
      "Train Epoch:2525 learning rate:1.0000e-05, Loss_tot:0.2277,\n",
      "save model\n",
      "Save model 2525\n",
      "Train Epoch:2526 learning rate:1.0000e-05, Loss_tot:0.2276,\n",
      "save model\n",
      "Save model 2526\n",
      "Train Epoch:2527 learning rate:1.0000e-05, Loss_tot:0.2276,\n",
      "save model\n",
      "Save model 2527\n",
      "Train Epoch:2528 learning rate:1.0000e-05, Loss_tot:0.2275,\n",
      "save model\n",
      "Save model 2528\n",
      "Train Epoch:2529 learning rate:1.0000e-05, Loss_tot:0.2274,\n",
      "save model\n",
      "Save model 2529\n",
      "Train Epoch:2530 learning rate:1.0000e-05, Loss_tot:0.2274,\n",
      "save model\n",
      "Save model 2530\n",
      "Train Epoch:2531 learning rate:1.0000e-05, Loss_tot:0.2273,\n",
      "save model\n",
      "Save model 2531\n",
      "Train Epoch:2532 learning rate:1.0000e-05, Loss_tot:0.2272,\n",
      "save model\n",
      "Save model 2532\n",
      "Train Epoch:2533 learning rate:1.0000e-05, Loss_tot:0.2271,\n",
      "save model\n",
      "Save model 2533\n",
      "Train Epoch:2534 learning rate:1.0000e-05, Loss_tot:0.2270,\n",
      "save model\n",
      "Save model 2534\n",
      "Train Epoch:2535 learning rate:1.0000e-05, Loss_tot:0.2269,\n",
      "save model\n",
      "Save model 2535\n",
      "Train Epoch:2536 learning rate:1.0000e-05, Loss_tot:0.2268,\n",
      "save model\n",
      "Save model 2536\n",
      "Train Epoch:2537 learning rate:1.0000e-05, Loss_tot:0.2268,\n",
      "save model\n",
      "Save model 2537\n",
      "Train Epoch:2538 learning rate:1.0000e-05, Loss_tot:0.2267,\n",
      "save model\n",
      "Save model 2538\n",
      "Train Epoch:2539 learning rate:1.0000e-05, Loss_tot:0.2266,\n",
      "save model\n",
      "Save model 2539\n",
      "Train Epoch:2540 learning rate:1.0000e-05, Loss_tot:0.2265,\n",
      "save model\n",
      "Save model 2540\n",
      "Train Epoch:2541 learning rate:1.0000e-05, Loss_tot:0.2264,\n",
      "save model\n",
      "Save model 2541\n",
      "Train Epoch:2542 learning rate:1.0000e-05, Loss_tot:0.2268,\n",
      "Save model 2542\n",
      "Train Epoch:2543 learning rate:1.0000e-05, Loss_tot:0.2267,\n",
      "Save model 2543\n",
      "Train Epoch:2544 learning rate:1.0000e-05, Loss_tot:0.2267,\n",
      "Save model 2544\n",
      "Train Epoch:2545 learning rate:1.0000e-05, Loss_tot:0.2266,\n",
      "Save model 2545\n",
      "Train Epoch:2546 learning rate:1.0000e-05, Loss_tot:0.2265,\n",
      "Save model 2546\n",
      "Train Epoch:2547 learning rate:1.0000e-05, Loss_tot:0.2264,\n",
      "Save model 2547\n",
      "Train Epoch:2548 learning rate:1.0000e-05, Loss_tot:0.2264,\n",
      "save model\n",
      "Save model 2548\n",
      "Train Epoch:2549 learning rate:1.0000e-05, Loss_tot:0.2263,\n",
      "save model\n",
      "Save model 2549\n",
      "Train Epoch:2550 learning rate:1.0000e-05, Loss_tot:0.2262,\n",
      "save model\n",
      "Save model 2550\n",
      "Train Epoch:2551 learning rate:1.0000e-05, Loss_tot:0.2261,\n",
      "save model\n",
      "Save model 2551\n",
      "Train Epoch:2552 learning rate:1.0000e-05, Loss_tot:0.2260,\n",
      "save model\n",
      "Save model 2552\n",
      "Train Epoch:2553 learning rate:1.0000e-05, Loss_tot:0.2259,\n",
      "save model\n",
      "Save model 2553\n",
      "Train Epoch:2554 learning rate:1.0000e-05, Loss_tot:0.2258,\n",
      "save model\n",
      "Save model 2554\n",
      "Train Epoch:2555 learning rate:1.0000e-05, Loss_tot:0.2258,\n",
      "save model\n",
      "Save model 2555\n",
      "Train Epoch:2556 learning rate:1.0000e-05, Loss_tot:0.2257,\n",
      "save model\n",
      "Save model 2556\n",
      "Train Epoch:2557 learning rate:1.0000e-05, Loss_tot:0.2256,\n",
      "save model\n",
      "Save model 2557\n",
      "Train Epoch:2558 learning rate:1.0000e-05, Loss_tot:0.2256,\n",
      "save model\n",
      "Save model 2558\n",
      "Train Epoch:2559 learning rate:1.0000e-05, Loss_tot:0.2255,\n",
      "save model\n",
      "Save model 2559\n",
      "Train Epoch:2560 learning rate:1.0000e-05, Loss_tot:0.2254,\n",
      "save model\n",
      "Save model 2560\n",
      "Train Epoch:2561 learning rate:1.0000e-05, Loss_tot:0.2253,\n",
      "save model\n",
      "Save model 2561\n",
      "Train Epoch:2562 learning rate:1.0000e-05, Loss_tot:0.2253,\n",
      "Save model 2562\n",
      "Train Epoch:2563 learning rate:1.0000e-05, Loss_tot:0.2252,\n",
      "save model\n",
      "Save model 2563\n",
      "Train Epoch:2564 learning rate:1.0000e-05, Loss_tot:0.2251,\n",
      "save model\n",
      "Save model 2564\n",
      "Train Epoch:2565 learning rate:1.0000e-05, Loss_tot:0.2250,\n",
      "save model\n",
      "Save model 2565\n",
      "Train Epoch:2566 learning rate:1.0000e-05, Loss_tot:0.2250,\n",
      "save model\n",
      "Save model 2566\n",
      "Train Epoch:2567 learning rate:1.0000e-05, Loss_tot:0.2250,\n",
      "save model\n",
      "Save model 2567\n",
      "Train Epoch:2568 learning rate:1.0000e-05, Loss_tot:0.2249,\n",
      "save model\n",
      "Save model 2568\n",
      "Train Epoch:2569 learning rate:1.0000e-05, Loss_tot:0.2248,\n",
      "save model\n",
      "Save model 2569\n",
      "Train Epoch:2570 learning rate:1.0000e-05, Loss_tot:0.2247,\n",
      "save model\n",
      "Save model 2570\n",
      "Train Epoch:2571 learning rate:1.0000e-05, Loss_tot:0.2246,\n",
      "save model\n",
      "Save model 2571\n",
      "Train Epoch:2572 learning rate:1.0000e-05, Loss_tot:0.2245,\n",
      "save model\n",
      "Save model 2572\n",
      "Train Epoch:2573 learning rate:1.0000e-05, Loss_tot:0.2245,\n",
      "save model\n",
      "Save model 2573\n",
      "Train Epoch:2574 learning rate:1.0000e-05, Loss_tot:0.2244,\n",
      "save model\n",
      "Save model 2574\n",
      "Train Epoch:2575 learning rate:1.0000e-05, Loss_tot:0.2243,\n",
      "save model\n",
      "Save model 2575\n",
      "Train Epoch:2576 learning rate:1.0000e-05, Loss_tot:0.2242,\n",
      "save model\n",
      "Save model 2576\n",
      "Train Epoch:2577 learning rate:1.0000e-05, Loss_tot:0.2242,\n",
      "save model\n",
      "Save model 2577\n",
      "Train Epoch:2578 learning rate:1.0000e-05, Loss_tot:0.2241,\n",
      "save model\n",
      "Save model 2578\n",
      "Train Epoch:2579 learning rate:1.0000e-05, Loss_tot:0.2240,\n",
      "save model\n",
      "Save model 2579\n",
      "Train Epoch:2580 learning rate:1.0000e-05, Loss_tot:0.2239,\n",
      "save model\n",
      "Save model 2580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2581 learning rate:1.0000e-05, Loss_tot:0.2239,\n",
      "Save model 2581\n",
      "Train Epoch:2582 learning rate:1.0000e-05, Loss_tot:0.2239,\n",
      "save model\n",
      "Save model 2582\n",
      "Train Epoch:2583 learning rate:1.0000e-05, Loss_tot:0.2237,\n",
      "save model\n",
      "Save model 2583\n",
      "Train Epoch:2584 learning rate:1.0000e-05, Loss_tot:0.2236,\n",
      "save model\n",
      "Save model 2584\n",
      "Train Epoch:2585 learning rate:1.0000e-05, Loss_tot:0.2236,\n",
      "save model\n",
      "Save model 2585\n",
      "Train Epoch:2586 learning rate:1.0000e-05, Loss_tot:0.2236,\n",
      "save model\n",
      "Save model 2586\n",
      "Train Epoch:2587 learning rate:1.0000e-05, Loss_tot:0.2235,\n",
      "save model\n",
      "Save model 2587\n",
      "Train Epoch:2588 learning rate:1.0000e-05, Loss_tot:0.2234,\n",
      "save model\n",
      "Save model 2588\n",
      "Train Epoch:2589 learning rate:1.0000e-05, Loss_tot:0.2233,\n",
      "save model\n",
      "Save model 2589\n",
      "Train Epoch:2590 learning rate:1.0000e-05, Loss_tot:0.2232,\n",
      "save model\n",
      "Save model 2590\n",
      "Train Epoch:2591 learning rate:1.0000e-05, Loss_tot:0.2232,\n",
      "save model\n",
      "Save model 2591\n",
      "Train Epoch:2592 learning rate:1.0000e-05, Loss_tot:0.2231,\n",
      "save model\n",
      "Save model 2592\n",
      "Train Epoch:2593 learning rate:1.0000e-05, Loss_tot:0.2230,\n",
      "save model\n",
      "Save model 2593\n",
      "Train Epoch:2594 learning rate:1.0000e-05, Loss_tot:0.2229,\n",
      "save model\n",
      "Save model 2594\n",
      "Train Epoch:2595 learning rate:1.0000e-05, Loss_tot:0.2229,\n",
      "save model\n",
      "Save model 2595\n",
      "Train Epoch:2596 learning rate:1.0000e-05, Loss_tot:0.2228,\n",
      "save model\n",
      "Save model 2596\n",
      "Train Epoch:2597 learning rate:1.0000e-05, Loss_tot:0.2227,\n",
      "save model\n",
      "Save model 2597\n",
      "Train Epoch:2598 learning rate:1.0000e-05, Loss_tot:0.2227,\n",
      "save model\n",
      "Save model 2598\n",
      "Train Epoch:2599 learning rate:1.0000e-05, Loss_tot:0.2226,\n",
      "save model\n",
      "Save model 2599\n",
      "Train Epoch:2600 learning rate:1.0000e-05, Loss_tot:0.2225,\n",
      "save model\n",
      "Save model 2600\n",
      "Train Epoch:2601 learning rate:1.0000e-05, Loss_tot:0.2224,\n",
      "save model\n",
      "Save model 2601\n",
      "Train Epoch:2602 learning rate:1.0000e-05, Loss_tot:0.2224,\n",
      "save model\n",
      "Save model 2602\n",
      "Train Epoch:2603 learning rate:1.0000e-05, Loss_tot:0.2223,\n",
      "save model\n",
      "Save model 2603\n",
      "Train Epoch:2604 learning rate:1.0000e-05, Loss_tot:0.2222,\n",
      "save model\n",
      "Save model 2604\n",
      "Train Epoch:2605 learning rate:1.0000e-05, Loss_tot:0.2222,\n",
      "save model\n",
      "Save model 2605\n",
      "Train Epoch:2606 learning rate:1.0000e-05, Loss_tot:0.2221,\n",
      "save model\n",
      "Save model 2606\n",
      "Train Epoch:2607 learning rate:1.0000e-05, Loss_tot:0.2220,\n",
      "save model\n",
      "Save model 2607\n",
      "Train Epoch:2608 learning rate:1.0000e-05, Loss_tot:0.2220,\n",
      "save model\n",
      "Save model 2608\n",
      "Train Epoch:2609 learning rate:1.0000e-05, Loss_tot:0.2219,\n",
      "save model\n",
      "Save model 2609\n",
      "Train Epoch:2610 learning rate:1.0000e-05, Loss_tot:0.2218,\n",
      "save model\n",
      "Save model 2610\n",
      "Train Epoch:2611 learning rate:1.0000e-05, Loss_tot:0.2218,\n",
      "save model\n",
      "Save model 2611\n",
      "Train Epoch:2612 learning rate:1.0000e-05, Loss_tot:0.2217,\n",
      "save model\n",
      "Save model 2612\n",
      "Train Epoch:2613 learning rate:1.0000e-05, Loss_tot:0.2216,\n",
      "save model\n",
      "Save model 2613\n",
      "Train Epoch:2614 learning rate:1.0000e-05, Loss_tot:0.2216,\n",
      "save model\n",
      "Save model 2614\n",
      "Train Epoch:2615 learning rate:1.0000e-05, Loss_tot:0.2215,\n",
      "save model\n",
      "Save model 2615\n",
      "Train Epoch:2616 learning rate:1.0000e-05, Loss_tot:0.2214,\n",
      "save model\n",
      "Save model 2616\n",
      "Train Epoch:2617 learning rate:1.0000e-05, Loss_tot:0.2214,\n",
      "save model\n",
      "Save model 2617\n",
      "Train Epoch:2618 learning rate:1.0000e-05, Loss_tot:0.2213,\n",
      "save model\n",
      "Save model 2618\n",
      "Train Epoch:2619 learning rate:1.0000e-05, Loss_tot:0.2212,\n",
      "save model\n",
      "Save model 2619\n",
      "Train Epoch:2620 learning rate:1.0000e-05, Loss_tot:0.2211,\n",
      "save model\n",
      "Save model 2620\n",
      "Train Epoch:2621 learning rate:1.0000e-05, Loss_tot:0.2211,\n",
      "save model\n",
      "Save model 2621\n",
      "Train Epoch:2622 learning rate:1.0000e-05, Loss_tot:0.2211,\n",
      "save model\n",
      "Save model 2622\n",
      "Train Epoch:2623 learning rate:1.0000e-05, Loss_tot:0.2209,\n",
      "save model\n",
      "Save model 2623\n",
      "Train Epoch:2624 learning rate:1.0000e-05, Loss_tot:0.2209,\n",
      "save model\n",
      "Save model 2624\n",
      "Train Epoch:2625 learning rate:1.0000e-05, Loss_tot:0.2209,\n",
      "save model\n",
      "Save model 2625\n",
      "Train Epoch:2626 learning rate:1.0000e-05, Loss_tot:0.2208,\n",
      "save model\n",
      "Save model 2626\n",
      "Train Epoch:2627 learning rate:1.0000e-05, Loss_tot:0.2208,\n",
      "save model\n",
      "Save model 2627\n",
      "Train Epoch:2628 learning rate:1.0000e-05, Loss_tot:0.2207,\n",
      "save model\n",
      "Save model 2628\n",
      "Train Epoch:2629 learning rate:1.0000e-05, Loss_tot:0.2206,\n",
      "save model\n",
      "Save model 2629\n",
      "Train Epoch:2630 learning rate:1.0000e-05, Loss_tot:0.2205,\n",
      "save model\n",
      "Save model 2630\n",
      "Train Epoch:2631 learning rate:1.0000e-05, Loss_tot:0.2204,\n",
      "save model\n",
      "Save model 2631\n",
      "Train Epoch:2632 learning rate:1.0000e-05, Loss_tot:0.2204,\n",
      "save model\n",
      "Save model 2632\n",
      "Train Epoch:2633 learning rate:1.0000e-05, Loss_tot:0.2203,\n",
      "save model\n",
      "Save model 2633\n",
      "Train Epoch:2634 learning rate:1.0000e-05, Loss_tot:0.2202,\n",
      "save model\n",
      "Save model 2634\n",
      "Train Epoch:2635 learning rate:1.0000e-05, Loss_tot:0.2202,\n",
      "save model\n",
      "Save model 2635\n",
      "Train Epoch:2636 learning rate:1.0000e-05, Loss_tot:0.2201,\n",
      "save model\n",
      "Save model 2636\n",
      "Train Epoch:2637 learning rate:1.0000e-05, Loss_tot:0.2200,\n",
      "save model\n",
      "Save model 2637\n",
      "Train Epoch:2638 learning rate:1.0000e-05, Loss_tot:0.2200,\n",
      "save model\n",
      "Save model 2638\n",
      "Train Epoch:2639 learning rate:1.0000e-05, Loss_tot:0.2199,\n",
      "save model\n",
      "Save model 2639\n",
      "Train Epoch:2640 learning rate:1.0000e-05, Loss_tot:0.2198,\n",
      "save model\n",
      "Save model 2640\n",
      "Train Epoch:2641 learning rate:1.0000e-05, Loss_tot:0.2198,\n",
      "save model\n",
      "Save model 2641\n",
      "Train Epoch:2642 learning rate:1.0000e-05, Loss_tot:0.2197,\n",
      "save model\n",
      "Save model 2642\n",
      "Train Epoch:2643 learning rate:1.0000e-05, Loss_tot:0.2196,\n",
      "save model\n",
      "Save model 2643\n",
      "Train Epoch:2644 learning rate:1.0000e-05, Loss_tot:0.2196,\n",
      "save model\n",
      "Save model 2644\n",
      "Train Epoch:2645 learning rate:1.0000e-05, Loss_tot:0.2195,\n",
      "save model\n",
      "Save model 2645\n",
      "Train Epoch:2646 learning rate:1.0000e-05, Loss_tot:0.2194,\n",
      "save model\n",
      "Save model 2646\n",
      "Train Epoch:2647 learning rate:1.0000e-05, Loss_tot:0.2194,\n",
      "save model\n",
      "Save model 2647\n",
      "Train Epoch:2648 learning rate:1.0000e-05, Loss_tot:0.2193,\n",
      "save model\n",
      "Save model 2648\n",
      "Train Epoch:2649 learning rate:1.0000e-05, Loss_tot:0.2193,\n",
      "save model\n",
      "Save model 2649\n",
      "Train Epoch:2650 learning rate:1.0000e-05, Loss_tot:0.2192,\n",
      "save model\n",
      "Save model 2650\n",
      "Train Epoch:2651 learning rate:1.0000e-05, Loss_tot:0.2191,\n",
      "save model\n",
      "Save model 2651\n",
      "Train Epoch:2652 learning rate:1.0000e-05, Loss_tot:0.2191,\n",
      "Save model 2652\n",
      "Train Epoch:2653 learning rate:1.0000e-05, Loss_tot:0.2190,\n",
      "save model\n",
      "Save model 2653\n",
      "Train Epoch:2654 learning rate:1.0000e-05, Loss_tot:0.2189,\n",
      "save model\n",
      "Save model 2654\n",
      "Train Epoch:2655 learning rate:1.0000e-05, Loss_tot:0.2188,\n",
      "save model\n",
      "Save model 2655\n",
      "Train Epoch:2656 learning rate:1.0000e-05, Loss_tot:0.2188,\n",
      "save model\n",
      "Save model 2656\n",
      "Train Epoch:2657 learning rate:1.0000e-05, Loss_tot:0.2188,\n",
      "save model\n",
      "Save model 2657\n",
      "Train Epoch:2658 learning rate:1.0000e-05, Loss_tot:0.2187,\n",
      "save model\n",
      "Save model 2658\n",
      "Train Epoch:2659 learning rate:1.0000e-05, Loss_tot:0.2186,\n",
      "save model\n",
      "Save model 2659\n",
      "Train Epoch:2660 learning rate:1.0000e-05, Loss_tot:0.2185,\n",
      "save model\n",
      "Save model 2660\n",
      "Train Epoch:2661 learning rate:1.0000e-05, Loss_tot:0.2184,\n",
      "save model\n",
      "Save model 2661\n",
      "Train Epoch:2662 learning rate:1.0000e-05, Loss_tot:0.2185,\n",
      "Save model 2662\n",
      "Train Epoch:2663 learning rate:1.0000e-05, Loss_tot:0.2185,\n",
      "Save model 2663\n",
      "Train Epoch:2664 learning rate:1.0000e-05, Loss_tot:0.2183,\n",
      "save model\n",
      "Save model 2664\n",
      "Train Epoch:2665 learning rate:1.0000e-05, Loss_tot:0.2182,\n",
      "save model\n",
      "Save model 2665\n",
      "Train Epoch:2666 learning rate:1.0000e-05, Loss_tot:0.2182,\n",
      "Save model 2666\n",
      "Train Epoch:2667 learning rate:1.0000e-05, Loss_tot:0.2182,\n",
      "Save model 2667\n",
      "Train Epoch:2668 learning rate:1.0000e-05, Loss_tot:0.2182,\n",
      "Save model 2668\n",
      "Train Epoch:2669 learning rate:1.0000e-05, Loss_tot:0.2181,\n",
      "save model\n",
      "Save model 2669\n",
      "Train Epoch:2670 learning rate:1.0000e-05, Loss_tot:0.2181,\n",
      "save model\n",
      "Save model 2670\n",
      "Train Epoch:2671 learning rate:1.0000e-05, Loss_tot:0.2180,\n",
      "save model\n",
      "Save model 2671\n",
      "Train Epoch:2672 learning rate:1.0000e-05, Loss_tot:0.2179,\n",
      "save model\n",
      "Save model 2672\n",
      "Train Epoch:2673 learning rate:1.0000e-05, Loss_tot:0.2177,\n",
      "save model\n",
      "Save model 2673\n",
      "Train Epoch:2674 learning rate:1.0000e-05, Loss_tot:0.2176,\n",
      "save model\n",
      "Save model 2674\n",
      "Train Epoch:2675 learning rate:1.0000e-05, Loss_tot:0.2177,\n",
      "Save model 2675\n",
      "Train Epoch:2676 learning rate:1.0000e-05, Loss_tot:0.2177,\n",
      "Save model 2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2677 learning rate:1.0000e-05, Loss_tot:0.2176,\n",
      "save model\n",
      "Save model 2677\n",
      "Train Epoch:2678 learning rate:1.0000e-05, Loss_tot:0.2174,\n",
      "save model\n",
      "Save model 2678\n",
      "Train Epoch:2679 learning rate:1.0000e-05, Loss_tot:0.2173,\n",
      "save model\n",
      "Save model 2679\n",
      "Train Epoch:2680 learning rate:1.0000e-05, Loss_tot:0.2173,\n",
      "save model\n",
      "Save model 2680\n",
      "Train Epoch:2681 learning rate:1.0000e-05, Loss_tot:0.2173,\n",
      "save model\n",
      "Save model 2681\n",
      "Train Epoch:2682 learning rate:1.0000e-05, Loss_tot:0.2172,\n",
      "save model\n",
      "Save model 2682\n",
      "Train Epoch:2683 learning rate:1.0000e-05, Loss_tot:0.2172,\n",
      "save model\n",
      "Save model 2683\n",
      "Train Epoch:2684 learning rate:1.0000e-05, Loss_tot:0.2171,\n",
      "save model\n",
      "Save model 2684\n",
      "Train Epoch:2685 learning rate:1.0000e-05, Loss_tot:0.2169,\n",
      "save model\n",
      "Save model 2685\n",
      "Train Epoch:2686 learning rate:1.0000e-05, Loss_tot:0.2169,\n",
      "save model\n",
      "Save model 2686\n",
      "Train Epoch:2687 learning rate:1.0000e-05, Loss_tot:0.2168,\n",
      "save model\n",
      "Save model 2687\n",
      "Train Epoch:2688 learning rate:1.0000e-05, Loss_tot:0.2167,\n",
      "save model\n",
      "Save model 2688\n",
      "Train Epoch:2689 learning rate:1.0000e-05, Loss_tot:0.2167,\n",
      "save model\n",
      "Save model 2689\n",
      "Train Epoch:2690 learning rate:1.0000e-05, Loss_tot:0.2167,\n",
      "save model\n",
      "Save model 2690\n",
      "Train Epoch:2691 learning rate:1.0000e-05, Loss_tot:0.2166,\n",
      "save model\n",
      "Save model 2691\n",
      "Train Epoch:2692 learning rate:1.0000e-05, Loss_tot:0.2166,\n",
      "save model\n",
      "Save model 2692\n",
      "Train Epoch:2693 learning rate:1.0000e-05, Loss_tot:0.2165,\n",
      "save model\n",
      "Save model 2693\n",
      "Train Epoch:2694 learning rate:1.0000e-05, Loss_tot:0.2164,\n",
      "save model\n",
      "Save model 2694\n",
      "Train Epoch:2695 learning rate:1.0000e-05, Loss_tot:0.2168,\n",
      "Save model 2695\n",
      "Train Epoch:2696 learning rate:1.0000e-05, Loss_tot:0.2167,\n",
      "Save model 2696\n",
      "Train Epoch:2697 learning rate:1.0000e-05, Loss_tot:0.2166,\n",
      "Save model 2697\n",
      "Train Epoch:2698 learning rate:1.0000e-05, Loss_tot:0.2162,\n",
      "save model\n",
      "Save model 2698\n",
      "Train Epoch:2699 learning rate:1.0000e-05, Loss_tot:0.2161,\n",
      "save model\n",
      "Save model 2699\n",
      "Train Epoch:2700 learning rate:1.0000e-05, Loss_tot:0.2165,\n",
      "Save model 2700\n",
      "Train Epoch:2701 learning rate:1.0000e-05, Loss_tot:0.2165,\n",
      "Save model 2701\n",
      "Train Epoch:2702 learning rate:1.0000e-05, Loss_tot:0.2164,\n",
      "Save model 2702\n",
      "Train Epoch:2703 learning rate:1.0000e-05, Loss_tot:0.2163,\n",
      "Save model 2703\n",
      "Train Epoch:2704 learning rate:1.0000e-05, Loss_tot:0.2162,\n",
      "Save model 2704\n",
      "Train Epoch:2705 learning rate:1.0000e-05, Loss_tot:0.2162,\n",
      "Save model 2705\n",
      "Train Epoch:2706 learning rate:1.0000e-05, Loss_tot:0.2161,\n",
      "save model\n",
      "Save model 2706\n",
      "Train Epoch:2707 learning rate:1.0000e-05, Loss_tot:0.2161,\n",
      "save model\n",
      "Save model 2707\n",
      "Train Epoch:2708 learning rate:1.0000e-05, Loss_tot:0.2160,\n",
      "save model\n",
      "Save model 2708\n",
      "Train Epoch:2709 learning rate:1.0000e-05, Loss_tot:0.2160,\n",
      "save model\n",
      "Save model 2709\n",
      "Train Epoch:2710 learning rate:1.0000e-05, Loss_tot:0.2159,\n",
      "save model\n",
      "Save model 2710\n",
      "Train Epoch:2711 learning rate:1.0000e-05, Loss_tot:0.2159,\n",
      "save model\n",
      "Save model 2711\n",
      "Train Epoch:2712 learning rate:1.0000e-05, Loss_tot:0.2157,\n",
      "save model\n",
      "Save model 2712\n",
      "Train Epoch:2713 learning rate:1.0000e-05, Loss_tot:0.2157,\n",
      "save model\n",
      "Save model 2713\n",
      "Train Epoch:2714 learning rate:1.0000e-05, Loss_tot:0.2157,\n",
      "save model\n",
      "Save model 2714\n",
      "Train Epoch:2715 learning rate:1.0000e-05, Loss_tot:0.2156,\n",
      "save model\n",
      "Save model 2715\n",
      "Train Epoch:2716 learning rate:1.0000e-05, Loss_tot:0.2155,\n",
      "save model\n",
      "Save model 2716\n",
      "Train Epoch:2717 learning rate:1.0000e-05, Loss_tot:0.2155,\n",
      "save model\n",
      "Save model 2717\n",
      "Train Epoch:2718 learning rate:1.0000e-05, Loss_tot:0.2155,\n",
      "save model\n",
      "Save model 2718\n",
      "Train Epoch:2719 learning rate:1.0000e-05, Loss_tot:0.2154,\n",
      "save model\n",
      "Save model 2719\n",
      "Train Epoch:2720 learning rate:1.0000e-05, Loss_tot:0.2153,\n",
      "save model\n",
      "Save model 2720\n",
      "Train Epoch:2721 learning rate:1.0000e-05, Loss_tot:0.2152,\n",
      "save model\n",
      "Save model 2721\n",
      "Train Epoch:2722 learning rate:1.0000e-05, Loss_tot:0.2152,\n",
      "save model\n",
      "Save model 2722\n",
      "Train Epoch:2723 learning rate:1.0000e-05, Loss_tot:0.2151,\n",
      "save model\n",
      "Save model 2723\n",
      "Train Epoch:2724 learning rate:1.0000e-05, Loss_tot:0.2150,\n",
      "save model\n",
      "Save model 2724\n",
      "Train Epoch:2725 learning rate:1.0000e-05, Loss_tot:0.2150,\n",
      "save model\n",
      "Save model 2725\n",
      "Train Epoch:2726 learning rate:1.0000e-05, Loss_tot:0.2149,\n",
      "save model\n",
      "Save model 2726\n",
      "Train Epoch:2727 learning rate:1.0000e-05, Loss_tot:0.2149,\n",
      "Save model 2727\n",
      "Train Epoch:2728 learning rate:1.0000e-05, Loss_tot:0.2148,\n",
      "save model\n",
      "Save model 2728\n",
      "Train Epoch:2729 learning rate:1.0000e-05, Loss_tot:0.2148,\n",
      "save model\n",
      "Save model 2729\n",
      "Train Epoch:2730 learning rate:1.0000e-05, Loss_tot:0.2147,\n",
      "save model\n",
      "Save model 2730\n",
      "Train Epoch:2731 learning rate:1.0000e-05, Loss_tot:0.2147,\n",
      "save model\n",
      "Save model 2731\n",
      "Train Epoch:2732 learning rate:1.0000e-05, Loss_tot:0.2146,\n",
      "save model\n",
      "Save model 2732\n",
      "Train Epoch:2733 learning rate:1.0000e-05, Loss_tot:0.2145,\n",
      "save model\n",
      "Save model 2733\n",
      "Train Epoch:2734 learning rate:1.0000e-05, Loss_tot:0.2145,\n",
      "Save model 2734\n",
      "Train Epoch:2735 learning rate:1.0000e-05, Loss_tot:0.2145,\n",
      "save model\n",
      "Save model 2735\n",
      "Train Epoch:2736 learning rate:1.0000e-05, Loss_tot:0.2144,\n",
      "save model\n",
      "Save model 2736\n",
      "Train Epoch:2737 learning rate:1.0000e-05, Loss_tot:0.2143,\n",
      "save model\n",
      "Save model 2737\n",
      "Train Epoch:2738 learning rate:1.0000e-05, Loss_tot:0.2143,\n",
      "save model\n",
      "Save model 2738\n",
      "Train Epoch:2739 learning rate:1.0000e-05, Loss_tot:0.2143,\n",
      "save model\n",
      "Save model 2739\n",
      "Train Epoch:2740 learning rate:1.0000e-05, Loss_tot:0.2142,\n",
      "save model\n",
      "Save model 2740\n",
      "Train Epoch:2741 learning rate:1.0000e-05, Loss_tot:0.2141,\n",
      "save model\n",
      "Save model 2741\n",
      "Train Epoch:2742 learning rate:1.0000e-05, Loss_tot:0.2140,\n",
      "save model\n",
      "Save model 2742\n",
      "Train Epoch:2743 learning rate:1.0000e-05, Loss_tot:0.2139,\n",
      "save model\n",
      "Save model 2743\n",
      "Train Epoch:2744 learning rate:1.0000e-05, Loss_tot:0.2140,\n",
      "Save model 2744\n",
      "Train Epoch:2745 learning rate:1.0000e-05, Loss_tot:0.2140,\n",
      "Save model 2745\n",
      "Train Epoch:2746 learning rate:1.0000e-05, Loss_tot:0.2139,\n",
      "save model\n",
      "Save model 2746\n",
      "Train Epoch:2747 learning rate:1.0000e-05, Loss_tot:0.2137,\n",
      "save model\n",
      "Save model 2747\n",
      "Train Epoch:2748 learning rate:1.0000e-05, Loss_tot:0.2137,\n",
      "Save model 2748\n",
      "Train Epoch:2749 learning rate:1.0000e-05, Loss_tot:0.2137,\n",
      "Save model 2749\n",
      "Train Epoch:2750 learning rate:1.0000e-05, Loss_tot:0.2137,\n",
      "Save model 2750\n",
      "Train Epoch:2751 learning rate:1.0000e-05, Loss_tot:0.2137,\n",
      "save model\n",
      "Save model 2751\n",
      "Train Epoch:2752 learning rate:1.0000e-05, Loss_tot:0.2136,\n",
      "save model\n",
      "Save model 2752\n",
      "Train Epoch:2753 learning rate:1.0000e-05, Loss_tot:0.2135,\n",
      "save model\n",
      "Save model 2753\n",
      "Train Epoch:2754 learning rate:1.0000e-05, Loss_tot:0.2134,\n",
      "save model\n",
      "Save model 2754\n",
      "Train Epoch:2755 learning rate:1.0000e-05, Loss_tot:0.2133,\n",
      "save model\n",
      "Save model 2755\n",
      "Train Epoch:2756 learning rate:1.0000e-05, Loss_tot:0.2132,\n",
      "save model\n",
      "Save model 2756\n",
      "Train Epoch:2757 learning rate:1.0000e-05, Loss_tot:0.2132,\n",
      "Save model 2757\n",
      "Train Epoch:2758 learning rate:1.0000e-05, Loss_tot:0.2131,\n",
      "save model\n",
      "Save model 2758\n",
      "Train Epoch:2759 learning rate:1.0000e-05, Loss_tot:0.2130,\n",
      "save model\n",
      "Save model 2759\n",
      "Train Epoch:2760 learning rate:1.0000e-05, Loss_tot:0.2130,\n",
      "save model\n",
      "Save model 2760\n",
      "Train Epoch:2761 learning rate:1.0000e-05, Loss_tot:0.2130,\n",
      "save model\n",
      "Save model 2761\n",
      "Train Epoch:2762 learning rate:1.0000e-05, Loss_tot:0.2129,\n",
      "save model\n",
      "Save model 2762\n",
      "Train Epoch:2763 learning rate:1.0000e-05, Loss_tot:0.2128,\n",
      "save model\n",
      "Save model 2763\n",
      "Train Epoch:2764 learning rate:1.0000e-05, Loss_tot:0.2128,\n",
      "save model\n",
      "Save model 2764\n",
      "Train Epoch:2765 learning rate:1.0000e-05, Loss_tot:0.2128,\n",
      "save model\n",
      "Save model 2765\n",
      "Train Epoch:2766 learning rate:1.0000e-05, Loss_tot:0.2126,\n",
      "save model\n",
      "Save model 2766\n",
      "Train Epoch:2767 learning rate:1.0000e-05, Loss_tot:0.2126,\n",
      "save model\n",
      "Save model 2767\n",
      "Train Epoch:2768 learning rate:1.0000e-05, Loss_tot:0.2125,\n",
      "save model\n",
      "Save model 2768\n",
      "Train Epoch:2769 learning rate:1.0000e-05, Loss_tot:0.2125,\n",
      "save model\n",
      "Save model 2769\n",
      "Train Epoch:2770 learning rate:1.0000e-05, Loss_tot:0.2124,\n",
      "save model\n",
      "Save model 2770\n",
      "Train Epoch:2771 learning rate:1.0000e-05, Loss_tot:0.2124,\n",
      "save model\n",
      "Save model 2771\n",
      "Train Epoch:2772 learning rate:1.0000e-05, Loss_tot:0.2124,\n",
      "save model\n",
      "Save model 2772\n",
      "Train Epoch:2773 learning rate:1.0000e-05, Loss_tot:0.2123,\n",
      "save model\n",
      "Save model 2773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2774 learning rate:1.0000e-05, Loss_tot:0.2123,\n",
      "save model\n",
      "Save model 2774\n",
      "Train Epoch:2775 learning rate:1.0000e-05, Loss_tot:0.2122,\n",
      "save model\n",
      "Save model 2775\n",
      "Train Epoch:2776 learning rate:1.0000e-05, Loss_tot:0.2121,\n",
      "save model\n",
      "Save model 2776\n",
      "Train Epoch:2777 learning rate:1.0000e-05, Loss_tot:0.2120,\n",
      "save model\n",
      "Save model 2777\n",
      "Train Epoch:2778 learning rate:1.0000e-05, Loss_tot:0.2120,\n",
      "save model\n",
      "Save model 2778\n",
      "Train Epoch:2779 learning rate:1.0000e-05, Loss_tot:0.2119,\n",
      "save model\n",
      "Save model 2779\n",
      "Train Epoch:2780 learning rate:1.0000e-05, Loss_tot:0.2119,\n",
      "save model\n",
      "Save model 2780\n",
      "Train Epoch:2781 learning rate:1.0000e-05, Loss_tot:0.2118,\n",
      "save model\n",
      "Save model 2781\n",
      "Train Epoch:2782 learning rate:1.0000e-05, Loss_tot:0.2118,\n",
      "Save model 2782\n",
      "Train Epoch:2783 learning rate:1.0000e-05, Loss_tot:0.2117,\n",
      "save model\n",
      "Save model 2783\n",
      "Train Epoch:2784 learning rate:1.0000e-05, Loss_tot:0.2111,\n",
      "save model\n",
      "Save model 2784\n",
      "Train Epoch:2785 learning rate:1.0000e-05, Loss_tot:0.2111,\n",
      "save model\n",
      "Save model 2785\n",
      "Train Epoch:2786 learning rate:1.0000e-05, Loss_tot:0.2115,\n",
      "Save model 2786\n",
      "Train Epoch:2787 learning rate:1.0000e-05, Loss_tot:0.2115,\n",
      "Save model 2787\n",
      "Train Epoch:2788 learning rate:1.0000e-05, Loss_tot:0.2109,\n",
      "save model\n",
      "Save model 2788\n",
      "Train Epoch:2789 learning rate:1.0000e-05, Loss_tot:0.2113,\n",
      "Save model 2789\n",
      "Train Epoch:2790 learning rate:1.0000e-05, Loss_tot:0.2113,\n",
      "Save model 2790\n",
      "Train Epoch:2791 learning rate:1.0000e-05, Loss_tot:0.2112,\n",
      "Save model 2791\n",
      "Train Epoch:2792 learning rate:1.0000e-05, Loss_tot:0.2112,\n",
      "Save model 2792\n",
      "Train Epoch:2793 learning rate:1.0000e-05, Loss_tot:0.2112,\n",
      "Save model 2793\n",
      "Train Epoch:2794 learning rate:1.0000e-05, Loss_tot:0.2106,\n",
      "save model\n",
      "Save model 2794\n",
      "Train Epoch:2795 learning rate:1.0000e-05, Loss_tot:0.2106,\n",
      "save model\n",
      "Save model 2795\n",
      "Train Epoch:2796 learning rate:1.0000e-05, Loss_tot:0.2110,\n",
      "Save model 2796\n",
      "Train Epoch:2797 learning rate:1.0000e-05, Loss_tot:0.2110,\n",
      "Save model 2797\n",
      "Train Epoch:2798 learning rate:1.0000e-05, Loss_tot:0.2109,\n",
      "Save model 2798\n",
      "Train Epoch:2799 learning rate:1.0000e-05, Loss_tot:0.2108,\n",
      "Save model 2799\n",
      "Train Epoch:2800 learning rate:1.0000e-05, Loss_tot:0.2108,\n",
      "Save model 2800\n",
      "Train Epoch:2801 learning rate:1.0000e-05, Loss_tot:0.2107,\n",
      "Save model 2801\n",
      "Train Epoch:2802 learning rate:1.0000e-05, Loss_tot:0.2106,\n",
      "Save model 2802\n",
      "Train Epoch:2803 learning rate:1.0000e-05, Loss_tot:0.2106,\n",
      "Save model 2803\n",
      "Train Epoch:2804 learning rate:1.0000e-05, Loss_tot:0.2106,\n",
      "save model\n",
      "Save model 2804\n",
      "Train Epoch:2805 learning rate:1.0000e-05, Loss_tot:0.2100,\n",
      "save model\n",
      "Save model 2805\n",
      "Train Epoch:2806 learning rate:1.0000e-05, Loss_tot:0.2099,\n",
      "save model\n",
      "Save model 2806\n",
      "Train Epoch:2807 learning rate:1.0000e-05, Loss_tot:0.2099,\n",
      "save model\n",
      "Save model 2807\n",
      "Train Epoch:2808 learning rate:1.0000e-05, Loss_tot:0.2098,\n",
      "save model\n",
      "Save model 2808\n",
      "Train Epoch:2809 learning rate:1.0000e-05, Loss_tot:0.2098,\n",
      "save model\n",
      "Save model 2809\n",
      "Train Epoch:2810 learning rate:1.0000e-05, Loss_tot:0.2097,\n",
      "save model\n",
      "Save model 2810\n",
      "Train Epoch:2811 learning rate:1.0000e-05, Loss_tot:0.2096,\n",
      "save model\n",
      "Save model 2811\n",
      "Train Epoch:2812 learning rate:1.0000e-05, Loss_tot:0.2096,\n",
      "save model\n",
      "Save model 2812\n",
      "Train Epoch:2813 learning rate:1.0000e-05, Loss_tot:0.2095,\n",
      "save model\n",
      "Save model 2813\n",
      "Train Epoch:2814 learning rate:1.0000e-05, Loss_tot:0.2095,\n",
      "save model\n",
      "Save model 2814\n",
      "Train Epoch:2815 learning rate:1.0000e-05, Loss_tot:0.2094,\n",
      "save model\n",
      "Save model 2815\n",
      "Train Epoch:2816 learning rate:1.0000e-05, Loss_tot:0.2094,\n",
      "save model\n",
      "Save model 2816\n",
      "Train Epoch:2817 learning rate:1.0000e-05, Loss_tot:0.2093,\n",
      "save model\n",
      "Save model 2817\n",
      "Train Epoch:2818 learning rate:1.0000e-05, Loss_tot:0.2093,\n",
      "save model\n",
      "Save model 2818\n",
      "Train Epoch:2819 learning rate:1.0000e-05, Loss_tot:0.2092,\n",
      "save model\n",
      "Save model 2819\n",
      "Train Epoch:2820 learning rate:1.0000e-05, Loss_tot:0.2091,\n",
      "save model\n",
      "Save model 2820\n",
      "Train Epoch:2821 learning rate:1.0000e-05, Loss_tot:0.2090,\n",
      "save model\n",
      "Save model 2821\n",
      "Train Epoch:2822 learning rate:1.0000e-05, Loss_tot:0.2090,\n",
      "save model\n",
      "Save model 2822\n",
      "Train Epoch:2823 learning rate:1.0000e-05, Loss_tot:0.2090,\n",
      "save model\n",
      "Save model 2823\n",
      "Train Epoch:2824 learning rate:1.0000e-05, Loss_tot:0.2089,\n",
      "save model\n",
      "Save model 2824\n",
      "Train Epoch:2825 learning rate:1.0000e-05, Loss_tot:0.2088,\n",
      "save model\n",
      "Save model 2825\n",
      "Train Epoch:2826 learning rate:1.0000e-05, Loss_tot:0.2088,\n",
      "save model\n",
      "Save model 2826\n",
      "Train Epoch:2827 learning rate:1.0000e-05, Loss_tot:0.2088,\n",
      "Save model 2827\n",
      "Train Epoch:2828 learning rate:1.0000e-05, Loss_tot:0.2087,\n",
      "save model\n",
      "Save model 2828\n",
      "Train Epoch:2829 learning rate:1.0000e-05, Loss_tot:0.2086,\n",
      "save model\n",
      "Save model 2829\n",
      "Train Epoch:2830 learning rate:1.0000e-05, Loss_tot:0.2086,\n",
      "save model\n",
      "Save model 2830\n",
      "Train Epoch:2831 learning rate:1.0000e-05, Loss_tot:0.2085,\n",
      "save model\n",
      "Save model 2831\n",
      "Train Epoch:2832 learning rate:1.0000e-05, Loss_tot:0.2084,\n",
      "save model\n",
      "Save model 2832\n",
      "Train Epoch:2833 learning rate:1.0000e-05, Loss_tot:0.2084,\n",
      "save model\n",
      "Save model 2833\n",
      "Train Epoch:2834 learning rate:1.0000e-05, Loss_tot:0.2083,\n",
      "save model\n",
      "Save model 2834\n",
      "Train Epoch:2835 learning rate:1.0000e-05, Loss_tot:0.2083,\n",
      "save model\n",
      "Save model 2835\n",
      "Train Epoch:2836 learning rate:1.0000e-05, Loss_tot:0.2082,\n",
      "save model\n",
      "Save model 2836\n",
      "Train Epoch:2837 learning rate:1.0000e-05, Loss_tot:0.2082,\n",
      "save model\n",
      "Save model 2837\n",
      "Train Epoch:2838 learning rate:1.0000e-05, Loss_tot:0.2081,\n",
      "save model\n",
      "Save model 2838\n",
      "Train Epoch:2839 learning rate:1.0000e-05, Loss_tot:0.2081,\n",
      "save model\n",
      "Save model 2839\n",
      "Train Epoch:2840 learning rate:1.0000e-05, Loss_tot:0.2080,\n",
      "save model\n",
      "Save model 2840\n",
      "Train Epoch:2841 learning rate:1.0000e-05, Loss_tot:0.2079,\n",
      "save model\n",
      "Save model 2841\n",
      "Train Epoch:2842 learning rate:1.0000e-05, Loss_tot:0.2079,\n",
      "save model\n",
      "Save model 2842\n",
      "Train Epoch:2843 learning rate:1.0000e-05, Loss_tot:0.2079,\n",
      "save model\n",
      "Save model 2843\n",
      "Train Epoch:2844 learning rate:1.0000e-05, Loss_tot:0.2078,\n",
      "save model\n",
      "Save model 2844\n",
      "Train Epoch:2845 learning rate:1.0000e-05, Loss_tot:0.2077,\n",
      "save model\n",
      "Save model 2845\n",
      "Train Epoch:2846 learning rate:1.0000e-05, Loss_tot:0.2077,\n",
      "Save model 2846\n",
      "Train Epoch:2847 learning rate:1.0000e-05, Loss_tot:0.2077,\n",
      "save model\n",
      "Save model 2847\n",
      "Train Epoch:2848 learning rate:1.0000e-05, Loss_tot:0.2075,\n",
      "save model\n",
      "Save model 2848\n",
      "Train Epoch:2849 learning rate:1.0000e-05, Loss_tot:0.2075,\n",
      "save model\n",
      "Save model 2849\n",
      "Train Epoch:2850 learning rate:1.0000e-05, Loss_tot:0.2075,\n",
      "save model\n",
      "Save model 2850\n",
      "Train Epoch:2851 learning rate:1.0000e-05, Loss_tot:0.2075,\n",
      "save model\n",
      "Save model 2851\n",
      "Train Epoch:2852 learning rate:1.0000e-05, Loss_tot:0.2074,\n",
      "save model\n",
      "Save model 2852\n",
      "Train Epoch:2853 learning rate:1.0000e-05, Loss_tot:0.2073,\n",
      "save model\n",
      "Save model 2853\n",
      "Train Epoch:2854 learning rate:1.0000e-05, Loss_tot:0.2072,\n",
      "save model\n",
      "Save model 2854\n",
      "Train Epoch:2855 learning rate:1.0000e-05, Loss_tot:0.2071,\n",
      "save model\n",
      "Save model 2855\n",
      "Train Epoch:2856 learning rate:1.0000e-05, Loss_tot:0.2071,\n",
      "save model\n",
      "Save model 2856\n",
      "Train Epoch:2857 learning rate:1.0000e-05, Loss_tot:0.2070,\n",
      "save model\n",
      "Save model 2857\n",
      "Train Epoch:2858 learning rate:1.0000e-05, Loss_tot:0.2070,\n",
      "save model\n",
      "Save model 2858\n",
      "Train Epoch:2859 learning rate:1.0000e-05, Loss_tot:0.2069,\n",
      "save model\n",
      "Save model 2859\n",
      "Train Epoch:2860 learning rate:1.0000e-05, Loss_tot:0.2069,\n",
      "save model\n",
      "Save model 2860\n",
      "Train Epoch:2861 learning rate:1.0000e-05, Loss_tot:0.2068,\n",
      "save model\n",
      "Save model 2861\n",
      "Train Epoch:2862 learning rate:1.0000e-05, Loss_tot:0.2068,\n",
      "save model\n",
      "Save model 2862\n",
      "Train Epoch:2863 learning rate:1.0000e-05, Loss_tot:0.2068,\n",
      "save model\n",
      "Save model 2863\n",
      "Train Epoch:2864 learning rate:1.0000e-05, Loss_tot:0.2067,\n",
      "save model\n",
      "Save model 2864\n",
      "Train Epoch:2865 learning rate:1.0000e-05, Loss_tot:0.2066,\n",
      "save model\n",
      "Save model 2865\n",
      "Train Epoch:2866 learning rate:1.0000e-05, Loss_tot:0.2065,\n",
      "save model\n",
      "Save model 2866\n",
      "Train Epoch:2867 learning rate:1.0000e-05, Loss_tot:0.2065,\n",
      "save model\n",
      "Save model 2867\n",
      "Train Epoch:2868 learning rate:1.0000e-05, Loss_tot:0.2065,\n",
      "save model\n",
      "Save model 2868\n",
      "Train Epoch:2869 learning rate:1.0000e-05, Loss_tot:0.2064,\n",
      "save model\n",
      "Save model 2869\n",
      "Train Epoch:2870 learning rate:1.0000e-05, Loss_tot:0.2064,\n",
      "Save model 2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2871 learning rate:1.0000e-05, Loss_tot:0.2064,\n",
      "save model\n",
      "Save model 2871\n",
      "Train Epoch:2872 learning rate:1.0000e-05, Loss_tot:0.2063,\n",
      "save model\n",
      "Save model 2872\n",
      "Train Epoch:2873 learning rate:1.0000e-05, Loss_tot:0.2063,\n",
      "save model\n",
      "Save model 2873\n",
      "Train Epoch:2874 learning rate:1.0000e-05, Loss_tot:0.2062,\n",
      "save model\n",
      "Save model 2874\n",
      "Train Epoch:2875 learning rate:1.0000e-05, Loss_tot:0.2061,\n",
      "save model\n",
      "Save model 2875\n",
      "Train Epoch:2876 learning rate:1.0000e-05, Loss_tot:0.2060,\n",
      "save model\n",
      "Save model 2876\n",
      "Train Epoch:2877 learning rate:1.0000e-05, Loss_tot:0.2061,\n",
      "Save model 2877\n",
      "Train Epoch:2878 learning rate:1.0000e-05, Loss_tot:0.2061,\n",
      "Save model 2878\n",
      "Train Epoch:2879 learning rate:1.0000e-05, Loss_tot:0.2064,\n",
      "Save model 2879\n",
      "Train Epoch:2880 learning rate:1.0000e-05, Loss_tot:0.2062,\n",
      "Save model 2880\n",
      "Train Epoch:2881 learning rate:1.0000e-05, Loss_tot:0.2062,\n",
      "Save model 2881\n",
      "Train Epoch:2882 learning rate:1.0000e-05, Loss_tot:0.2062,\n",
      "Save model 2882\n",
      "Train Epoch:2883 learning rate:1.0000e-05, Loss_tot:0.2062,\n",
      "Save model 2883\n",
      "Train Epoch:2884 learning rate:1.0000e-05, Loss_tot:0.2061,\n",
      "Save model 2884\n",
      "Train Epoch:2885 learning rate:1.0000e-05, Loss_tot:0.2061,\n",
      "Save model 2885\n",
      "Train Epoch:2886 learning rate:1.0000e-05, Loss_tot:0.2060,\n",
      "save model\n",
      "Save model 2886\n",
      "Train Epoch:2887 learning rate:1.0000e-05, Loss_tot:0.2058,\n",
      "save model\n",
      "Save model 2887\n",
      "Train Epoch:2888 learning rate:1.0000e-05, Loss_tot:0.2057,\n",
      "save model\n",
      "Save model 2888\n",
      "Train Epoch:2889 learning rate:1.0000e-05, Loss_tot:0.2057,\n",
      "save model\n",
      "Save model 2889\n",
      "Train Epoch:2890 learning rate:1.0000e-05, Loss_tot:0.2057,\n",
      "save model\n",
      "Save model 2890\n",
      "Train Epoch:2891 learning rate:1.0000e-05, Loss_tot:0.2056,\n",
      "save model\n",
      "Save model 2891\n",
      "Train Epoch:2892 learning rate:1.0000e-05, Loss_tot:0.2054,\n",
      "save model\n",
      "Save model 2892\n",
      "Train Epoch:2893 learning rate:1.0000e-05, Loss_tot:0.2054,\n",
      "save model\n",
      "Save model 2893\n",
      "Train Epoch:2894 learning rate:1.0000e-05, Loss_tot:0.2054,\n",
      "save model\n",
      "Save model 2894\n",
      "Train Epoch:2895 learning rate:1.0000e-05, Loss_tot:0.2053,\n",
      "save model\n",
      "Save model 2895\n",
      "Train Epoch:2896 learning rate:1.0000e-05, Loss_tot:0.2052,\n",
      "save model\n",
      "Save model 2896\n",
      "Train Epoch:2897 learning rate:1.0000e-05, Loss_tot:0.2052,\n",
      "save model\n",
      "Save model 2897\n",
      "Train Epoch:2898 learning rate:1.0000e-05, Loss_tot:0.2051,\n",
      "save model\n",
      "Save model 2898\n",
      "Train Epoch:2899 learning rate:1.0000e-05, Loss_tot:0.2051,\n",
      "save model\n",
      "Save model 2899\n",
      "Train Epoch:2900 learning rate:1.0000e-05, Loss_tot:0.2050,\n",
      "save model\n",
      "Save model 2900\n",
      "Train Epoch:2901 learning rate:1.0000e-05, Loss_tot:0.2049,\n",
      "save model\n",
      "Save model 2901\n",
      "Train Epoch:2902 learning rate:1.0000e-05, Loss_tot:0.2049,\n",
      "save model\n",
      "Save model 2902\n",
      "Train Epoch:2903 learning rate:1.0000e-05, Loss_tot:0.2048,\n",
      "save model\n",
      "Save model 2903\n",
      "Train Epoch:2904 learning rate:1.0000e-05, Loss_tot:0.2048,\n",
      "save model\n",
      "Save model 2904\n",
      "Train Epoch:2905 learning rate:1.0000e-05, Loss_tot:0.2048,\n",
      "save model\n",
      "Save model 2905\n",
      "Train Epoch:2906 learning rate:1.0000e-05, Loss_tot:0.2047,\n",
      "save model\n",
      "Save model 2906\n",
      "Train Epoch:2907 learning rate:1.0000e-05, Loss_tot:0.2047,\n",
      "save model\n",
      "Save model 2907\n",
      "Train Epoch:2908 learning rate:1.0000e-05, Loss_tot:0.2046,\n",
      "save model\n",
      "Save model 2908\n",
      "Train Epoch:2909 learning rate:1.0000e-05, Loss_tot:0.2046,\n",
      "Save model 2909\n",
      "Train Epoch:2910 learning rate:1.0000e-05, Loss_tot:0.2046,\n",
      "save model\n",
      "Save model 2910\n",
      "Train Epoch:2911 learning rate:1.0000e-05, Loss_tot:0.2044,\n",
      "save model\n",
      "Save model 2911\n",
      "Train Epoch:2912 learning rate:1.0000e-05, Loss_tot:0.2044,\n",
      "save model\n",
      "Save model 2912\n",
      "Train Epoch:2913 learning rate:1.0000e-05, Loss_tot:0.2044,\n",
      "save model\n",
      "Save model 2913\n",
      "Train Epoch:2914 learning rate:1.0000e-05, Loss_tot:0.2044,\n",
      "save model\n",
      "Save model 2914\n",
      "Train Epoch:2915 learning rate:1.0000e-05, Loss_tot:0.2043,\n",
      "save model\n",
      "Save model 2915\n",
      "Train Epoch:2916 learning rate:1.0000e-05, Loss_tot:0.2042,\n",
      "save model\n",
      "Save model 2916\n",
      "Train Epoch:2917 learning rate:1.0000e-05, Loss_tot:0.2041,\n",
      "save model\n",
      "Save model 2917\n",
      "Train Epoch:2918 learning rate:1.0000e-05, Loss_tot:0.2041,\n",
      "save model\n",
      "Save model 2918\n",
      "Train Epoch:2919 learning rate:1.0000e-05, Loss_tot:0.2041,\n",
      "save model\n",
      "Save model 2919\n",
      "Train Epoch:2920 learning rate:1.0000e-05, Loss_tot:0.2039,\n",
      "save model\n",
      "Save model 2920\n",
      "Train Epoch:2921 learning rate:1.0000e-05, Loss_tot:0.2039,\n",
      "save model\n",
      "Save model 2921\n",
      "Train Epoch:2922 learning rate:1.0000e-05, Loss_tot:0.2039,\n",
      "save model\n",
      "Save model 2922\n",
      "Train Epoch:2923 learning rate:1.0000e-05, Loss_tot:0.2039,\n",
      "save model\n",
      "Save model 2923\n",
      "Train Epoch:2924 learning rate:1.0000e-05, Loss_tot:0.2038,\n",
      "save model\n",
      "Save model 2924\n",
      "Train Epoch:2925 learning rate:1.0000e-05, Loss_tot:0.2038,\n",
      "save model\n",
      "Save model 2925\n",
      "Train Epoch:2926 learning rate:1.0000e-05, Loss_tot:0.2037,\n",
      "save model\n",
      "Save model 2926\n",
      "Train Epoch:2927 learning rate:1.0000e-05, Loss_tot:0.2036,\n",
      "save model\n",
      "Save model 2927\n",
      "Train Epoch:2928 learning rate:1.0000e-05, Loss_tot:0.2036,\n",
      "save model\n",
      "Save model 2928\n",
      "Train Epoch:2929 learning rate:1.0000e-05, Loss_tot:0.2035,\n",
      "save model\n",
      "Save model 2929\n",
      "Train Epoch:2930 learning rate:1.0000e-05, Loss_tot:0.2035,\n",
      "save model\n",
      "Save model 2930\n",
      "Train Epoch:2931 learning rate:1.0000e-05, Loss_tot:0.2035,\n",
      "save model\n",
      "Save model 2931\n",
      "Train Epoch:2932 learning rate:1.0000e-05, Loss_tot:0.2034,\n",
      "save model\n",
      "Save model 2932\n",
      "Train Epoch:2933 learning rate:1.0000e-05, Loss_tot:0.2034,\n",
      "save model\n",
      "Save model 2933\n",
      "Train Epoch:2934 learning rate:1.0000e-05, Loss_tot:0.2033,\n",
      "save model\n",
      "Save model 2934\n",
      "Train Epoch:2935 learning rate:1.0000e-05, Loss_tot:0.2032,\n",
      "save model\n",
      "Save model 2935\n",
      "Train Epoch:2936 learning rate:1.0000e-05, Loss_tot:0.2032,\n",
      "save model\n",
      "Save model 2936\n",
      "Train Epoch:2937 learning rate:1.0000e-05, Loss_tot:0.2032,\n",
      "save model\n",
      "Save model 2937\n",
      "Train Epoch:2938 learning rate:1.0000e-05, Loss_tot:0.2030,\n",
      "save model\n",
      "Save model 2938\n",
      "Train Epoch:2939 learning rate:1.0000e-05, Loss_tot:0.2030,\n",
      "save model\n",
      "Save model 2939\n",
      "Train Epoch:2940 learning rate:1.0000e-05, Loss_tot:0.2030,\n",
      "save model\n",
      "Save model 2940\n",
      "Train Epoch:2941 learning rate:1.0000e-05, Loss_tot:0.2029,\n",
      "save model\n",
      "Save model 2941\n",
      "Train Epoch:2942 learning rate:1.0000e-05, Loss_tot:0.2029,\n",
      "save model\n",
      "Save model 2942\n",
      "Train Epoch:2943 learning rate:1.0000e-05, Loss_tot:0.2028,\n",
      "save model\n",
      "Save model 2943\n",
      "Train Epoch:2944 learning rate:1.0000e-05, Loss_tot:0.2031,\n",
      "Save model 2944\n",
      "Train Epoch:2945 learning rate:1.0000e-05, Loss_tot:0.2027,\n",
      "save model\n",
      "Save model 2945\n",
      "Train Epoch:2946 learning rate:1.0000e-05, Loss_tot:0.2027,\n",
      "save model\n",
      "Save model 2946\n",
      "Train Epoch:2947 learning rate:1.0000e-05, Loss_tot:0.2026,\n",
      "save model\n",
      "Save model 2947\n",
      "Train Epoch:2948 learning rate:1.0000e-05, Loss_tot:0.2025,\n",
      "save model\n",
      "Save model 2948\n",
      "Train Epoch:2949 learning rate:1.0000e-05, Loss_tot:0.2025,\n",
      "save model\n",
      "Save model 2949\n",
      "Train Epoch:2950 learning rate:1.0000e-05, Loss_tot:0.2025,\n",
      "save model\n",
      "Save model 2950\n",
      "Train Epoch:2951 learning rate:1.0000e-05, Loss_tot:0.2024,\n",
      "save model\n",
      "Save model 2951\n",
      "Train Epoch:2952 learning rate:1.0000e-05, Loss_tot:0.2023,\n",
      "save model\n",
      "Save model 2952\n",
      "Train Epoch:2953 learning rate:1.0000e-05, Loss_tot:0.2022,\n",
      "save model\n",
      "Save model 2953\n",
      "Train Epoch:2954 learning rate:1.0000e-05, Loss_tot:0.2022,\n",
      "save model\n",
      "Save model 2954\n",
      "Train Epoch:2955 learning rate:1.0000e-05, Loss_tot:0.2022,\n",
      "save model\n",
      "Save model 2955\n",
      "Train Epoch:2956 learning rate:1.0000e-05, Loss_tot:0.2021,\n",
      "save model\n",
      "Save model 2956\n",
      "Train Epoch:2957 learning rate:1.0000e-05, Loss_tot:0.2021,\n",
      "save model\n",
      "Save model 2957\n",
      "Train Epoch:2958 learning rate:1.0000e-05, Loss_tot:0.2020,\n",
      "save model\n",
      "Save model 2958\n",
      "Train Epoch:2959 learning rate:1.0000e-05, Loss_tot:0.2020,\n",
      "save model\n",
      "Save model 2959\n",
      "Train Epoch:2960 learning rate:1.0000e-05, Loss_tot:0.2019,\n",
      "save model\n",
      "Save model 2960\n",
      "Train Epoch:2961 learning rate:1.0000e-05, Loss_tot:0.2018,\n",
      "save model\n",
      "Save model 2961\n",
      "Train Epoch:2962 learning rate:1.0000e-05, Loss_tot:0.2018,\n",
      "save model\n",
      "Save model 2962\n",
      "Train Epoch:2963 learning rate:1.0000e-05, Loss_tot:0.2017,\n",
      "save model\n",
      "Save model 2963\n",
      "Train Epoch:2964 learning rate:1.0000e-05, Loss_tot:0.2017,\n",
      "save model\n",
      "Save model 2964\n",
      "Train Epoch:2965 learning rate:1.0000e-05, Loss_tot:0.2016,\n",
      "save model\n",
      "Save model 2965\n",
      "Train Epoch:2966 learning rate:1.0000e-05, Loss_tot:0.2016,\n",
      "save model\n",
      "Save model 2966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:2967 learning rate:1.0000e-05, Loss_tot:0.2016,\n",
      "save model\n",
      "Save model 2967\n",
      "Train Epoch:2968 learning rate:1.0000e-05, Loss_tot:0.2015,\n",
      "save model\n",
      "Save model 2968\n",
      "Train Epoch:2969 learning rate:1.0000e-05, Loss_tot:0.2015,\n",
      "save model\n",
      "Save model 2969\n",
      "Train Epoch:2970 learning rate:1.0000e-05, Loss_tot:0.2014,\n",
      "save model\n",
      "Save model 2970\n",
      "Train Epoch:2971 learning rate:1.0000e-05, Loss_tot:0.2014,\n",
      "Save model 2971\n",
      "Train Epoch:2972 learning rate:1.0000e-05, Loss_tot:0.2014,\n",
      "save model\n",
      "Save model 2972\n",
      "Train Epoch:2973 learning rate:1.0000e-05, Loss_tot:0.2013,\n",
      "save model\n",
      "Save model 2973\n",
      "Train Epoch:2974 learning rate:1.0000e-05, Loss_tot:0.2013,\n",
      "save model\n",
      "Save model 2974\n",
      "Train Epoch:2975 learning rate:1.0000e-05, Loss_tot:0.2013,\n",
      "save model\n",
      "Save model 2975\n",
      "Train Epoch:2976 learning rate:1.0000e-05, Loss_tot:0.2012,\n",
      "save model\n",
      "Save model 2976\n",
      "Train Epoch:2977 learning rate:1.0000e-05, Loss_tot:0.2012,\n",
      "save model\n",
      "Save model 2977\n",
      "Train Epoch:2978 learning rate:1.0000e-05, Loss_tot:0.2011,\n",
      "save model\n",
      "Save model 2978\n",
      "Train Epoch:2979 learning rate:1.0000e-05, Loss_tot:0.2010,\n",
      "save model\n",
      "Save model 2979\n",
      "Train Epoch:2980 learning rate:1.0000e-05, Loss_tot:0.2009,\n",
      "save model\n",
      "Save model 2980\n",
      "Train Epoch:2981 learning rate:1.0000e-05, Loss_tot:0.2014,\n",
      "Save model 2981\n",
      "Train Epoch:2982 learning rate:1.0000e-05, Loss_tot:0.2014,\n",
      "Save model 2982\n",
      "Train Epoch:2983 learning rate:1.0000e-05, Loss_tot:0.2009,\n",
      "save model\n",
      "Save model 2983\n",
      "Train Epoch:2984 learning rate:1.0000e-05, Loss_tot:0.2007,\n",
      "save model\n",
      "Save model 2984\n",
      "Train Epoch:2985 learning rate:1.0000e-05, Loss_tot:0.2007,\n",
      "Save model 2985\n",
      "Train Epoch:2986 learning rate:1.0000e-05, Loss_tot:0.2007,\n",
      "Save model 2986\n",
      "Train Epoch:2987 learning rate:1.0000e-05, Loss_tot:0.2007,\n",
      "Save model 2987\n",
      "Train Epoch:2988 learning rate:1.0000e-05, Loss_tot:0.2007,\n",
      "save model\n",
      "Save model 2988\n",
      "Train Epoch:2989 learning rate:1.0000e-05, Loss_tot:0.2006,\n",
      "save model\n",
      "Save model 2989\n",
      "Train Epoch:2990 learning rate:1.0000e-05, Loss_tot:0.2006,\n",
      "save model\n",
      "Save model 2990\n",
      "Train Epoch:2991 learning rate:1.0000e-05, Loss_tot:0.2005,\n",
      "save model\n",
      "Save model 2991\n",
      "Train Epoch:2992 learning rate:1.0000e-05, Loss_tot:0.2003,\n",
      "save model\n",
      "Save model 2992\n",
      "Train Epoch:2993 learning rate:1.0000e-05, Loss_tot:0.2003,\n",
      "save model\n",
      "Save model 2993\n",
      "Train Epoch:2994 learning rate:1.0000e-05, Loss_tot:0.2003,\n",
      "Save model 2994\n",
      "Train Epoch:2995 learning rate:1.0000e-05, Loss_tot:0.2002,\n",
      "save model\n",
      "Save model 2995\n",
      "Train Epoch:2996 learning rate:1.0000e-05, Loss_tot:0.2001,\n",
      "save model\n",
      "Save model 2996\n",
      "Train Epoch:2997 learning rate:1.0000e-05, Loss_tot:0.2001,\n",
      "save model\n",
      "Save model 2997\n",
      "Train Epoch:2998 learning rate:1.0000e-05, Loss_tot:0.2000,\n",
      "save model\n",
      "Save model 2998\n",
      "Train Epoch:2999 learning rate:1.0000e-05, Loss_tot:0.2000,\n",
      "save model\n",
      "Save model 2999\n",
      "Train Epoch:3000 learning rate:1.0000e-05, Loss_tot:0.1999,\n",
      "save model\n",
      "Save model 3000\n",
      "Train Epoch:3001 learning rate:1.0000e-05, Loss_tot:0.1999,\n",
      "save model\n",
      "Save model 3001\n",
      "Train Epoch:3002 learning rate:1.0000e-05, Loss_tot:0.1998,\n",
      "save model\n",
      "Save model 3002\n",
      "Train Epoch:3003 learning rate:1.0000e-05, Loss_tot:0.1998,\n",
      "save model\n",
      "Save model 3003\n",
      "Train Epoch:3004 learning rate:1.0000e-05, Loss_tot:0.1997,\n",
      "save model\n",
      "Save model 3004\n",
      "Train Epoch:3005 learning rate:1.0000e-05, Loss_tot:0.1997,\n",
      "save model\n",
      "Save model 3005\n",
      "Train Epoch:3006 learning rate:1.0000e-05, Loss_tot:0.1996,\n",
      "save model\n",
      "Save model 3006\n",
      "Train Epoch:3007 learning rate:1.0000e-05, Loss_tot:0.1995,\n",
      "save model\n",
      "Save model 3007\n",
      "Train Epoch:3008 learning rate:1.0000e-05, Loss_tot:0.1995,\n",
      "save model\n",
      "Save model 3008\n",
      "Train Epoch:3009 learning rate:1.0000e-05, Loss_tot:0.1994,\n",
      "save model\n",
      "Save model 3009\n",
      "Train Epoch:3010 learning rate:1.0000e-05, Loss_tot:0.1994,\n",
      "save model\n",
      "Save model 3010\n",
      "Train Epoch:3011 learning rate:1.0000e-05, Loss_tot:0.1993,\n",
      "save model\n",
      "Save model 3011\n",
      "Train Epoch:3012 learning rate:1.0000e-05, Loss_tot:0.1993,\n",
      "save model\n",
      "Save model 3012\n",
      "Train Epoch:3013 learning rate:1.0000e-05, Loss_tot:0.1992,\n",
      "save model\n",
      "Save model 3013\n",
      "Train Epoch:3014 learning rate:1.0000e-05, Loss_tot:0.1992,\n",
      "save model\n",
      "Save model 3014\n",
      "Train Epoch:3015 learning rate:1.0000e-05, Loss_tot:0.1991,\n",
      "save model\n",
      "Save model 3015\n",
      "Train Epoch:3016 learning rate:1.0000e-05, Loss_tot:0.1991,\n",
      "save model\n",
      "Save model 3016\n",
      "Train Epoch:3017 learning rate:1.0000e-05, Loss_tot:0.1990,\n",
      "save model\n",
      "Save model 3017\n",
      "Train Epoch:3018 learning rate:1.0000e-05, Loss_tot:0.1990,\n",
      "save model\n",
      "Save model 3018\n",
      "Train Epoch:3019 learning rate:1.0000e-05, Loss_tot:0.1989,\n",
      "save model\n",
      "Save model 3019\n",
      "Train Epoch:3020 learning rate:1.0000e-05, Loss_tot:0.1989,\n",
      "save model\n",
      "Save model 3020\n",
      "Train Epoch:3021 learning rate:1.0000e-05, Loss_tot:0.1988,\n",
      "save model\n",
      "Save model 3021\n",
      "Train Epoch:3022 learning rate:1.0000e-05, Loss_tot:0.1988,\n",
      "save model\n",
      "Save model 3022\n",
      "Train Epoch:3023 learning rate:1.0000e-05, Loss_tot:0.1987,\n",
      "save model\n",
      "Save model 3023\n",
      "Train Epoch:3024 learning rate:1.0000e-05, Loss_tot:0.1987,\n",
      "save model\n",
      "Save model 3024\n",
      "Train Epoch:3025 learning rate:1.0000e-05, Loss_tot:0.1986,\n",
      "save model\n",
      "Save model 3025\n",
      "Train Epoch:3026 learning rate:1.0000e-05, Loss_tot:0.1986,\n",
      "save model\n",
      "Save model 3026\n",
      "Train Epoch:3027 learning rate:1.0000e-05, Loss_tot:0.1985,\n",
      "save model\n",
      "Save model 3027\n",
      "Train Epoch:3028 learning rate:1.0000e-05, Loss_tot:0.1985,\n",
      "save model\n",
      "Save model 3028\n",
      "Train Epoch:3029 learning rate:1.0000e-05, Loss_tot:0.1985,\n",
      "save model\n",
      "Save model 3029\n",
      "Train Epoch:3030 learning rate:1.0000e-05, Loss_tot:0.1984,\n",
      "save model\n",
      "Save model 3030\n",
      "Train Epoch:3031 learning rate:1.0000e-05, Loss_tot:0.1984,\n",
      "save model\n",
      "Save model 3031\n",
      "Train Epoch:3032 learning rate:1.0000e-05, Loss_tot:0.1984,\n",
      "save model\n",
      "Save model 3032\n",
      "Train Epoch:3033 learning rate:1.0000e-05, Loss_tot:0.1983,\n",
      "save model\n",
      "Save model 3033\n",
      "Train Epoch:3034 learning rate:1.0000e-05, Loss_tot:0.1983,\n",
      "save model\n",
      "Save model 3034\n",
      "Train Epoch:3035 learning rate:1.0000e-05, Loss_tot:0.1982,\n",
      "save model\n",
      "Save model 3035\n",
      "Train Epoch:3036 learning rate:1.0000e-05, Loss_tot:0.1981,\n",
      "save model\n",
      "Save model 3036\n",
      "Train Epoch:3037 learning rate:1.0000e-05, Loss_tot:0.1981,\n",
      "save model\n",
      "Save model 3037\n",
      "Train Epoch:3038 learning rate:1.0000e-05, Loss_tot:0.1980,\n",
      "save model\n",
      "Save model 3038\n",
      "Train Epoch:3039 learning rate:1.0000e-05, Loss_tot:0.1980,\n",
      "save model\n",
      "Save model 3039\n",
      "Train Epoch:3040 learning rate:1.0000e-05, Loss_tot:0.1979,\n",
      "save model\n",
      "Save model 3040\n",
      "Train Epoch:3041 learning rate:1.0000e-05, Loss_tot:0.1979,\n",
      "save model\n",
      "Save model 3041\n",
      "Train Epoch:3042 learning rate:1.0000e-05, Loss_tot:0.1978,\n",
      "save model\n",
      "Save model 3042\n",
      "Train Epoch:3043 learning rate:1.0000e-05, Loss_tot:0.1978,\n",
      "save model\n",
      "Save model 3043\n",
      "Train Epoch:3044 learning rate:1.0000e-05, Loss_tot:0.1978,\n",
      "save model\n",
      "Save model 3044\n",
      "Train Epoch:3045 learning rate:1.0000e-05, Loss_tot:0.1977,\n",
      "save model\n",
      "Save model 3045\n",
      "Train Epoch:3046 learning rate:1.0000e-05, Loss_tot:0.1976,\n",
      "save model\n",
      "Save model 3046\n",
      "Train Epoch:3047 learning rate:1.0000e-05, Loss_tot:0.1975,\n",
      "save model\n",
      "Save model 3047\n",
      "Train Epoch:3048 learning rate:1.0000e-05, Loss_tot:0.1976,\n",
      "Save model 3048\n",
      "Train Epoch:3049 learning rate:1.0000e-05, Loss_tot:0.1975,\n",
      "save model\n",
      "Save model 3049\n",
      "Train Epoch:3050 learning rate:1.0000e-05, Loss_tot:0.1974,\n",
      "save model\n",
      "Save model 3050\n",
      "Train Epoch:3051 learning rate:1.0000e-05, Loss_tot:0.1974,\n",
      "save model\n",
      "Save model 3051\n",
      "Train Epoch:3052 learning rate:1.0000e-05, Loss_tot:0.1974,\n",
      "save model\n",
      "Save model 3052\n",
      "Train Epoch:3053 learning rate:1.0000e-05, Loss_tot:0.1974,\n",
      "save model\n",
      "Save model 3053\n",
      "Train Epoch:3054 learning rate:1.0000e-05, Loss_tot:0.1973,\n",
      "save model\n",
      "Save model 3054\n",
      "Train Epoch:3055 learning rate:1.0000e-05, Loss_tot:0.1972,\n",
      "save model\n",
      "Save model 3055\n",
      "Train Epoch:3056 learning rate:1.0000e-05, Loss_tot:0.1971,\n",
      "save model\n",
      "Save model 3056\n",
      "Train Epoch:3057 learning rate:1.0000e-05, Loss_tot:0.1971,\n",
      "save model\n",
      "Save model 3057\n",
      "Train Epoch:3058 learning rate:1.0000e-05, Loss_tot:0.1971,\n",
      "save model\n",
      "Save model 3058\n",
      "Train Epoch:3059 learning rate:1.0000e-05, Loss_tot:0.1970,\n",
      "save model\n",
      "Save model 3059\n",
      "Train Epoch:3060 learning rate:1.0000e-05, Loss_tot:0.1969,\n",
      "save model\n",
      "Save model 3060\n",
      "Train Epoch:3061 learning rate:1.0000e-05, Loss_tot:0.1969,\n",
      "save model\n",
      "Save model 3061\n",
      "Train Epoch:3062 learning rate:1.0000e-05, Loss_tot:0.1969,\n",
      "save model\n",
      "Save model 3062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3063 learning rate:1.0000e-05, Loss_tot:0.1968,\n",
      "save model\n",
      "Save model 3063\n",
      "Train Epoch:3064 learning rate:1.0000e-05, Loss_tot:0.1968,\n",
      "save model\n",
      "Save model 3064\n",
      "Train Epoch:3065 learning rate:1.0000e-05, Loss_tot:0.1967,\n",
      "save model\n",
      "Save model 3065\n",
      "Train Epoch:3066 learning rate:1.0000e-05, Loss_tot:0.1967,\n",
      "Save model 3066\n",
      "Train Epoch:3067 learning rate:1.0000e-05, Loss_tot:0.1967,\n",
      "Save model 3067\n",
      "Train Epoch:3068 learning rate:1.0000e-05, Loss_tot:0.1966,\n",
      "save model\n",
      "Save model 3068\n",
      "Train Epoch:3069 learning rate:1.0000e-05, Loss_tot:0.1965,\n",
      "save model\n",
      "Save model 3069\n",
      "Train Epoch:3070 learning rate:1.0000e-05, Loss_tot:0.1965,\n",
      "save model\n",
      "Save model 3070\n",
      "Train Epoch:3071 learning rate:1.0000e-05, Loss_tot:0.1964,\n",
      "save model\n",
      "Save model 3071\n",
      "Train Epoch:3072 learning rate:1.0000e-05, Loss_tot:0.1964,\n",
      "save model\n",
      "Save model 3072\n",
      "Train Epoch:3073 learning rate:1.0000e-05, Loss_tot:0.1963,\n",
      "save model\n",
      "Save model 3073\n",
      "Train Epoch:3074 learning rate:1.0000e-05, Loss_tot:0.1963,\n",
      "save model\n",
      "Save model 3074\n",
      "Train Epoch:3075 learning rate:1.0000e-05, Loss_tot:0.1962,\n",
      "save model\n",
      "Save model 3075\n",
      "Train Epoch:3076 learning rate:1.0000e-05, Loss_tot:0.1961,\n",
      "save model\n",
      "Save model 3076\n",
      "Train Epoch:3077 learning rate:1.0000e-05, Loss_tot:0.1961,\n",
      "save model\n",
      "Save model 3077\n",
      "Train Epoch:3078 learning rate:1.0000e-05, Loss_tot:0.1960,\n",
      "save model\n",
      "Save model 3078\n",
      "Train Epoch:3079 learning rate:1.0000e-05, Loss_tot:0.1960,\n",
      "save model\n",
      "Save model 3079\n",
      "Train Epoch:3080 learning rate:1.0000e-05, Loss_tot:0.1959,\n",
      "save model\n",
      "Save model 3080\n",
      "Train Epoch:3081 learning rate:1.0000e-05, Loss_tot:0.1959,\n",
      "Save model 3081\n",
      "Train Epoch:3082 learning rate:1.0000e-05, Loss_tot:0.1959,\n",
      "save model\n",
      "Save model 3082\n",
      "Train Epoch:3083 learning rate:1.0000e-05, Loss_tot:0.1959,\n",
      "save model\n",
      "Save model 3083\n",
      "Train Epoch:3084 learning rate:1.0000e-05, Loss_tot:0.1958,\n",
      "save model\n",
      "Save model 3084\n",
      "Train Epoch:3085 learning rate:1.0000e-05, Loss_tot:0.1957,\n",
      "save model\n",
      "Save model 3085\n",
      "Train Epoch:3086 learning rate:1.0000e-05, Loss_tot:0.1957,\n",
      "save model\n",
      "Save model 3086\n",
      "Train Epoch:3087 learning rate:1.0000e-05, Loss_tot:0.1957,\n",
      "save model\n",
      "Save model 3087\n",
      "Train Epoch:3088 learning rate:1.0000e-05, Loss_tot:0.1956,\n",
      "save model\n",
      "Save model 3088\n",
      "Train Epoch:3089 learning rate:1.0000e-05, Loss_tot:0.1956,\n",
      "Save model 3089\n",
      "Train Epoch:3090 learning rate:1.0000e-05, Loss_tot:0.1956,\n",
      "Save model 3090\n",
      "Train Epoch:3091 learning rate:1.0000e-05, Loss_tot:0.1956,\n",
      "save model\n",
      "Save model 3091\n",
      "Train Epoch:3092 learning rate:1.0000e-05, Loss_tot:0.1955,\n",
      "save model\n",
      "Save model 3092\n",
      "Train Epoch:3093 learning rate:1.0000e-05, Loss_tot:0.1954,\n",
      "save model\n",
      "Save model 3093\n",
      "Train Epoch:3094 learning rate:1.0000e-05, Loss_tot:0.1953,\n",
      "save model\n",
      "Save model 3094\n",
      "Train Epoch:3095 learning rate:1.0000e-05, Loss_tot:0.1953,\n",
      "save model\n",
      "Save model 3095\n",
      "Train Epoch:3096 learning rate:1.0000e-05, Loss_tot:0.1952,\n",
      "save model\n",
      "Save model 3096\n",
      "Train Epoch:3097 learning rate:1.0000e-05, Loss_tot:0.1951,\n",
      "save model\n",
      "Save model 3097\n",
      "Train Epoch:3098 learning rate:1.0000e-05, Loss_tot:0.1951,\n",
      "save model\n",
      "Save model 3098\n",
      "Train Epoch:3099 learning rate:1.0000e-05, Loss_tot:0.1951,\n",
      "save model\n",
      "Save model 3099\n",
      "Train Epoch:3100 learning rate:1.0000e-05, Loss_tot:0.1950,\n",
      "save model\n",
      "Save model 3100\n",
      "Train Epoch:3101 learning rate:1.0000e-05, Loss_tot:0.1950,\n",
      "save model\n",
      "Save model 3101\n",
      "Train Epoch:3102 learning rate:1.0000e-05, Loss_tot:0.1949,\n",
      "save model\n",
      "Save model 3102\n",
      "Train Epoch:3103 learning rate:1.0000e-05, Loss_tot:0.1949,\n",
      "save model\n",
      "Save model 3103\n",
      "Train Epoch:3104 learning rate:1.0000e-05, Loss_tot:0.1948,\n",
      "save model\n",
      "Save model 3104\n",
      "Train Epoch:3105 learning rate:1.0000e-05, Loss_tot:0.1948,\n",
      "save model\n",
      "Save model 3105\n",
      "Train Epoch:3106 learning rate:1.0000e-05, Loss_tot:0.1947,\n",
      "save model\n",
      "Save model 3106\n",
      "Train Epoch:3107 learning rate:1.0000e-05, Loss_tot:0.1947,\n",
      "save model\n",
      "Save model 3107\n",
      "Train Epoch:3108 learning rate:1.0000e-05, Loss_tot:0.1946,\n",
      "save model\n",
      "Save model 3108\n",
      "Train Epoch:3109 learning rate:1.0000e-05, Loss_tot:0.1946,\n",
      "save model\n",
      "Save model 3109\n",
      "Train Epoch:3110 learning rate:1.0000e-05, Loss_tot:0.1945,\n",
      "save model\n",
      "Save model 3110\n",
      "Train Epoch:3111 learning rate:1.0000e-05, Loss_tot:0.1945,\n",
      "save model\n",
      "Save model 3111\n",
      "Train Epoch:3112 learning rate:1.0000e-05, Loss_tot:0.1944,\n",
      "save model\n",
      "Save model 3112\n",
      "Train Epoch:3113 learning rate:1.0000e-05, Loss_tot:0.1944,\n",
      "save model\n",
      "Save model 3113\n",
      "Train Epoch:3114 learning rate:1.0000e-05, Loss_tot:0.1944,\n",
      "Save model 3114\n",
      "Train Epoch:3115 learning rate:1.0000e-05, Loss_tot:0.1943,\n",
      "save model\n",
      "Save model 3115\n",
      "Train Epoch:3116 learning rate:1.0000e-05, Loss_tot:0.1943,\n",
      "save model\n",
      "Save model 3116\n",
      "Train Epoch:3117 learning rate:1.0000e-05, Loss_tot:0.1943,\n",
      "save model\n",
      "Save model 3117\n",
      "Train Epoch:3118 learning rate:1.0000e-05, Loss_tot:0.1943,\n",
      "save model\n",
      "Save model 3118\n",
      "Train Epoch:3119 learning rate:1.0000e-05, Loss_tot:0.1942,\n",
      "save model\n",
      "Save model 3119\n",
      "Train Epoch:3120 learning rate:1.0000e-05, Loss_tot:0.1941,\n",
      "save model\n",
      "Save model 3120\n",
      "Train Epoch:3121 learning rate:1.0000e-05, Loss_tot:0.1941,\n",
      "save model\n",
      "Save model 3121\n",
      "Train Epoch:3122 learning rate:1.0000e-05, Loss_tot:0.1940,\n",
      "save model\n",
      "Save model 3122\n",
      "Train Epoch:3123 learning rate:1.0000e-05, Loss_tot:0.1940,\n",
      "save model\n",
      "Save model 3123\n",
      "Train Epoch:3124 learning rate:1.0000e-05, Loss_tot:0.1939,\n",
      "save model\n",
      "Save model 3124\n",
      "Train Epoch:3125 learning rate:1.0000e-05, Loss_tot:0.1939,\n",
      "save model\n",
      "Save model 3125\n",
      "Train Epoch:3126 learning rate:1.0000e-05, Loss_tot:0.1939,\n",
      "Save model 3126\n",
      "Train Epoch:3127 learning rate:1.0000e-05, Loss_tot:0.1938,\n",
      "save model\n",
      "Save model 3127\n",
      "Train Epoch:3128 learning rate:1.0000e-05, Loss_tot:0.1938,\n",
      "save model\n",
      "Save model 3128\n",
      "Train Epoch:3129 learning rate:1.0000e-05, Loss_tot:0.1937,\n",
      "save model\n",
      "Save model 3129\n",
      "Train Epoch:3130 learning rate:1.0000e-05, Loss_tot:0.1937,\n",
      "save model\n",
      "Save model 3130\n",
      "Train Epoch:3131 learning rate:1.0000e-05, Loss_tot:0.1936,\n",
      "save model\n",
      "Save model 3131\n",
      "Train Epoch:3132 learning rate:1.0000e-05, Loss_tot:0.1935,\n",
      "save model\n",
      "Save model 3132\n",
      "Train Epoch:3133 learning rate:1.0000e-05, Loss_tot:0.1936,\n",
      "Save model 3133\n",
      "Train Epoch:3134 learning rate:1.0000e-05, Loss_tot:0.1936,\n",
      "Save model 3134\n",
      "Train Epoch:3135 learning rate:1.0000e-05, Loss_tot:0.1935,\n",
      "save model\n",
      "Save model 3135\n",
      "Train Epoch:3136 learning rate:1.0000e-05, Loss_tot:0.1934,\n",
      "save model\n",
      "Save model 3136\n",
      "Train Epoch:3137 learning rate:1.0000e-05, Loss_tot:0.1934,\n",
      "save model\n",
      "Save model 3137\n",
      "Train Epoch:3138 learning rate:1.0000e-05, Loss_tot:0.1934,\n",
      "save model\n",
      "Save model 3138\n",
      "Train Epoch:3139 learning rate:1.0000e-05, Loss_tot:0.1933,\n",
      "save model\n",
      "Save model 3139\n",
      "Train Epoch:3140 learning rate:1.0000e-05, Loss_tot:0.1932,\n",
      "save model\n",
      "Save model 3140\n",
      "Train Epoch:3141 learning rate:1.0000e-05, Loss_tot:0.1931,\n",
      "save model\n",
      "Save model 3141\n",
      "Train Epoch:3142 learning rate:1.0000e-05, Loss_tot:0.1932,\n",
      "Save model 3142\n",
      "Train Epoch:3143 learning rate:1.0000e-05, Loss_tot:0.1932,\n",
      "Save model 3143\n",
      "Train Epoch:3144 learning rate:1.0000e-05, Loss_tot:0.1931,\n",
      "save model\n",
      "Save model 3144\n",
      "Train Epoch:3145 learning rate:1.0000e-05, Loss_tot:0.1930,\n",
      "save model\n",
      "Save model 3145\n",
      "Train Epoch:3146 learning rate:1.0000e-05, Loss_tot:0.1930,\n",
      "save model\n",
      "Save model 3146\n",
      "Train Epoch:3147 learning rate:1.0000e-05, Loss_tot:0.1929,\n",
      "save model\n",
      "Save model 3147\n",
      "Train Epoch:3148 learning rate:1.0000e-05, Loss_tot:0.1929,\n",
      "save model\n",
      "Save model 3148\n",
      "Train Epoch:3149 learning rate:1.0000e-05, Loss_tot:0.1928,\n",
      "save model\n",
      "Save model 3149\n",
      "Train Epoch:3150 learning rate:1.0000e-05, Loss_tot:0.1927,\n",
      "save model\n",
      "Save model 3150\n",
      "Train Epoch:3151 learning rate:1.0000e-05, Loss_tot:0.1927,\n",
      "save model\n",
      "Save model 3151\n",
      "Train Epoch:3152 learning rate:1.0000e-05, Loss_tot:0.1927,\n",
      "save model\n",
      "Save model 3152\n",
      "Train Epoch:3153 learning rate:1.0000e-05, Loss_tot:0.1926,\n",
      "save model\n",
      "Save model 3153\n",
      "Train Epoch:3154 learning rate:1.0000e-05, Loss_tot:0.1926,\n",
      "save model\n",
      "Save model 3154\n",
      "Train Epoch:3155 learning rate:1.0000e-05, Loss_tot:0.1925,\n",
      "save model\n",
      "Save model 3155\n",
      "Train Epoch:3156 learning rate:1.0000e-05, Loss_tot:0.1925,\n",
      "Save model 3156\n",
      "Train Epoch:3157 learning rate:1.0000e-05, Loss_tot:0.1925,\n",
      "Save model 3157\n",
      "Train Epoch:3158 learning rate:1.0000e-05, Loss_tot:0.1924,\n",
      "save model\n",
      "Save model 3158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3159 learning rate:1.0000e-05, Loss_tot:0.1924,\n",
      "Save model 3159\n",
      "Train Epoch:3160 learning rate:1.0000e-05, Loss_tot:0.1924,\n",
      "Save model 3160\n",
      "Train Epoch:3161 learning rate:1.0000e-05, Loss_tot:0.1924,\n",
      "save model\n",
      "Save model 3161\n",
      "Train Epoch:3162 learning rate:1.0000e-05, Loss_tot:0.1923,\n",
      "save model\n",
      "Save model 3162\n",
      "Train Epoch:3163 learning rate:1.0000e-05, Loss_tot:0.1922,\n",
      "save model\n",
      "Save model 3163\n",
      "Train Epoch:3164 learning rate:1.0000e-05, Loss_tot:0.1922,\n",
      "save model\n",
      "Save model 3164\n",
      "Train Epoch:3165 learning rate:1.0000e-05, Loss_tot:0.1920,\n",
      "save model\n",
      "Save model 3165\n",
      "Train Epoch:3166 learning rate:1.0000e-05, Loss_tot:0.1921,\n",
      "Save model 3166\n",
      "Train Epoch:3167 learning rate:1.0000e-05, Loss_tot:0.1921,\n",
      "Save model 3167\n",
      "Train Epoch:3168 learning rate:1.0000e-05, Loss_tot:0.1921,\n",
      "Save model 3168\n",
      "Train Epoch:3169 learning rate:1.0000e-05, Loss_tot:0.1919,\n",
      "save model\n",
      "Save model 3169\n",
      "Train Epoch:3170 learning rate:1.0000e-05, Loss_tot:0.1919,\n",
      "save model\n",
      "Save model 3170\n",
      "Train Epoch:3171 learning rate:1.0000e-05, Loss_tot:0.1919,\n",
      "Save model 3171\n",
      "Train Epoch:3172 learning rate:1.0000e-05, Loss_tot:0.1919,\n",
      "Save model 3172\n",
      "Train Epoch:3173 learning rate:1.0000e-05, Loss_tot:0.1919,\n",
      "save model\n",
      "Save model 3173\n",
      "Train Epoch:3174 learning rate:1.0000e-05, Loss_tot:0.1918,\n",
      "save model\n",
      "Save model 3174\n",
      "Train Epoch:3175 learning rate:1.0000e-05, Loss_tot:0.1917,\n",
      "save model\n",
      "Save model 3175\n",
      "Train Epoch:3176 learning rate:1.0000e-05, Loss_tot:0.1916,\n",
      "save model\n",
      "Save model 3176\n",
      "Train Epoch:3177 learning rate:1.0000e-05, Loss_tot:0.1915,\n",
      "save model\n",
      "Save model 3177\n",
      "Train Epoch:3178 learning rate:1.0000e-05, Loss_tot:0.1916,\n",
      "Save model 3178\n",
      "Train Epoch:3179 learning rate:1.0000e-05, Loss_tot:0.1916,\n",
      "Save model 3179\n",
      "Train Epoch:3180 learning rate:1.0000e-05, Loss_tot:0.1915,\n",
      "Save model 3180\n",
      "Train Epoch:3181 learning rate:1.0000e-05, Loss_tot:0.1914,\n",
      "save model\n",
      "Save model 3181\n",
      "Train Epoch:3182 learning rate:1.0000e-05, Loss_tot:0.1913,\n",
      "save model\n",
      "Save model 3182\n",
      "Train Epoch:3183 learning rate:1.0000e-05, Loss_tot:0.1913,\n",
      "Save model 3183\n",
      "Train Epoch:3184 learning rate:1.0000e-05, Loss_tot:0.1913,\n",
      "Save model 3184\n",
      "Train Epoch:3185 learning rate:1.0000e-05, Loss_tot:0.1913,\n",
      "save model\n",
      "Save model 3185\n",
      "Train Epoch:3186 learning rate:1.0000e-05, Loss_tot:0.1912,\n",
      "save model\n",
      "Save model 3186\n",
      "Train Epoch:3187 learning rate:1.0000e-05, Loss_tot:0.1911,\n",
      "save model\n",
      "Save model 3187\n",
      "Train Epoch:3188 learning rate:1.0000e-05, Loss_tot:0.1910,\n",
      "save model\n",
      "Save model 3188\n",
      "Train Epoch:3189 learning rate:1.0000e-05, Loss_tot:0.1910,\n",
      "save model\n",
      "Save model 3189\n",
      "Train Epoch:3190 learning rate:1.0000e-05, Loss_tot:0.1910,\n",
      "Save model 3190\n",
      "Train Epoch:3191 learning rate:1.0000e-05, Loss_tot:0.1909,\n",
      "save model\n",
      "Save model 3191\n",
      "Train Epoch:3192 learning rate:1.0000e-05, Loss_tot:0.1908,\n",
      "save model\n",
      "Save model 3192\n",
      "Train Epoch:3193 learning rate:1.0000e-05, Loss_tot:0.1908,\n",
      "save model\n",
      "Save model 3193\n",
      "Train Epoch:3194 learning rate:1.0000e-05, Loss_tot:0.1908,\n",
      "save model\n",
      "Save model 3194\n",
      "Train Epoch:3195 learning rate:1.0000e-05, Loss_tot:0.1907,\n",
      "save model\n",
      "Save model 3195\n",
      "Train Epoch:3196 learning rate:1.0000e-05, Loss_tot:0.1906,\n",
      "save model\n",
      "Save model 3196\n",
      "Train Epoch:3197 learning rate:1.0000e-05, Loss_tot:0.1906,\n",
      "save model\n",
      "Save model 3197\n",
      "Train Epoch:3198 learning rate:1.0000e-05, Loss_tot:0.1906,\n",
      "save model\n",
      "Save model 3198\n",
      "Train Epoch:3199 learning rate:1.0000e-05, Loss_tot:0.1905,\n",
      "save model\n",
      "Save model 3199\n",
      "Train Epoch:3200 learning rate:1.0000e-05, Loss_tot:0.1905,\n",
      "Save model 3200\n",
      "Train Epoch:3201 learning rate:1.0000e-05, Loss_tot:0.1905,\n",
      "Save model 3201\n",
      "Train Epoch:3202 learning rate:1.0000e-05, Loss_tot:0.1905,\n",
      "Save model 3202\n",
      "Train Epoch:3203 learning rate:1.0000e-05, Loss_tot:0.1904,\n",
      "save model\n",
      "Save model 3203\n",
      "Train Epoch:3204 learning rate:1.0000e-05, Loss_tot:0.1904,\n",
      "save model\n",
      "Save model 3204\n",
      "Train Epoch:3205 learning rate:1.0000e-05, Loss_tot:0.1903,\n",
      "save model\n",
      "Save model 3205\n",
      "Train Epoch:3206 learning rate:1.0000e-05, Loss_tot:0.1902,\n",
      "save model\n",
      "Save model 3206\n",
      "Train Epoch:3207 learning rate:1.0000e-05, Loss_tot:0.1901,\n",
      "save model\n",
      "Save model 3207\n",
      "Train Epoch:3208 learning rate:1.0000e-05, Loss_tot:0.1901,\n",
      "save model\n",
      "Save model 3208\n",
      "Train Epoch:3209 learning rate:1.0000e-05, Loss_tot:0.1900,\n",
      "save model\n",
      "Save model 3209\n",
      "Train Epoch:3210 learning rate:1.0000e-05, Loss_tot:0.1900,\n",
      "save model\n",
      "Save model 3210\n",
      "Train Epoch:3211 learning rate:1.0000e-05, Loss_tot:0.1899,\n",
      "save model\n",
      "Save model 3211\n",
      "Train Epoch:3212 learning rate:1.0000e-05, Loss_tot:0.1899,\n",
      "save model\n",
      "Save model 3212\n",
      "Train Epoch:3213 learning rate:1.0000e-05, Loss_tot:0.1898,\n",
      "save model\n",
      "Save model 3213\n",
      "Train Epoch:3214 learning rate:1.0000e-05, Loss_tot:0.1899,\n",
      "Save model 3214\n",
      "Train Epoch:3215 learning rate:1.0000e-05, Loss_tot:0.1898,\n",
      "save model\n",
      "Save model 3215\n",
      "Train Epoch:3216 learning rate:1.0000e-05, Loss_tot:0.1897,\n",
      "save model\n",
      "Save model 3216\n",
      "Train Epoch:3217 learning rate:1.0000e-05, Loss_tot:0.1897,\n",
      "save model\n",
      "Save model 3217\n",
      "Train Epoch:3218 learning rate:1.0000e-05, Loss_tot:0.1896,\n",
      "save model\n",
      "Save model 3218\n",
      "Train Epoch:3219 learning rate:1.0000e-05, Loss_tot:0.1895,\n",
      "save model\n",
      "Save model 3219\n",
      "Train Epoch:3220 learning rate:1.0000e-05, Loss_tot:0.1896,\n",
      "Save model 3220\n",
      "Train Epoch:3221 learning rate:1.0000e-05, Loss_tot:0.1895,\n",
      "save model\n",
      "Save model 3221\n",
      "Train Epoch:3222 learning rate:1.0000e-05, Loss_tot:0.1894,\n",
      "save model\n",
      "Save model 3222\n",
      "Train Epoch:3223 learning rate:1.0000e-05, Loss_tot:0.1894,\n",
      "save model\n",
      "Save model 3223\n",
      "Train Epoch:3224 learning rate:1.0000e-05, Loss_tot:0.1893,\n",
      "save model\n",
      "Save model 3224\n",
      "Train Epoch:3225 learning rate:1.0000e-05, Loss_tot:0.1893,\n",
      "save model\n",
      "Save model 3225\n",
      "Train Epoch:3226 learning rate:1.0000e-05, Loss_tot:0.1892,\n",
      "save model\n",
      "Save model 3226\n",
      "Train Epoch:3227 learning rate:1.0000e-05, Loss_tot:0.1892,\n",
      "save model\n",
      "Save model 3227\n",
      "Train Epoch:3228 learning rate:1.0000e-05, Loss_tot:0.1892,\n",
      "Save model 3228\n",
      "Train Epoch:3229 learning rate:1.0000e-05, Loss_tot:0.1891,\n",
      "save model\n",
      "Save model 3229\n",
      "Train Epoch:3230 learning rate:1.0000e-05, Loss_tot:0.1891,\n",
      "save model\n",
      "Save model 3230\n",
      "Train Epoch:3231 learning rate:1.0000e-05, Loss_tot:0.1891,\n",
      "save model\n",
      "Save model 3231\n",
      "Train Epoch:3232 learning rate:1.0000e-05, Loss_tot:0.1890,\n",
      "save model\n",
      "Save model 3232\n",
      "Train Epoch:3233 learning rate:1.0000e-05, Loss_tot:0.1890,\n",
      "save model\n",
      "Save model 3233\n",
      "Train Epoch:3234 learning rate:1.0000e-05, Loss_tot:0.1889,\n",
      "save model\n",
      "Save model 3234\n",
      "Train Epoch:3235 learning rate:1.0000e-05, Loss_tot:0.1888,\n",
      "save model\n",
      "Save model 3235\n",
      "Train Epoch:3236 learning rate:1.0000e-05, Loss_tot:0.1888,\n",
      "save model\n",
      "Save model 3236\n",
      "Train Epoch:3237 learning rate:1.0000e-05, Loss_tot:0.1887,\n",
      "save model\n",
      "Save model 3237\n",
      "Train Epoch:3238 learning rate:1.0000e-05, Loss_tot:0.1887,\n",
      "save model\n",
      "Save model 3238\n",
      "Train Epoch:3239 learning rate:1.0000e-05, Loss_tot:0.1886,\n",
      "save model\n",
      "Save model 3239\n",
      "Train Epoch:3240 learning rate:1.0000e-05, Loss_tot:0.1886,\n",
      "save model\n",
      "Save model 3240\n",
      "Train Epoch:3241 learning rate:1.0000e-05, Loss_tot:0.1885,\n",
      "save model\n",
      "Save model 3241\n",
      "Train Epoch:3242 learning rate:1.0000e-05, Loss_tot:0.1885,\n",
      "save model\n",
      "Save model 3242\n",
      "Train Epoch:3243 learning rate:1.0000e-05, Loss_tot:0.1884,\n",
      "save model\n",
      "Save model 3243\n",
      "Train Epoch:3244 learning rate:1.0000e-05, Loss_tot:0.1884,\n",
      "save model\n",
      "Save model 3244\n",
      "Train Epoch:3245 learning rate:1.0000e-05, Loss_tot:0.1883,\n",
      "save model\n",
      "Save model 3245\n",
      "Train Epoch:3246 learning rate:1.0000e-05, Loss_tot:0.1883,\n",
      "save model\n",
      "Save model 3246\n",
      "Train Epoch:3247 learning rate:1.0000e-05, Loss_tot:0.1883,\n",
      "save model\n",
      "Save model 3247\n",
      "Train Epoch:3248 learning rate:1.0000e-05, Loss_tot:0.1882,\n",
      "save model\n",
      "Save model 3248\n",
      "Train Epoch:3249 learning rate:1.0000e-05, Loss_tot:0.1881,\n",
      "save model\n",
      "Save model 3249\n",
      "Train Epoch:3250 learning rate:1.0000e-05, Loss_tot:0.1881,\n",
      "save model\n",
      "Save model 3250\n",
      "Train Epoch:3251 learning rate:1.0000e-05, Loss_tot:0.1881,\n",
      "save model\n",
      "Save model 3251\n",
      "Train Epoch:3252 learning rate:1.0000e-05, Loss_tot:0.1880,\n",
      "save model\n",
      "Save model 3252\n",
      "Train Epoch:3253 learning rate:1.0000e-05, Loss_tot:0.1880,\n",
      "save model\n",
      "Save model 3253\n",
      "Train Epoch:3254 learning rate:1.0000e-05, Loss_tot:0.1879,\n",
      "save model\n",
      "Save model 3254\n",
      "Train Epoch:3255 learning rate:1.0000e-05, Loss_tot:0.1879,\n",
      "save model\n",
      "Save model 3255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3256 learning rate:1.0000e-05, Loss_tot:0.1878,\n",
      "save model\n",
      "Save model 3256\n",
      "Train Epoch:3257 learning rate:1.0000e-05, Loss_tot:0.1878,\n",
      "save model\n",
      "Save model 3257\n",
      "Train Epoch:3258 learning rate:1.0000e-05, Loss_tot:0.1877,\n",
      "save model\n",
      "Save model 3258\n",
      "Train Epoch:3259 learning rate:1.0000e-05, Loss_tot:0.1877,\n",
      "save model\n",
      "Save model 3259\n",
      "Train Epoch:3260 learning rate:1.0000e-05, Loss_tot:0.1876,\n",
      "save model\n",
      "Save model 3260\n",
      "Train Epoch:3261 learning rate:1.0000e-05, Loss_tot:0.1877,\n",
      "Save model 3261\n",
      "Train Epoch:3262 learning rate:1.0000e-05, Loss_tot:0.1876,\n",
      "save model\n",
      "Save model 3262\n",
      "Train Epoch:3263 learning rate:1.0000e-05, Loss_tot:0.1875,\n",
      "save model\n",
      "Save model 3263\n",
      "Train Epoch:3264 learning rate:1.0000e-05, Loss_tot:0.1875,\n",
      "Save model 3264\n",
      "Train Epoch:3265 learning rate:1.0000e-05, Loss_tot:0.1875,\n",
      "Save model 3265\n",
      "Train Epoch:3266 learning rate:1.0000e-05, Loss_tot:0.1875,\n",
      "Save model 3266\n",
      "Train Epoch:3267 learning rate:1.0000e-05, Loss_tot:0.1874,\n",
      "save model\n",
      "Save model 3267\n",
      "Train Epoch:3268 learning rate:1.0000e-05, Loss_tot:0.1874,\n",
      "save model\n",
      "Save model 3268\n",
      "Train Epoch:3269 learning rate:1.0000e-05, Loss_tot:0.1873,\n",
      "save model\n",
      "Save model 3269\n",
      "Train Epoch:3270 learning rate:1.0000e-05, Loss_tot:0.1872,\n",
      "save model\n",
      "Save model 3270\n",
      "Train Epoch:3271 learning rate:1.0000e-05, Loss_tot:0.1872,\n",
      "Save model 3271\n",
      "Train Epoch:3272 learning rate:1.0000e-05, Loss_tot:0.1873,\n",
      "Save model 3272\n",
      "Train Epoch:3273 learning rate:1.0000e-05, Loss_tot:0.1872,\n",
      "Save model 3273\n",
      "Train Epoch:3274 learning rate:1.0000e-05, Loss_tot:0.1870,\n",
      "save model\n",
      "Save model 3274\n",
      "Train Epoch:3275 learning rate:1.0000e-05, Loss_tot:0.1870,\n",
      "save model\n",
      "Save model 3275\n",
      "Train Epoch:3276 learning rate:1.0000e-05, Loss_tot:0.1870,\n",
      "Save model 3276\n",
      "Train Epoch:3277 learning rate:1.0000e-05, Loss_tot:0.1870,\n",
      "Save model 3277\n",
      "Train Epoch:3278 learning rate:1.0000e-05, Loss_tot:0.1870,\n",
      "save model\n",
      "Save model 3278\n",
      "Train Epoch:3279 learning rate:1.0000e-05, Loss_tot:0.1869,\n",
      "save model\n",
      "Save model 3279\n",
      "Train Epoch:3280 learning rate:1.0000e-05, Loss_tot:0.1868,\n",
      "save model\n",
      "Save model 3280\n",
      "Train Epoch:3281 learning rate:1.0000e-05, Loss_tot:0.1867,\n",
      "save model\n",
      "Save model 3281\n",
      "Train Epoch:3282 learning rate:1.0000e-05, Loss_tot:0.1866,\n",
      "save model\n",
      "Save model 3282\n",
      "Train Epoch:3283 learning rate:1.0000e-05, Loss_tot:0.1866,\n",
      "Save model 3283\n",
      "Train Epoch:3284 learning rate:1.0000e-05, Loss_tot:0.1865,\n",
      "save model\n",
      "Save model 3284\n",
      "Train Epoch:3285 learning rate:1.0000e-05, Loss_tot:0.1865,\n",
      "Save model 3285\n",
      "Train Epoch:3286 learning rate:1.0000e-05, Loss_tot:0.1865,\n",
      "save model\n",
      "Save model 3286\n",
      "Train Epoch:3287 learning rate:1.0000e-05, Loss_tot:0.1865,\n",
      "save model\n",
      "Save model 3287\n",
      "Train Epoch:3288 learning rate:1.0000e-05, Loss_tot:0.1864,\n",
      "save model\n",
      "Save model 3288\n",
      "Train Epoch:3289 learning rate:1.0000e-05, Loss_tot:0.1863,\n",
      "save model\n",
      "Save model 3289\n",
      "Train Epoch:3290 learning rate:1.0000e-05, Loss_tot:0.1884,\n",
      "Save model 3290\n",
      "Train Epoch:3291 learning rate:1.0000e-05, Loss_tot:0.1883,\n",
      "Save model 3291\n",
      "Train Epoch:3292 learning rate:1.0000e-05, Loss_tot:0.1881,\n",
      "Save model 3292\n",
      "Train Epoch:3293 learning rate:1.0000e-05, Loss_tot:0.1879,\n",
      "Save model 3293\n",
      "Train Epoch:3294 learning rate:1.0000e-05, Loss_tot:0.1877,\n",
      "Save model 3294\n",
      "Train Epoch:3295 learning rate:1.0000e-05, Loss_tot:0.1875,\n",
      "Save model 3295\n",
      "Train Epoch:3296 learning rate:1.0000e-05, Loss_tot:0.1873,\n",
      "Save model 3296\n",
      "Train Epoch:3297 learning rate:1.0000e-05, Loss_tot:0.1872,\n",
      "Save model 3297\n",
      "Train Epoch:3298 learning rate:1.0000e-05, Loss_tot:0.1871,\n",
      "Save model 3298\n",
      "Train Epoch:3299 learning rate:1.0000e-05, Loss_tot:0.1869,\n",
      "Save model 3299\n",
      "Train Epoch:3300 learning rate:1.0000e-05, Loss_tot:0.1868,\n",
      "Save model 3300\n",
      "Train Epoch:3301 learning rate:1.0000e-05, Loss_tot:0.1867,\n",
      "Save model 3301\n",
      "Train Epoch:3302 learning rate:1.0000e-05, Loss_tot:0.1866,\n",
      "Save model 3302\n",
      "Train Epoch:3303 learning rate:1.0000e-05, Loss_tot:0.1869,\n",
      "Save model 3303\n",
      "Train Epoch:3304 learning rate:1.0000e-05, Loss_tot:0.1868,\n",
      "Save model 3304\n",
      "Train Epoch:3305 learning rate:1.0000e-05, Loss_tot:0.1867,\n",
      "Save model 3305\n",
      "Train Epoch:3306 learning rate:1.0000e-05, Loss_tot:0.1863,\n",
      "save model\n",
      "Save model 3306\n",
      "Train Epoch:3307 learning rate:1.0000e-05, Loss_tot:0.1863,\n",
      "save model\n",
      "Save model 3307\n",
      "Train Epoch:3308 learning rate:1.0000e-05, Loss_tot:0.1862,\n",
      "save model\n",
      "Save model 3308\n",
      "Train Epoch:3309 learning rate:1.0000e-05, Loss_tot:0.1861,\n",
      "save model\n",
      "Save model 3309\n",
      "Train Epoch:3310 learning rate:1.0000e-05, Loss_tot:0.1860,\n",
      "save model\n",
      "Save model 3310\n",
      "Train Epoch:3311 learning rate:1.0000e-05, Loss_tot:0.1860,\n",
      "save model\n",
      "Save model 3311\n",
      "Train Epoch:3312 learning rate:1.0000e-05, Loss_tot:0.1859,\n",
      "save model\n",
      "Save model 3312\n",
      "Train Epoch:3313 learning rate:1.0000e-05, Loss_tot:0.1858,\n",
      "save model\n",
      "Save model 3313\n",
      "Train Epoch:3314 learning rate:1.0000e-05, Loss_tot:0.1858,\n",
      "save model\n",
      "Save model 3314\n",
      "Train Epoch:3315 learning rate:1.0000e-05, Loss_tot:0.1858,\n",
      "save model\n",
      "Save model 3315\n",
      "Train Epoch:3316 learning rate:1.0000e-05, Loss_tot:0.1857,\n",
      "save model\n",
      "Save model 3316\n",
      "Train Epoch:3317 learning rate:1.0000e-05, Loss_tot:0.1857,\n",
      "save model\n",
      "Save model 3317\n",
      "Train Epoch:3318 learning rate:1.0000e-05, Loss_tot:0.1856,\n",
      "save model\n",
      "Save model 3318\n",
      "Train Epoch:3319 learning rate:1.0000e-05, Loss_tot:0.1856,\n",
      "save model\n",
      "Save model 3319\n",
      "Train Epoch:3320 learning rate:1.0000e-05, Loss_tot:0.1855,\n",
      "save model\n",
      "Save model 3320\n",
      "Train Epoch:3321 learning rate:1.0000e-05, Loss_tot:0.1854,\n",
      "save model\n",
      "Save model 3321\n",
      "Train Epoch:3322 learning rate:1.0000e-05, Loss_tot:0.1854,\n",
      "save model\n",
      "Save model 3322\n",
      "Train Epoch:3323 learning rate:1.0000e-05, Loss_tot:0.1854,\n",
      "save model\n",
      "Save model 3323\n",
      "Train Epoch:3324 learning rate:1.0000e-05, Loss_tot:0.1854,\n",
      "save model\n",
      "Save model 3324\n",
      "Train Epoch:3325 learning rate:1.0000e-05, Loss_tot:0.1853,\n",
      "save model\n",
      "Save model 3325\n",
      "Train Epoch:3326 learning rate:1.0000e-05, Loss_tot:0.1852,\n",
      "save model\n",
      "Save model 3326\n",
      "Train Epoch:3327 learning rate:1.0000e-05, Loss_tot:0.1851,\n",
      "save model\n",
      "Save model 3327\n",
      "Train Epoch:3328 learning rate:1.0000e-05, Loss_tot:0.1851,\n",
      "save model\n",
      "Save model 3328\n",
      "Train Epoch:3329 learning rate:1.0000e-05, Loss_tot:0.1851,\n",
      "save model\n",
      "Save model 3329\n",
      "Train Epoch:3330 learning rate:1.0000e-05, Loss_tot:0.1850,\n",
      "save model\n",
      "Save model 3330\n",
      "Train Epoch:3331 learning rate:1.0000e-05, Loss_tot:0.1849,\n",
      "save model\n",
      "Save model 3331\n",
      "Train Epoch:3332 learning rate:1.0000e-05, Loss_tot:0.1849,\n",
      "save model\n",
      "Save model 3332\n",
      "Train Epoch:3333 learning rate:1.0000e-05, Loss_tot:0.1849,\n",
      "save model\n",
      "Save model 3333\n",
      "Train Epoch:3334 learning rate:1.0000e-05, Loss_tot:0.1848,\n",
      "save model\n",
      "Save model 3334\n",
      "Train Epoch:3335 learning rate:1.0000e-05, Loss_tot:0.1848,\n",
      "save model\n",
      "Save model 3335\n",
      "Train Epoch:3336 learning rate:1.0000e-05, Loss_tot:0.1847,\n",
      "save model\n",
      "Save model 3336\n",
      "Train Epoch:3337 learning rate:1.0000e-05, Loss_tot:0.1846,\n",
      "save model\n",
      "Save model 3337\n",
      "Train Epoch:3338 learning rate:1.0000e-05, Loss_tot:0.1846,\n",
      "save model\n",
      "Save model 3338\n",
      "Train Epoch:3339 learning rate:1.0000e-05, Loss_tot:0.1846,\n",
      "save model\n",
      "Save model 3339\n",
      "Train Epoch:3340 learning rate:1.0000e-05, Loss_tot:0.1845,\n",
      "save model\n",
      "Save model 3340\n",
      "Train Epoch:3341 learning rate:1.0000e-05, Loss_tot:0.1844,\n",
      "save model\n",
      "Save model 3341\n",
      "Train Epoch:3342 learning rate:1.0000e-05, Loss_tot:0.1844,\n",
      "save model\n",
      "Save model 3342\n",
      "Train Epoch:3343 learning rate:1.0000e-05, Loss_tot:0.1843,\n",
      "save model\n",
      "Save model 3343\n",
      "Train Epoch:3344 learning rate:1.0000e-05, Loss_tot:0.1844,\n",
      "Save model 3344\n",
      "Train Epoch:3345 learning rate:1.0000e-05, Loss_tot:0.1843,\n",
      "save model\n",
      "Save model 3345\n",
      "Train Epoch:3346 learning rate:1.0000e-05, Loss_tot:0.1842,\n",
      "save model\n",
      "Save model 3346\n",
      "Train Epoch:3347 learning rate:1.0000e-05, Loss_tot:0.1842,\n",
      "save model\n",
      "Save model 3347\n",
      "Train Epoch:3348 learning rate:1.0000e-05, Loss_tot:0.1842,\n",
      "save model\n",
      "Save model 3348\n",
      "Train Epoch:3349 learning rate:1.0000e-05, Loss_tot:0.1841,\n",
      "save model\n",
      "Save model 3349\n",
      "Train Epoch:3350 learning rate:1.0000e-05, Loss_tot:0.1840,\n",
      "save model\n",
      "Save model 3350\n",
      "Train Epoch:3351 learning rate:1.0000e-05, Loss_tot:0.1841,\n",
      "Save model 3351\n",
      "Train Epoch:3352 learning rate:1.0000e-05, Loss_tot:0.1840,\n",
      "Save model 3352\n",
      "Train Epoch:3353 learning rate:1.0000e-05, Loss_tot:0.1839,\n",
      "save model\n",
      "Save model 3353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3354 learning rate:1.0000e-05, Loss_tot:0.1839,\n",
      "save model\n",
      "Save model 3354\n",
      "Train Epoch:3355 learning rate:1.0000e-05, Loss_tot:0.1839,\n",
      "save model\n",
      "Save model 3355\n",
      "Train Epoch:3356 learning rate:1.0000e-05, Loss_tot:0.1838,\n",
      "save model\n",
      "Save model 3356\n",
      "Train Epoch:3357 learning rate:1.0000e-05, Loss_tot:0.1838,\n",
      "save model\n",
      "Save model 3357\n",
      "Train Epoch:3358 learning rate:1.0000e-05, Loss_tot:0.1837,\n",
      "save model\n",
      "Save model 3358\n",
      "Train Epoch:3359 learning rate:1.0000e-05, Loss_tot:0.1836,\n",
      "save model\n",
      "Save model 3359\n",
      "Train Epoch:3360 learning rate:1.0000e-05, Loss_tot:0.1836,\n",
      "Save model 3360\n",
      "Train Epoch:3361 learning rate:1.0000e-05, Loss_tot:0.1836,\n",
      "Save model 3361\n",
      "Train Epoch:3362 learning rate:1.0000e-05, Loss_tot:0.1835,\n",
      "save model\n",
      "Save model 3362\n",
      "Train Epoch:3363 learning rate:1.0000e-05, Loss_tot:0.1834,\n",
      "save model\n",
      "Save model 3363\n",
      "Train Epoch:3364 learning rate:1.0000e-05, Loss_tot:0.1834,\n",
      "save model\n",
      "Save model 3364\n",
      "Train Epoch:3365 learning rate:1.0000e-05, Loss_tot:0.1834,\n",
      "save model\n",
      "Save model 3365\n",
      "Train Epoch:3366 learning rate:1.0000e-05, Loss_tot:0.1833,\n",
      "save model\n",
      "Save model 3366\n",
      "Train Epoch:3367 learning rate:1.0000e-05, Loss_tot:0.1833,\n",
      "save model\n",
      "Save model 3367\n",
      "Train Epoch:3368 learning rate:1.0000e-05, Loss_tot:0.1832,\n",
      "save model\n",
      "Save model 3368\n",
      "Train Epoch:3369 learning rate:1.0000e-05, Loss_tot:0.1832,\n",
      "save model\n",
      "Save model 3369\n",
      "Train Epoch:3370 learning rate:1.0000e-05, Loss_tot:0.1831,\n",
      "save model\n",
      "Save model 3370\n",
      "Train Epoch:3371 learning rate:1.0000e-05, Loss_tot:0.1831,\n",
      "save model\n",
      "Save model 3371\n",
      "Train Epoch:3372 learning rate:1.0000e-05, Loss_tot:0.1830,\n",
      "save model\n",
      "Save model 3372\n",
      "Train Epoch:3373 learning rate:1.0000e-05, Loss_tot:0.1830,\n",
      "save model\n",
      "Save model 3373\n",
      "Train Epoch:3374 learning rate:1.0000e-05, Loss_tot:0.1829,\n",
      "save model\n",
      "Save model 3374\n",
      "Train Epoch:3375 learning rate:1.0000e-05, Loss_tot:0.1829,\n",
      "save model\n",
      "Save model 3375\n",
      "Train Epoch:3376 learning rate:1.0000e-05, Loss_tot:0.1828,\n",
      "save model\n",
      "Save model 3376\n",
      "Train Epoch:3377 learning rate:1.0000e-05, Loss_tot:0.1828,\n",
      "save model\n",
      "Save model 3377\n",
      "Train Epoch:3378 learning rate:1.0000e-05, Loss_tot:0.1827,\n",
      "save model\n",
      "Save model 3378\n",
      "Train Epoch:3379 learning rate:1.0000e-05, Loss_tot:0.1827,\n",
      "save model\n",
      "Save model 3379\n",
      "Train Epoch:3380 learning rate:1.0000e-05, Loss_tot:0.1827,\n",
      "save model\n",
      "Save model 3380\n",
      "Train Epoch:3381 learning rate:1.0000e-05, Loss_tot:0.1826,\n",
      "save model\n",
      "Save model 3381\n",
      "Train Epoch:3382 learning rate:1.0000e-05, Loss_tot:0.1826,\n",
      "save model\n",
      "Save model 3382\n",
      "Train Epoch:3383 learning rate:1.0000e-05, Loss_tot:0.1826,\n",
      "save model\n",
      "Save model 3383\n",
      "Train Epoch:3384 learning rate:1.0000e-05, Loss_tot:0.1825,\n",
      "save model\n",
      "Save model 3384\n",
      "Train Epoch:3385 learning rate:1.0000e-05, Loss_tot:0.1824,\n",
      "save model\n",
      "Save model 3385\n",
      "Train Epoch:3386 learning rate:1.0000e-05, Loss_tot:0.1824,\n",
      "save model\n",
      "Save model 3386\n",
      "Train Epoch:3387 learning rate:1.0000e-05, Loss_tot:0.1824,\n",
      "save model\n",
      "Save model 3387\n",
      "Train Epoch:3388 learning rate:1.0000e-05, Loss_tot:0.1823,\n",
      "save model\n",
      "Save model 3388\n",
      "Train Epoch:3389 learning rate:1.0000e-05, Loss_tot:0.1823,\n",
      "save model\n",
      "Save model 3389\n",
      "Train Epoch:3390 learning rate:1.0000e-05, Loss_tot:0.1822,\n",
      "save model\n",
      "Save model 3390\n",
      "Train Epoch:3391 learning rate:1.0000e-05, Loss_tot:0.1822,\n",
      "save model\n",
      "Save model 3391\n",
      "Train Epoch:3392 learning rate:1.0000e-05, Loss_tot:0.1822,\n",
      "save model\n",
      "Save model 3392\n",
      "Train Epoch:3393 learning rate:1.0000e-05, Loss_tot:0.1821,\n",
      "save model\n",
      "Save model 3393\n",
      "Train Epoch:3394 learning rate:1.0000e-05, Loss_tot:0.1821,\n",
      "save model\n",
      "Save model 3394\n",
      "Train Epoch:3395 learning rate:1.0000e-05, Loss_tot:0.1820,\n",
      "save model\n",
      "Save model 3395\n",
      "Train Epoch:3396 learning rate:1.0000e-05, Loss_tot:0.1820,\n",
      "save model\n",
      "Save model 3396\n",
      "Train Epoch:3397 learning rate:1.0000e-05, Loss_tot:0.1819,\n",
      "save model\n",
      "Save model 3397\n",
      "Train Epoch:3398 learning rate:1.0000e-05, Loss_tot:0.1819,\n",
      "save model\n",
      "Save model 3398\n",
      "Train Epoch:3399 learning rate:1.0000e-05, Loss_tot:0.1818,\n",
      "save model\n",
      "Save model 3399\n",
      "Train Epoch:3400 learning rate:1.0000e-05, Loss_tot:0.1818,\n",
      "save model\n",
      "Save model 3400\n",
      "Train Epoch:3401 learning rate:1.0000e-05, Loss_tot:0.1818,\n",
      "save model\n",
      "Save model 3401\n",
      "Train Epoch:3402 learning rate:1.0000e-05, Loss_tot:0.1817,\n",
      "save model\n",
      "Save model 3402\n",
      "Train Epoch:3403 learning rate:1.0000e-05, Loss_tot:0.1816,\n",
      "save model\n",
      "Save model 3403\n",
      "Train Epoch:3404 learning rate:1.0000e-05, Loss_tot:0.1816,\n",
      "save model\n",
      "Save model 3404\n",
      "Train Epoch:3405 learning rate:1.0000e-05, Loss_tot:0.1816,\n",
      "save model\n",
      "Save model 3405\n",
      "Train Epoch:3406 learning rate:1.0000e-05, Loss_tot:0.1815,\n",
      "save model\n",
      "Save model 3406\n",
      "Train Epoch:3407 learning rate:1.0000e-05, Loss_tot:0.1815,\n",
      "save model\n",
      "Save model 3407\n",
      "Train Epoch:3408 learning rate:1.0000e-05, Loss_tot:0.1815,\n",
      "Save model 3408\n",
      "Train Epoch:3409 learning rate:1.0000e-05, Loss_tot:0.1814,\n",
      "save model\n",
      "Save model 3409\n",
      "Train Epoch:3410 learning rate:1.0000e-05, Loss_tot:0.1814,\n",
      "save model\n",
      "Save model 3410\n",
      "Train Epoch:3411 learning rate:1.0000e-05, Loss_tot:0.1813,\n",
      "save model\n",
      "Save model 3411\n",
      "Train Epoch:3412 learning rate:1.0000e-05, Loss_tot:0.1813,\n",
      "save model\n",
      "Save model 3412\n",
      "Train Epoch:3413 learning rate:1.0000e-05, Loss_tot:0.1812,\n",
      "save model\n",
      "Save model 3413\n",
      "Train Epoch:3414 learning rate:1.0000e-05, Loss_tot:0.1812,\n",
      "save model\n",
      "Save model 3414\n",
      "Train Epoch:3415 learning rate:1.0000e-05, Loss_tot:0.1812,\n",
      "save model\n",
      "Save model 3415\n",
      "Train Epoch:3416 learning rate:1.0000e-05, Loss_tot:0.1811,\n",
      "save model\n",
      "Save model 3416\n",
      "Train Epoch:3417 learning rate:1.0000e-05, Loss_tot:0.1811,\n",
      "save model\n",
      "Save model 3417\n",
      "Train Epoch:3418 learning rate:1.0000e-05, Loss_tot:0.1810,\n",
      "save model\n",
      "Save model 3418\n",
      "Train Epoch:3419 learning rate:1.0000e-05, Loss_tot:0.1810,\n",
      "save model\n",
      "Save model 3419\n",
      "Train Epoch:3420 learning rate:1.0000e-05, Loss_tot:0.1810,\n",
      "Save model 3420\n",
      "Train Epoch:3421 learning rate:1.0000e-05, Loss_tot:0.1809,\n",
      "save model\n",
      "Save model 3421\n",
      "Train Epoch:3422 learning rate:1.0000e-05, Loss_tot:0.1808,\n",
      "save model\n",
      "Save model 3422\n",
      "Train Epoch:3423 learning rate:1.0000e-05, Loss_tot:0.1808,\n",
      "save model\n",
      "Save model 3423\n",
      "Train Epoch:3424 learning rate:1.0000e-05, Loss_tot:0.1807,\n",
      "save model\n",
      "Save model 3424\n",
      "Train Epoch:3425 learning rate:1.0000e-05, Loss_tot:0.1807,\n",
      "save model\n",
      "Save model 3425\n",
      "Train Epoch:3426 learning rate:1.0000e-05, Loss_tot:0.1806,\n",
      "save model\n",
      "Save model 3426\n",
      "Train Epoch:3427 learning rate:1.0000e-05, Loss_tot:0.1806,\n",
      "save model\n",
      "Save model 3427\n",
      "Train Epoch:3428 learning rate:1.0000e-05, Loss_tot:0.1806,\n",
      "save model\n",
      "Save model 3428\n",
      "Train Epoch:3429 learning rate:1.0000e-05, Loss_tot:0.1806,\n",
      "save model\n",
      "Save model 3429\n",
      "Train Epoch:3430 learning rate:1.0000e-05, Loss_tot:0.1805,\n",
      "save model\n",
      "Save model 3430\n",
      "Train Epoch:3431 learning rate:1.0000e-05, Loss_tot:0.1805,\n",
      "save model\n",
      "Save model 3431\n",
      "Train Epoch:3432 learning rate:1.0000e-05, Loss_tot:0.1804,\n",
      "save model\n",
      "Save model 3432\n",
      "Train Epoch:3433 learning rate:1.0000e-05, Loss_tot:0.1804,\n",
      "Save model 3433\n",
      "Train Epoch:3434 learning rate:1.0000e-05, Loss_tot:0.1804,\n",
      "save model\n",
      "Save model 3434\n",
      "Train Epoch:3435 learning rate:1.0000e-05, Loss_tot:0.1803,\n",
      "save model\n",
      "Save model 3435\n",
      "Train Epoch:3436 learning rate:1.0000e-05, Loss_tot:0.1803,\n",
      "save model\n",
      "Save model 3436\n",
      "Train Epoch:3437 learning rate:1.0000e-05, Loss_tot:0.1802,\n",
      "save model\n",
      "Save model 3437\n",
      "Train Epoch:3438 learning rate:1.0000e-05, Loss_tot:0.1802,\n",
      "Save model 3438\n",
      "Train Epoch:3439 learning rate:1.0000e-05, Loss_tot:0.1802,\n",
      "Save model 3439\n",
      "Train Epoch:3440 learning rate:1.0000e-05, Loss_tot:0.1801,\n",
      "save model\n",
      "Save model 3440\n",
      "Train Epoch:3441 learning rate:1.0000e-05, Loss_tot:0.1800,\n",
      "save model\n",
      "Save model 3441\n",
      "Train Epoch:3442 learning rate:1.0000e-05, Loss_tot:0.1800,\n",
      "Save model 3442\n",
      "Train Epoch:3443 learning rate:1.0000e-05, Loss_tot:0.1800,\n",
      "save model\n",
      "Save model 3443\n",
      "Train Epoch:3444 learning rate:1.0000e-05, Loss_tot:0.1800,\n",
      "save model\n",
      "Save model 3444\n",
      "Train Epoch:3445 learning rate:1.0000e-05, Loss_tot:0.1799,\n",
      "save model\n",
      "Save model 3445\n",
      "Train Epoch:3446 learning rate:1.0000e-05, Loss_tot:0.1798,\n",
      "save model\n",
      "Save model 3446\n",
      "Train Epoch:3447 learning rate:1.0000e-05, Loss_tot:0.1799,\n",
      "Save model 3447\n",
      "Train Epoch:3448 learning rate:1.0000e-05, Loss_tot:0.1799,\n",
      "Save model 3448\n",
      "Train Epoch:3449 learning rate:1.0000e-05, Loss_tot:0.1798,\n",
      "save model\n",
      "Save model 3449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3450 learning rate:1.0000e-05, Loss_tot:0.1796,\n",
      "save model\n",
      "Save model 3450\n",
      "Train Epoch:3451 learning rate:1.0000e-05, Loss_tot:0.1796,\n",
      "save model\n",
      "Save model 3451\n",
      "Train Epoch:3452 learning rate:1.0000e-05, Loss_tot:0.1796,\n",
      "save model\n",
      "Save model 3452\n",
      "Train Epoch:3453 learning rate:1.0000e-05, Loss_tot:0.1795,\n",
      "save model\n",
      "Save model 3453\n",
      "Train Epoch:3454 learning rate:1.0000e-05, Loss_tot:0.1795,\n",
      "save model\n",
      "Save model 3454\n",
      "Train Epoch:3455 learning rate:1.0000e-05, Loss_tot:0.1795,\n",
      "Save model 3455\n",
      "Train Epoch:3456 learning rate:1.0000e-05, Loss_tot:0.1794,\n",
      "save model\n",
      "Save model 3456\n",
      "Train Epoch:3457 learning rate:1.0000e-05, Loss_tot:0.1793,\n",
      "save model\n",
      "Save model 3457\n",
      "Train Epoch:3458 learning rate:1.0000e-05, Loss_tot:0.1794,\n",
      "Save model 3458\n",
      "Train Epoch:3459 learning rate:1.0000e-05, Loss_tot:0.1794,\n",
      "Save model 3459\n",
      "Train Epoch:3460 learning rate:1.0000e-05, Loss_tot:0.1793,\n",
      "Save model 3460\n",
      "Train Epoch:3461 learning rate:1.0000e-05, Loss_tot:0.1793,\n",
      "save model\n",
      "Save model 3461\n",
      "Train Epoch:3462 learning rate:1.0000e-05, Loss_tot:0.1792,\n",
      "save model\n",
      "Save model 3462\n",
      "Train Epoch:3463 learning rate:1.0000e-05, Loss_tot:0.1791,\n",
      "save model\n",
      "Save model 3463\n",
      "Train Epoch:3464 learning rate:1.0000e-05, Loss_tot:0.1790,\n",
      "save model\n",
      "Save model 3464\n",
      "Train Epoch:3465 learning rate:1.0000e-05, Loss_tot:0.1790,\n",
      "save model\n",
      "Save model 3465\n",
      "Train Epoch:3466 learning rate:1.0000e-05, Loss_tot:0.1790,\n",
      "save model\n",
      "Save model 3466\n",
      "Train Epoch:3467 learning rate:1.0000e-05, Loss_tot:0.1789,\n",
      "save model\n",
      "Save model 3467\n",
      "Train Epoch:3468 learning rate:1.0000e-05, Loss_tot:0.1789,\n",
      "save model\n",
      "Save model 3468\n",
      "Train Epoch:3469 learning rate:1.0000e-05, Loss_tot:0.1788,\n",
      "save model\n",
      "Save model 3469\n",
      "Train Epoch:3470 learning rate:1.0000e-05, Loss_tot:0.1788,\n",
      "save model\n",
      "Save model 3470\n",
      "Train Epoch:3471 learning rate:1.0000e-05, Loss_tot:0.1787,\n",
      "save model\n",
      "Save model 3471\n",
      "Train Epoch:3472 learning rate:1.0000e-05, Loss_tot:0.1788,\n",
      "Save model 3472\n",
      "Train Epoch:3473 learning rate:1.0000e-05, Loss_tot:0.1787,\n",
      "save model\n",
      "Save model 3473\n",
      "Train Epoch:3474 learning rate:1.0000e-05, Loss_tot:0.1786,\n",
      "save model\n",
      "Save model 3474\n",
      "Train Epoch:3475 learning rate:1.0000e-05, Loss_tot:0.1786,\n",
      "save model\n",
      "Save model 3475\n",
      "Train Epoch:3476 learning rate:1.0000e-05, Loss_tot:0.1785,\n",
      "save model\n",
      "Save model 3476\n",
      "Train Epoch:3477 learning rate:1.0000e-05, Loss_tot:0.1785,\n",
      "save model\n",
      "Save model 3477\n",
      "Train Epoch:3478 learning rate:1.0000e-05, Loss_tot:0.1785,\n",
      "Save model 3478\n",
      "Train Epoch:3479 learning rate:1.0000e-05, Loss_tot:0.1785,\n",
      "Save model 3479\n",
      "Train Epoch:3480 learning rate:1.0000e-05, Loss_tot:0.1783,\n",
      "save model\n",
      "Save model 3480\n",
      "Train Epoch:3481 learning rate:1.0000e-05, Loss_tot:0.1784,\n",
      "Save model 3481\n",
      "Train Epoch:3482 learning rate:1.0000e-05, Loss_tot:0.1784,\n",
      "Save model 3482\n",
      "Train Epoch:3483 learning rate:1.0000e-05, Loss_tot:0.1784,\n",
      "Save model 3483\n",
      "Train Epoch:3484 learning rate:1.0000e-05, Loss_tot:0.1783,\n",
      "save model\n",
      "Save model 3484\n",
      "Train Epoch:3485 learning rate:1.0000e-05, Loss_tot:0.1783,\n",
      "save model\n",
      "Save model 3485\n",
      "Train Epoch:3486 learning rate:1.0000e-05, Loss_tot:0.1782,\n",
      "save model\n",
      "Save model 3486\n",
      "Train Epoch:3487 learning rate:1.0000e-05, Loss_tot:0.1781,\n",
      "save model\n",
      "Save model 3487\n",
      "Train Epoch:3488 learning rate:1.0000e-05, Loss_tot:0.1781,\n",
      "Save model 3488\n",
      "Train Epoch:3489 learning rate:1.0000e-05, Loss_tot:0.1782,\n",
      "Save model 3489\n",
      "Train Epoch:3490 learning rate:1.0000e-05, Loss_tot:0.1781,\n",
      "Save model 3490\n",
      "Train Epoch:3491 learning rate:1.0000e-05, Loss_tot:0.1779,\n",
      "save model\n",
      "Save model 3491\n",
      "Train Epoch:3492 learning rate:1.0000e-05, Loss_tot:0.1779,\n",
      "save model\n",
      "Save model 3492\n",
      "Train Epoch:3493 learning rate:1.0000e-05, Loss_tot:0.1779,\n",
      "Save model 3493\n",
      "Train Epoch:3494 learning rate:1.0000e-05, Loss_tot:0.1779,\n",
      "Save model 3494\n",
      "Train Epoch:3495 learning rate:1.0000e-05, Loss_tot:0.1779,\n",
      "save model\n",
      "Save model 3495\n",
      "Train Epoch:3496 learning rate:1.0000e-05, Loss_tot:0.1778,\n",
      "save model\n",
      "Save model 3496\n",
      "Train Epoch:3497 learning rate:1.0000e-05, Loss_tot:0.1778,\n",
      "save model\n",
      "Save model 3497\n",
      "Train Epoch:3498 learning rate:1.0000e-05, Loss_tot:0.1777,\n",
      "save model\n",
      "Save model 3498\n",
      "Train Epoch:3499 learning rate:1.0000e-05, Loss_tot:0.1776,\n",
      "save model\n",
      "Save model 3499\n",
      "Train Epoch:3500 learning rate:1.0000e-05, Loss_tot:0.1776,\n",
      "Save model 3500\n",
      "Train Epoch:3501 learning rate:1.0000e-05, Loss_tot:0.1775,\n",
      "save model\n",
      "Save model 3501\n",
      "Train Epoch:3502 learning rate:1.0000e-05, Loss_tot:0.1775,\n",
      "save model\n",
      "Save model 3502\n",
      "Train Epoch:3503 learning rate:1.0000e-05, Loss_tot:0.1775,\n",
      "save model\n",
      "Save model 3503\n",
      "Train Epoch:3504 learning rate:1.0000e-05, Loss_tot:0.1774,\n",
      "save model\n",
      "Save model 3504\n",
      "Train Epoch:3505 learning rate:1.0000e-05, Loss_tot:0.1774,\n",
      "save model\n",
      "Save model 3505\n",
      "Train Epoch:3506 learning rate:1.0000e-05, Loss_tot:0.1773,\n",
      "save model\n",
      "Save model 3506\n",
      "Train Epoch:3507 learning rate:1.0000e-05, Loss_tot:0.1773,\n",
      "save model\n",
      "Save model 3507\n",
      "Train Epoch:3508 learning rate:1.0000e-05, Loss_tot:0.1773,\n",
      "save model\n",
      "Save model 3508\n",
      "Train Epoch:3509 learning rate:1.0000e-05, Loss_tot:0.1771,\n",
      "save model\n",
      "Save model 3509\n",
      "Train Epoch:3510 learning rate:1.0000e-05, Loss_tot:0.1772,\n",
      "Save model 3510\n",
      "Train Epoch:3511 learning rate:1.0000e-05, Loss_tot:0.1772,\n",
      "Save model 3511\n",
      "Train Epoch:3512 learning rate:1.0000e-05, Loss_tot:0.1771,\n",
      "Save model 3512\n",
      "Train Epoch:3513 learning rate:1.0000e-05, Loss_tot:0.1771,\n",
      "save model\n",
      "Save model 3513\n",
      "Train Epoch:3514 learning rate:1.0000e-05, Loss_tot:0.1770,\n",
      "save model\n",
      "Save model 3514\n",
      "Train Epoch:3515 learning rate:1.0000e-05, Loss_tot:0.1769,\n",
      "save model\n",
      "Save model 3515\n",
      "Train Epoch:3516 learning rate:1.0000e-05, Loss_tot:0.1769,\n",
      "save model\n",
      "Save model 3516\n",
      "Train Epoch:3517 learning rate:1.0000e-05, Loss_tot:0.1769,\n",
      "save model\n",
      "Save model 3517\n",
      "Train Epoch:3518 learning rate:1.0000e-05, Loss_tot:0.1768,\n",
      "save model\n",
      "Save model 3518\n",
      "Train Epoch:3519 learning rate:1.0000e-05, Loss_tot:0.1768,\n",
      "save model\n",
      "Save model 3519\n",
      "Train Epoch:3520 learning rate:1.0000e-05, Loss_tot:0.1768,\n",
      "save model\n",
      "Save model 3520\n",
      "Train Epoch:3521 learning rate:1.0000e-05, Loss_tot:0.1767,\n",
      "save model\n",
      "Save model 3521\n",
      "Train Epoch:3522 learning rate:1.0000e-05, Loss_tot:0.1767,\n",
      "save model\n",
      "Save model 3522\n",
      "Train Epoch:3523 learning rate:1.0000e-05, Loss_tot:0.1766,\n",
      "save model\n",
      "Save model 3523\n",
      "Train Epoch:3524 learning rate:1.0000e-05, Loss_tot:0.1765,\n",
      "save model\n",
      "Save model 3524\n",
      "Train Epoch:3525 learning rate:1.0000e-05, Loss_tot:0.1765,\n",
      "save model\n",
      "Save model 3525\n",
      "Train Epoch:3526 learning rate:1.0000e-05, Loss_tot:0.1765,\n",
      "save model\n",
      "Save model 3526\n",
      "Train Epoch:3527 learning rate:1.0000e-05, Loss_tot:0.1764,\n",
      "save model\n",
      "Save model 3527\n",
      "Train Epoch:3528 learning rate:1.0000e-05, Loss_tot:0.1764,\n",
      "save model\n",
      "Save model 3528\n",
      "Train Epoch:3529 learning rate:1.0000e-05, Loss_tot:0.1764,\n",
      "save model\n",
      "Save model 3529\n",
      "Train Epoch:3530 learning rate:1.0000e-05, Loss_tot:0.1763,\n",
      "save model\n",
      "Save model 3530\n",
      "Train Epoch:3531 learning rate:1.0000e-05, Loss_tot:0.1763,\n",
      "save model\n",
      "Save model 3531\n",
      "Train Epoch:3532 learning rate:1.0000e-05, Loss_tot:0.1763,\n",
      "save model\n",
      "Save model 3532\n",
      "Train Epoch:3533 learning rate:1.0000e-05, Loss_tot:0.1762,\n",
      "save model\n",
      "Save model 3533\n",
      "Train Epoch:3534 learning rate:1.0000e-05, Loss_tot:0.1762,\n",
      "save model\n",
      "Save model 3534\n",
      "Train Epoch:3535 learning rate:1.0000e-05, Loss_tot:0.1761,\n",
      "save model\n",
      "Save model 3535\n",
      "Train Epoch:3536 learning rate:1.0000e-05, Loss_tot:0.1760,\n",
      "save model\n",
      "Save model 3536\n",
      "Train Epoch:3537 learning rate:1.0000e-05, Loss_tot:0.1760,\n",
      "save model\n",
      "Save model 3537\n",
      "Train Epoch:3538 learning rate:1.0000e-05, Loss_tot:0.1760,\n",
      "save model\n",
      "Save model 3538\n",
      "Train Epoch:3539 learning rate:1.0000e-05, Loss_tot:0.1759,\n",
      "save model\n",
      "Save model 3539\n",
      "Train Epoch:3540 learning rate:1.0000e-05, Loss_tot:0.1759,\n",
      "save model\n",
      "Save model 3540\n",
      "Train Epoch:3541 learning rate:1.0000e-05, Loss_tot:0.1758,\n",
      "save model\n",
      "Save model 3541\n",
      "Train Epoch:3542 learning rate:1.0000e-05, Loss_tot:0.1758,\n",
      "save model\n",
      "Save model 3542\n",
      "Train Epoch:3543 learning rate:1.0000e-05, Loss_tot:0.1758,\n",
      "Save model 3543\n",
      "Train Epoch:3544 learning rate:1.0000e-05, Loss_tot:0.1757,\n",
      "save model\n",
      "Save model 3544\n",
      "Train Epoch:3545 learning rate:1.0000e-05, Loss_tot:0.1757,\n",
      "Save model 3545\n",
      "Train Epoch:3546 learning rate:1.0000e-05, Loss_tot:0.1757,\n",
      "Save model 3546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3547 learning rate:1.0000e-05, Loss_tot:0.1757,\n",
      "save model\n",
      "Save model 3547\n",
      "Train Epoch:3548 learning rate:1.0000e-05, Loss_tot:0.1756,\n",
      "save model\n",
      "Save model 3548\n",
      "Train Epoch:3549 learning rate:1.0000e-05, Loss_tot:0.1756,\n",
      "save model\n",
      "Save model 3549\n",
      "Train Epoch:3550 learning rate:1.0000e-05, Loss_tot:0.1756,\n",
      "save model\n",
      "Save model 3550\n",
      "Train Epoch:3551 learning rate:1.0000e-05, Loss_tot:0.1755,\n",
      "save model\n",
      "Save model 3551\n",
      "Train Epoch:3552 learning rate:1.0000e-05, Loss_tot:0.1754,\n",
      "save model\n",
      "Save model 3552\n",
      "Train Epoch:3553 learning rate:1.0000e-05, Loss_tot:0.1755,\n",
      "Save model 3553\n",
      "Train Epoch:3554 learning rate:1.0000e-05, Loss_tot:0.1755,\n",
      "Save model 3554\n",
      "Train Epoch:3555 learning rate:1.0000e-05, Loss_tot:0.1754,\n",
      "Save model 3555\n",
      "Train Epoch:3556 learning rate:1.0000e-05, Loss_tot:0.1754,\n",
      "save model\n",
      "Save model 3556\n",
      "Train Epoch:3557 learning rate:1.0000e-05, Loss_tot:0.1753,\n",
      "save model\n",
      "Save model 3557\n",
      "Train Epoch:3558 learning rate:1.0000e-05, Loss_tot:0.1752,\n",
      "save model\n",
      "Save model 3558\n",
      "Train Epoch:3559 learning rate:1.0000e-05, Loss_tot:0.1752,\n",
      "Save model 3559\n",
      "Train Epoch:3560 learning rate:1.0000e-05, Loss_tot:0.1752,\n",
      "Save model 3560\n",
      "Train Epoch:3561 learning rate:1.0000e-05, Loss_tot:0.1752,\n",
      "save model\n",
      "Save model 3561\n",
      "Train Epoch:3562 learning rate:1.0000e-05, Loss_tot:0.1751,\n",
      "save model\n",
      "Save model 3562\n",
      "Train Epoch:3563 learning rate:1.0000e-05, Loss_tot:0.1751,\n",
      "Save model 3563\n",
      "Train Epoch:3564 learning rate:1.0000e-05, Loss_tot:0.1751,\n",
      "save model\n",
      "Save model 3564\n",
      "Train Epoch:3565 learning rate:1.0000e-05, Loss_tot:0.1750,\n",
      "save model\n",
      "Save model 3565\n",
      "Train Epoch:3566 learning rate:1.0000e-05, Loss_tot:0.1749,\n",
      "save model\n",
      "Save model 3566\n",
      "Train Epoch:3567 learning rate:1.0000e-05, Loss_tot:0.1749,\n",
      "save model\n",
      "Save model 3567\n",
      "Train Epoch:3568 learning rate:1.0000e-05, Loss_tot:0.1749,\n",
      "save model\n",
      "Save model 3568\n",
      "Train Epoch:3569 learning rate:1.0000e-05, Loss_tot:0.1748,\n",
      "save model\n",
      "Save model 3569\n",
      "Train Epoch:3570 learning rate:1.0000e-05, Loss_tot:0.1749,\n",
      "Save model 3570\n",
      "Train Epoch:3571 learning rate:1.0000e-05, Loss_tot:0.1749,\n",
      "Save model 3571\n",
      "Train Epoch:3572 learning rate:1.0000e-05, Loss_tot:0.1748,\n",
      "Save model 3572\n",
      "Train Epoch:3573 learning rate:1.0000e-05, Loss_tot:0.1748,\n",
      "save model\n",
      "Save model 3573\n",
      "Train Epoch:3574 learning rate:1.0000e-05, Loss_tot:0.1747,\n",
      "save model\n",
      "Save model 3574\n",
      "Train Epoch:3575 learning rate:1.0000e-05, Loss_tot:0.1747,\n",
      "save model\n",
      "Save model 3575\n",
      "Train Epoch:3576 learning rate:1.0000e-05, Loss_tot:0.1746,\n",
      "save model\n",
      "Save model 3576\n",
      "Train Epoch:3577 learning rate:1.0000e-05, Loss_tot:0.1746,\n",
      "Save model 3577\n",
      "Train Epoch:3578 learning rate:1.0000e-05, Loss_tot:0.1745,\n",
      "save model\n",
      "Save model 3578\n",
      "Train Epoch:3579 learning rate:1.0000e-05, Loss_tot:0.1745,\n",
      "Save model 3579\n",
      "Train Epoch:3580 learning rate:1.0000e-05, Loss_tot:0.1745,\n",
      "Save model 3580\n",
      "Train Epoch:3581 learning rate:1.0000e-05, Loss_tot:0.1745,\n",
      "save model\n",
      "Save model 3581\n",
      "Train Epoch:3582 learning rate:1.0000e-05, Loss_tot:0.1745,\n",
      "save model\n",
      "Save model 3582\n",
      "Train Epoch:3583 learning rate:1.0000e-05, Loss_tot:0.1744,\n",
      "save model\n",
      "Save model 3583\n",
      "Train Epoch:3584 learning rate:1.0000e-05, Loss_tot:0.1743,\n",
      "save model\n",
      "Save model 3584\n",
      "Train Epoch:3585 learning rate:1.0000e-05, Loss_tot:0.1743,\n",
      "save model\n",
      "Save model 3585\n",
      "Train Epoch:3586 learning rate:1.0000e-05, Loss_tot:0.1743,\n",
      "save model\n",
      "Save model 3586\n",
      "Train Epoch:3587 learning rate:1.0000e-05, Loss_tot:0.1742,\n",
      "save model\n",
      "Save model 3587\n",
      "Train Epoch:3588 learning rate:1.0000e-05, Loss_tot:0.1742,\n",
      "save model\n",
      "Save model 3588\n",
      "Train Epoch:3589 learning rate:1.0000e-05, Loss_tot:0.1741,\n",
      "save model\n",
      "Save model 3589\n",
      "Train Epoch:3590 learning rate:1.0000e-05, Loss_tot:0.1741,\n",
      "save model\n",
      "Save model 3590\n",
      "Train Epoch:3591 learning rate:1.0000e-05, Loss_tot:0.1741,\n",
      "save model\n",
      "Save model 3591\n",
      "Train Epoch:3592 learning rate:1.0000e-05, Loss_tot:0.1740,\n",
      "save model\n",
      "Save model 3592\n",
      "Train Epoch:3593 learning rate:1.0000e-05, Loss_tot:0.1740,\n",
      "save model\n",
      "Save model 3593\n",
      "Train Epoch:3594 learning rate:1.0000e-05, Loss_tot:0.1740,\n",
      "save model\n",
      "Save model 3594\n",
      "Train Epoch:3595 learning rate:1.0000e-05, Loss_tot:0.1740,\n",
      "Save model 3595\n",
      "Train Epoch:3596 learning rate:1.0000e-05, Loss_tot:0.1739,\n",
      "save model\n",
      "Save model 3596\n",
      "Train Epoch:3597 learning rate:1.0000e-05, Loss_tot:0.1739,\n",
      "save model\n",
      "Save model 3597\n",
      "Train Epoch:3598 learning rate:1.0000e-05, Loss_tot:0.1739,\n",
      "save model\n",
      "Save model 3598\n",
      "Train Epoch:3599 learning rate:1.0000e-05, Loss_tot:0.1738,\n",
      "save model\n",
      "Save model 3599\n",
      "Train Epoch:3600 learning rate:1.0000e-05, Loss_tot:0.1738,\n",
      "save model\n",
      "Save model 3600\n",
      "Train Epoch:3601 learning rate:1.0000e-05, Loss_tot:0.1737,\n",
      "save model\n",
      "Save model 3601\n",
      "Train Epoch:3602 learning rate:1.0000e-05, Loss_tot:0.1737,\n",
      "save model\n",
      "Save model 3602\n",
      "Train Epoch:3603 learning rate:1.0000e-05, Loss_tot:0.1737,\n",
      "save model\n",
      "Save model 3603\n",
      "Train Epoch:3604 learning rate:1.0000e-05, Loss_tot:0.1736,\n",
      "save model\n",
      "Save model 3604\n",
      "Train Epoch:3605 learning rate:1.0000e-05, Loss_tot:0.1736,\n",
      "save model\n",
      "Save model 3605\n",
      "Train Epoch:3606 learning rate:1.0000e-05, Loss_tot:0.1735,\n",
      "save model\n",
      "Save model 3606\n",
      "Train Epoch:3607 learning rate:1.0000e-05, Loss_tot:0.1736,\n",
      "Save model 3607\n",
      "Train Epoch:3608 learning rate:1.0000e-05, Loss_tot:0.1735,\n",
      "Save model 3608\n",
      "Train Epoch:3609 learning rate:1.0000e-05, Loss_tot:0.1734,\n",
      "save model\n",
      "Save model 3609\n",
      "Train Epoch:3610 learning rate:1.0000e-05, Loss_tot:0.1735,\n",
      "Save model 3610\n",
      "Train Epoch:3611 learning rate:1.0000e-05, Loss_tot:0.1735,\n",
      "Save model 3611\n",
      "Train Epoch:3612 learning rate:1.0000e-05, Loss_tot:0.1735,\n",
      "Save model 3612\n",
      "Train Epoch:3613 learning rate:1.0000e-05, Loss_tot:0.1734,\n",
      "save model\n",
      "Save model 3613\n",
      "Train Epoch:3614 learning rate:1.0000e-05, Loss_tot:0.1734,\n",
      "save model\n",
      "Save model 3614\n",
      "Train Epoch:3615 learning rate:1.0000e-05, Loss_tot:0.1733,\n",
      "save model\n",
      "Save model 3615\n",
      "Train Epoch:3616 learning rate:1.0000e-05, Loss_tot:0.1732,\n",
      "save model\n",
      "Save model 3616\n",
      "Train Epoch:3617 learning rate:1.0000e-05, Loss_tot:0.1733,\n",
      "Save model 3617\n",
      "Train Epoch:3618 learning rate:1.0000e-05, Loss_tot:0.1733,\n",
      "Save model 3618\n",
      "Train Epoch:3619 learning rate:1.0000e-05, Loss_tot:0.1733,\n",
      "Save model 3619\n",
      "Train Epoch:3620 learning rate:1.0000e-05, Loss_tot:0.1731,\n",
      "save model\n",
      "Save model 3620\n",
      "Train Epoch:3621 learning rate:1.0000e-05, Loss_tot:0.1731,\n",
      "save model\n",
      "Save model 3621\n",
      "Train Epoch:3622 learning rate:1.0000e-05, Loss_tot:0.1731,\n",
      "Save model 3622\n",
      "Train Epoch:3623 learning rate:1.0000e-05, Loss_tot:0.1731,\n",
      "Save model 3623\n",
      "Train Epoch:3624 learning rate:1.0000e-05, Loss_tot:0.1731,\n",
      "save model\n",
      "Save model 3624\n",
      "Train Epoch:3625 learning rate:1.0000e-05, Loss_tot:0.1730,\n",
      "save model\n",
      "Save model 3625\n",
      "Train Epoch:3626 learning rate:1.0000e-05, Loss_tot:0.1729,\n",
      "save model\n",
      "Save model 3626\n",
      "Train Epoch:3627 learning rate:1.0000e-05, Loss_tot:0.1728,\n",
      "save model\n",
      "Save model 3627\n",
      "Train Epoch:3628 learning rate:1.0000e-05, Loss_tot:0.1728,\n",
      "Save model 3628\n",
      "Train Epoch:3629 learning rate:1.0000e-05, Loss_tot:0.1729,\n",
      "Save model 3629\n",
      "Train Epoch:3630 learning rate:1.0000e-05, Loss_tot:0.1728,\n",
      "save model\n",
      "Save model 3630\n",
      "Train Epoch:3631 learning rate:1.0000e-05, Loss_tot:0.1727,\n",
      "save model\n",
      "Save model 3631\n",
      "Train Epoch:3632 learning rate:1.0000e-05, Loss_tot:0.1727,\n",
      "save model\n",
      "Save model 3632\n",
      "Train Epoch:3633 learning rate:1.0000e-05, Loss_tot:0.1726,\n",
      "save model\n",
      "Save model 3633\n",
      "Train Epoch:3634 learning rate:1.0000e-05, Loss_tot:0.1726,\n",
      "save model\n",
      "Save model 3634\n",
      "Train Epoch:3635 learning rate:1.0000e-05, Loss_tot:0.1725,\n",
      "save model\n",
      "Save model 3635\n",
      "Train Epoch:3636 learning rate:1.0000e-05, Loss_tot:0.1725,\n",
      "save model\n",
      "Save model 3636\n",
      "Train Epoch:3637 learning rate:1.0000e-05, Loss_tot:0.1725,\n",
      "save model\n",
      "Save model 3637\n",
      "Train Epoch:3638 learning rate:1.0000e-05, Loss_tot:0.1724,\n",
      "save model\n",
      "Save model 3638\n",
      "Train Epoch:3639 learning rate:1.0000e-05, Loss_tot:0.1724,\n",
      "save model\n",
      "Save model 3639\n",
      "Train Epoch:3640 learning rate:1.0000e-05, Loss_tot:0.1724,\n",
      "save model\n",
      "Save model 3640\n",
      "Train Epoch:3641 learning rate:1.0000e-05, Loss_tot:0.1723,\n",
      "save model\n",
      "Save model 3641\n",
      "Train Epoch:3642 learning rate:1.0000e-05, Loss_tot:0.1723,\n",
      "save model\n",
      "Save model 3642\n",
      "Train Epoch:3643 learning rate:1.0000e-05, Loss_tot:0.1723,\n",
      "save model\n",
      "Save model 3643\n",
      "Train Epoch:3644 learning rate:1.0000e-05, Loss_tot:0.1722,\n",
      "save model\n",
      "Save model 3644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3645 learning rate:1.0000e-05, Loss_tot:0.1722,\n",
      "save model\n",
      "Save model 3645\n",
      "Train Epoch:3646 learning rate:1.0000e-05, Loss_tot:0.1722,\n",
      "save model\n",
      "Save model 3646\n",
      "Train Epoch:3647 learning rate:1.0000e-05, Loss_tot:0.1721,\n",
      "save model\n",
      "Save model 3647\n",
      "Train Epoch:3648 learning rate:1.0000e-05, Loss_tot:0.1724,\n",
      "Save model 3648\n",
      "Train Epoch:3649 learning rate:1.0000e-05, Loss_tot:0.1724,\n",
      "Save model 3649\n",
      "Train Epoch:3650 learning rate:1.0000e-05, Loss_tot:0.1723,\n",
      "Save model 3650\n",
      "Train Epoch:3651 learning rate:1.0000e-05, Loss_tot:0.1720,\n",
      "save model\n",
      "Save model 3651\n",
      "Train Epoch:3652 learning rate:1.0000e-05, Loss_tot:0.1720,\n",
      "save model\n",
      "Save model 3652\n",
      "Train Epoch:3653 learning rate:1.0000e-05, Loss_tot:0.1720,\n",
      "save model\n",
      "Save model 3653\n",
      "Train Epoch:3654 learning rate:1.0000e-05, Loss_tot:0.1719,\n",
      "save model\n",
      "Save model 3654\n",
      "Train Epoch:3655 learning rate:1.0000e-05, Loss_tot:0.1721,\n",
      "Save model 3655\n",
      "Train Epoch:3656 learning rate:1.0000e-05, Loss_tot:0.1719,\n",
      "save model\n",
      "Save model 3656\n",
      "Train Epoch:3657 learning rate:1.0000e-05, Loss_tot:0.1721,\n",
      "Save model 3657\n",
      "Train Epoch:3658 learning rate:1.0000e-05, Loss_tot:0.1721,\n",
      "Save model 3658\n",
      "Train Epoch:3659 learning rate:1.0000e-05, Loss_tot:0.1718,\n",
      "save model\n",
      "Save model 3659\n",
      "Train Epoch:3660 learning rate:1.0000e-05, Loss_tot:0.1718,\n",
      "save model\n",
      "Save model 3660\n",
      "Train Epoch:3661 learning rate:1.0000e-05, Loss_tot:0.1717,\n",
      "save model\n",
      "Save model 3661\n",
      "Train Epoch:3662 learning rate:1.0000e-05, Loss_tot:0.1716,\n",
      "save model\n",
      "Save model 3662\n",
      "Train Epoch:3663 learning rate:1.0000e-05, Loss_tot:0.1716,\n",
      "Save model 3663\n",
      "Train Epoch:3664 learning rate:1.0000e-05, Loss_tot:0.1716,\n",
      "save model\n",
      "Save model 3664\n",
      "Train Epoch:3665 learning rate:1.0000e-05, Loss_tot:0.1715,\n",
      "save model\n",
      "Save model 3665\n",
      "Train Epoch:3666 learning rate:1.0000e-05, Loss_tot:0.1715,\n",
      "Save model 3666\n",
      "Train Epoch:3667 learning rate:1.0000e-05, Loss_tot:0.1715,\n",
      "save model\n",
      "Save model 3667\n",
      "Train Epoch:3668 learning rate:1.0000e-05, Loss_tot:0.1715,\n",
      "save model\n",
      "Save model 3668\n",
      "Train Epoch:3669 learning rate:1.0000e-05, Loss_tot:0.1714,\n",
      "save model\n",
      "Save model 3669\n",
      "Train Epoch:3670 learning rate:1.0000e-05, Loss_tot:0.1713,\n",
      "save model\n",
      "Save model 3670\n",
      "Train Epoch:3671 learning rate:1.0000e-05, Loss_tot:0.1714,\n",
      "Save model 3671\n",
      "Train Epoch:3672 learning rate:1.0000e-05, Loss_tot:0.1714,\n",
      "Save model 3672\n",
      "Train Epoch:3673 learning rate:1.0000e-05, Loss_tot:0.1713,\n",
      "Save model 3673\n",
      "Train Epoch:3674 learning rate:1.0000e-05, Loss_tot:0.1712,\n",
      "save model\n",
      "Save model 3674\n",
      "Train Epoch:3675 learning rate:1.0000e-05, Loss_tot:0.1712,\n",
      "Save model 3675\n",
      "Train Epoch:3676 learning rate:1.0000e-05, Loss_tot:0.1713,\n",
      "Save model 3676\n",
      "Train Epoch:3677 learning rate:1.0000e-05, Loss_tot:0.1713,\n",
      "Save model 3677\n",
      "Train Epoch:3678 learning rate:1.0000e-05, Loss_tot:0.1712,\n",
      "Save model 3678\n",
      "Train Epoch:3679 learning rate:1.0000e-05, Loss_tot:0.1712,\n",
      "Save model 3679\n",
      "Train Epoch:3680 learning rate:1.0000e-05, Loss_tot:0.1711,\n",
      "save model\n",
      "Save model 3680\n",
      "Train Epoch:3681 learning rate:1.0000e-05, Loss_tot:0.1710,\n",
      "save model\n",
      "Save model 3681\n",
      "Train Epoch:3682 learning rate:1.0000e-05, Loss_tot:0.1709,\n",
      "save model\n",
      "Save model 3682\n",
      "Train Epoch:3683 learning rate:1.0000e-05, Loss_tot:0.1709,\n",
      "save model\n",
      "Save model 3683\n",
      "Train Epoch:3684 learning rate:1.0000e-05, Loss_tot:0.1709,\n",
      "Save model 3684\n",
      "Train Epoch:3685 learning rate:1.0000e-05, Loss_tot:0.1709,\n",
      "save model\n",
      "Save model 3685\n",
      "Train Epoch:3686 learning rate:1.0000e-05, Loss_tot:0.1707,\n",
      "save model\n",
      "Save model 3686\n",
      "Train Epoch:3687 learning rate:1.0000e-05, Loss_tot:0.1707,\n",
      "Save model 3687\n",
      "Train Epoch:3688 learning rate:1.0000e-05, Loss_tot:0.1708,\n",
      "Save model 3688\n",
      "Train Epoch:3689 learning rate:1.0000e-05, Loss_tot:0.1708,\n",
      "Save model 3689\n",
      "Train Epoch:3690 learning rate:1.0000e-05, Loss_tot:0.1707,\n",
      "Save model 3690\n",
      "Train Epoch:3691 learning rate:1.0000e-05, Loss_tot:0.1707,\n",
      "save model\n",
      "Save model 3691\n",
      "Train Epoch:3692 learning rate:1.0000e-05, Loss_tot:0.1706,\n",
      "save model\n",
      "Save model 3692\n",
      "Train Epoch:3693 learning rate:1.0000e-05, Loss_tot:0.1705,\n",
      "save model\n",
      "Save model 3693\n",
      "Train Epoch:3694 learning rate:1.0000e-05, Loss_tot:0.1704,\n",
      "save model\n",
      "Save model 3694\n",
      "Train Epoch:3695 learning rate:1.0000e-05, Loss_tot:0.1705,\n",
      "Save model 3695\n",
      "Train Epoch:3696 learning rate:1.0000e-05, Loss_tot:0.1704,\n",
      "save model\n",
      "Save model 3696\n",
      "Train Epoch:3697 learning rate:1.0000e-05, Loss_tot:0.1703,\n",
      "save model\n",
      "Save model 3697\n",
      "Train Epoch:3698 learning rate:1.0000e-05, Loss_tot:0.1703,\n",
      "save model\n",
      "Save model 3698\n",
      "Train Epoch:3699 learning rate:1.0000e-05, Loss_tot:0.1703,\n",
      "save model\n",
      "Save model 3699\n",
      "Train Epoch:3700 learning rate:1.0000e-05, Loss_tot:0.1702,\n",
      "save model\n",
      "Save model 3700\n",
      "Train Epoch:3701 learning rate:1.0000e-05, Loss_tot:0.1701,\n",
      "save model\n",
      "Save model 3701\n",
      "Train Epoch:3702 learning rate:1.0000e-05, Loss_tot:0.1702,\n",
      "Save model 3702\n",
      "Train Epoch:3703 learning rate:1.0000e-05, Loss_tot:0.1702,\n",
      "Save model 3703\n",
      "Train Epoch:3704 learning rate:1.0000e-05, Loss_tot:0.1700,\n",
      "save model\n",
      "Save model 3704\n",
      "Train Epoch:3705 learning rate:1.0000e-05, Loss_tot:0.1700,\n",
      "save model\n",
      "Save model 3705\n",
      "Train Epoch:3706 learning rate:1.0000e-05, Loss_tot:0.1700,\n",
      "Save model 3706\n",
      "Train Epoch:3707 learning rate:1.0000e-05, Loss_tot:0.1700,\n",
      "save model\n",
      "Save model 3707\n",
      "Train Epoch:3708 learning rate:1.0000e-05, Loss_tot:0.1700,\n",
      "save model\n",
      "Save model 3708\n",
      "Train Epoch:3709 learning rate:1.0000e-05, Loss_tot:0.1699,\n",
      "save model\n",
      "Save model 3709\n",
      "Train Epoch:3710 learning rate:1.0000e-05, Loss_tot:0.1698,\n",
      "save model\n",
      "Save model 3710\n",
      "Train Epoch:3711 learning rate:1.0000e-05, Loss_tot:0.1698,\n",
      "Save model 3711\n",
      "Train Epoch:3712 learning rate:1.0000e-05, Loss_tot:0.1698,\n",
      "Save model 3712\n",
      "Train Epoch:3713 learning rate:1.0000e-05, Loss_tot:0.1697,\n",
      "save model\n",
      "Save model 3713\n",
      "Train Epoch:3714 learning rate:1.0000e-05, Loss_tot:0.1696,\n",
      "save model\n",
      "Save model 3714\n",
      "Train Epoch:3715 learning rate:1.0000e-05, Loss_tot:0.1696,\n",
      "save model\n",
      "Save model 3715\n",
      "Train Epoch:3716 learning rate:1.0000e-05, Loss_tot:0.1696,\n",
      "save model\n",
      "Save model 3716\n",
      "Train Epoch:3717 learning rate:1.0000e-05, Loss_tot:0.1696,\n",
      "save model\n",
      "Save model 3717\n",
      "Train Epoch:3718 learning rate:1.0000e-05, Loss_tot:0.1695,\n",
      "save model\n",
      "Save model 3718\n",
      "Train Epoch:3719 learning rate:1.0000e-05, Loss_tot:0.1695,\n",
      "save model\n",
      "Save model 3719\n",
      "Train Epoch:3720 learning rate:1.0000e-05, Loss_tot:0.1695,\n",
      "save model\n",
      "Save model 3720\n",
      "Train Epoch:3721 learning rate:1.0000e-05, Loss_tot:0.1693,\n",
      "save model\n",
      "Save model 3721\n",
      "Train Epoch:3722 learning rate:1.0000e-05, Loss_tot:0.1693,\n",
      "save model\n",
      "Save model 3722\n",
      "Train Epoch:3723 learning rate:1.0000e-05, Loss_tot:0.1693,\n",
      "save model\n",
      "Save model 3723\n",
      "Train Epoch:3724 learning rate:1.0000e-05, Loss_tot:0.1693,\n",
      "save model\n",
      "Save model 3724\n",
      "Train Epoch:3725 learning rate:1.0000e-05, Loss_tot:0.1692,\n",
      "save model\n",
      "Save model 3725\n",
      "Train Epoch:3726 learning rate:1.0000e-05, Loss_tot:0.1692,\n",
      "save model\n",
      "Save model 3726\n",
      "Train Epoch:3727 learning rate:1.0000e-05, Loss_tot:0.1692,\n",
      "Save model 3727\n",
      "Train Epoch:3728 learning rate:1.0000e-05, Loss_tot:0.1691,\n",
      "save model\n",
      "Save model 3728\n",
      "Train Epoch:3729 learning rate:1.0000e-05, Loss_tot:0.1691,\n",
      "save model\n",
      "Save model 3729\n",
      "Train Epoch:3730 learning rate:1.0000e-05, Loss_tot:0.1691,\n",
      "save model\n",
      "Save model 3730\n",
      "Train Epoch:3731 learning rate:1.0000e-05, Loss_tot:0.1690,\n",
      "save model\n",
      "Save model 3731\n",
      "Train Epoch:3732 learning rate:1.0000e-05, Loss_tot:0.1690,\n",
      "save model\n",
      "Save model 3732\n",
      "Train Epoch:3733 learning rate:1.0000e-05, Loss_tot:0.1690,\n",
      "Save model 3733\n",
      "Train Epoch:3734 learning rate:1.0000e-05, Loss_tot:0.1689,\n",
      "save model\n",
      "Save model 3734\n",
      "Train Epoch:3735 learning rate:1.0000e-05, Loss_tot:0.1688,\n",
      "save model\n",
      "Save model 3735\n",
      "Train Epoch:3736 learning rate:1.0000e-05, Loss_tot:0.1688,\n",
      "save model\n",
      "Save model 3736\n",
      "Train Epoch:3737 learning rate:1.0000e-05, Loss_tot:0.1688,\n",
      "save model\n",
      "Save model 3737\n",
      "Train Epoch:3738 learning rate:1.0000e-05, Loss_tot:0.1687,\n",
      "save model\n",
      "Save model 3738\n",
      "Train Epoch:3739 learning rate:1.0000e-05, Loss_tot:0.1687,\n",
      "save model\n",
      "Save model 3739\n",
      "Train Epoch:3740 learning rate:1.0000e-05, Loss_tot:0.1687,\n",
      "save model\n",
      "Save model 3740\n",
      "Train Epoch:3741 learning rate:1.0000e-05, Loss_tot:0.1686,\n",
      "save model\n",
      "Save model 3741\n",
      "Train Epoch:3742 learning rate:1.0000e-05, Loss_tot:0.1686,\n",
      "save model\n",
      "Save model 3742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3743 learning rate:1.0000e-05, Loss_tot:0.1685,\n",
      "save model\n",
      "Save model 3743\n",
      "Train Epoch:3744 learning rate:1.0000e-05, Loss_tot:0.1685,\n",
      "save model\n",
      "Save model 3744\n",
      "Train Epoch:3745 learning rate:1.0000e-05, Loss_tot:0.1685,\n",
      "save model\n",
      "Save model 3745\n",
      "Train Epoch:3746 learning rate:1.0000e-05, Loss_tot:0.1685,\n",
      "save model\n",
      "Save model 3746\n",
      "Train Epoch:3747 learning rate:1.0000e-05, Loss_tot:0.1684,\n",
      "save model\n",
      "Save model 3747\n",
      "Train Epoch:3748 learning rate:1.0000e-05, Loss_tot:0.1684,\n",
      "save model\n",
      "Save model 3748\n",
      "Train Epoch:3749 learning rate:1.0000e-05, Loss_tot:0.1683,\n",
      "save model\n",
      "Save model 3749\n",
      "Train Epoch:3750 learning rate:1.0000e-05, Loss_tot:0.1683,\n",
      "save model\n",
      "Save model 3750\n",
      "Train Epoch:3751 learning rate:1.0000e-05, Loss_tot:0.1683,\n",
      "save model\n",
      "Save model 3751\n",
      "Train Epoch:3752 learning rate:1.0000e-05, Loss_tot:0.1682,\n",
      "save model\n",
      "Save model 3752\n",
      "Train Epoch:3753 learning rate:1.0000e-05, Loss_tot:0.1682,\n",
      "save model\n",
      "Save model 3753\n",
      "Train Epoch:3754 learning rate:1.0000e-05, Loss_tot:0.1682,\n",
      "save model\n",
      "Save model 3754\n",
      "Train Epoch:3755 learning rate:1.0000e-05, Loss_tot:0.1681,\n",
      "save model\n",
      "Save model 3755\n",
      "Train Epoch:3756 learning rate:1.0000e-05, Loss_tot:0.1681,\n",
      "Save model 3756\n",
      "Train Epoch:3757 learning rate:1.0000e-05, Loss_tot:0.1681,\n",
      "save model\n",
      "Save model 3757\n",
      "Train Epoch:3758 learning rate:1.0000e-05, Loss_tot:0.1680,\n",
      "save model\n",
      "Save model 3758\n",
      "Train Epoch:3759 learning rate:1.0000e-05, Loss_tot:0.1680,\n",
      "save model\n",
      "Save model 3759\n",
      "Train Epoch:3760 learning rate:1.0000e-05, Loss_tot:0.1679,\n",
      "save model\n",
      "Save model 3760\n",
      "Train Epoch:3761 learning rate:1.0000e-05, Loss_tot:0.1679,\n",
      "Save model 3761\n",
      "Train Epoch:3762 learning rate:1.0000e-05, Loss_tot:0.1679,\n",
      "save model\n",
      "Save model 3762\n",
      "Train Epoch:3763 learning rate:1.0000e-05, Loss_tot:0.1678,\n",
      "save model\n",
      "Save model 3763\n",
      "Train Epoch:3764 learning rate:1.0000e-05, Loss_tot:0.1678,\n",
      "save model\n",
      "Save model 3764\n",
      "Train Epoch:3765 learning rate:1.0000e-05, Loss_tot:0.1678,\n",
      "save model\n",
      "Save model 3765\n",
      "Train Epoch:3766 learning rate:1.0000e-05, Loss_tot:0.1677,\n",
      "save model\n",
      "Save model 3766\n",
      "Train Epoch:3767 learning rate:1.0000e-05, Loss_tot:0.1676,\n",
      "save model\n",
      "Save model 3767\n",
      "Train Epoch:3768 learning rate:1.0000e-05, Loss_tot:0.1677,\n",
      "Save model 3768\n",
      "Train Epoch:3769 learning rate:1.0000e-05, Loss_tot:0.1677,\n",
      "Save model 3769\n",
      "Train Epoch:3770 learning rate:1.0000e-05, Loss_tot:0.1676,\n",
      "save model\n",
      "Save model 3770\n",
      "Train Epoch:3771 learning rate:1.0000e-05, Loss_tot:0.1675,\n",
      "save model\n",
      "Save model 3771\n",
      "Train Epoch:3772 learning rate:1.0000e-05, Loss_tot:0.1675,\n",
      "Save model 3772\n",
      "Train Epoch:3773 learning rate:1.0000e-05, Loss_tot:0.1675,\n",
      "save model\n",
      "Save model 3773\n",
      "Train Epoch:3774 learning rate:1.0000e-05, Loss_tot:0.1675,\n",
      "save model\n",
      "Save model 3774\n",
      "Train Epoch:3775 learning rate:1.0000e-05, Loss_tot:0.1674,\n",
      "save model\n",
      "Save model 3775\n",
      "Train Epoch:3776 learning rate:1.0000e-05, Loss_tot:0.1674,\n",
      "save model\n",
      "Save model 3776\n",
      "Train Epoch:3777 learning rate:1.0000e-05, Loss_tot:0.1673,\n",
      "save model\n",
      "Save model 3777\n",
      "Train Epoch:3778 learning rate:1.0000e-05, Loss_tot:0.1673,\n",
      "save model\n",
      "Save model 3778\n",
      "Train Epoch:3779 learning rate:1.0000e-05, Loss_tot:0.1672,\n",
      "save model\n",
      "Save model 3779\n",
      "Train Epoch:3780 learning rate:1.0000e-05, Loss_tot:0.1672,\n",
      "save model\n",
      "Save model 3780\n",
      "Train Epoch:3781 learning rate:1.0000e-05, Loss_tot:0.1672,\n",
      "save model\n",
      "Save model 3781\n",
      "Train Epoch:3782 learning rate:1.0000e-05, Loss_tot:0.1671,\n",
      "save model\n",
      "Save model 3782\n",
      "Train Epoch:3783 learning rate:1.0000e-05, Loss_tot:0.1671,\n",
      "Save model 3783\n",
      "Train Epoch:3784 learning rate:1.0000e-05, Loss_tot:0.1671,\n",
      "save model\n",
      "Save model 3784\n",
      "Train Epoch:3785 learning rate:1.0000e-05, Loss_tot:0.1671,\n",
      "save model\n",
      "Save model 3785\n",
      "Train Epoch:3786 learning rate:1.0000e-05, Loss_tot:0.1670,\n",
      "save model\n",
      "Save model 3786\n",
      "Train Epoch:3787 learning rate:1.0000e-05, Loss_tot:0.1669,\n",
      "save model\n",
      "Save model 3787\n",
      "Train Epoch:3788 learning rate:1.0000e-05, Loss_tot:0.1670,\n",
      "Save model 3788\n",
      "Train Epoch:3789 learning rate:1.0000e-05, Loss_tot:0.1670,\n",
      "Save model 3789\n",
      "Train Epoch:3790 learning rate:1.0000e-05, Loss_tot:0.1669,\n",
      "save model\n",
      "Save model 3790\n",
      "Train Epoch:3791 learning rate:1.0000e-05, Loss_tot:0.1668,\n",
      "save model\n",
      "Save model 3791\n",
      "Train Epoch:3792 learning rate:1.0000e-05, Loss_tot:0.1668,\n",
      "Save model 3792\n",
      "Train Epoch:3793 learning rate:1.0000e-05, Loss_tot:0.1668,\n",
      "save model\n",
      "Save model 3793\n",
      "Train Epoch:3794 learning rate:1.0000e-05, Loss_tot:0.1667,\n",
      "save model\n",
      "Save model 3794\n",
      "Train Epoch:3795 learning rate:1.0000e-05, Loss_tot:0.1666,\n",
      "save model\n",
      "Save model 3795\n",
      "Train Epoch:3796 learning rate:1.0000e-05, Loss_tot:0.1666,\n",
      "save model\n",
      "Save model 3796\n",
      "Train Epoch:3797 learning rate:1.0000e-05, Loss_tot:0.1666,\n",
      "save model\n",
      "Save model 3797\n",
      "Train Epoch:3798 learning rate:1.0000e-05, Loss_tot:0.1665,\n",
      "save model\n",
      "Save model 3798\n",
      "Train Epoch:3799 learning rate:1.0000e-05, Loss_tot:0.1665,\n",
      "save model\n",
      "Save model 3799\n",
      "Train Epoch:3800 learning rate:1.0000e-05, Loss_tot:0.1664,\n",
      "save model\n",
      "Save model 3800\n",
      "Train Epoch:3801 learning rate:1.0000e-05, Loss_tot:0.1664,\n",
      "save model\n",
      "Save model 3801\n",
      "Train Epoch:3802 learning rate:1.0000e-05, Loss_tot:0.1663,\n",
      "save model\n",
      "Save model 3802\n",
      "Train Epoch:3803 learning rate:1.0000e-05, Loss_tot:0.1664,\n",
      "Save model 3803\n",
      "Train Epoch:3804 learning rate:1.0000e-05, Loss_tot:0.1663,\n",
      "Save model 3804\n",
      "Train Epoch:3805 learning rate:1.0000e-05, Loss_tot:0.1663,\n",
      "save model\n",
      "Save model 3805\n",
      "Train Epoch:3806 learning rate:1.0000e-05, Loss_tot:0.1662,\n",
      "save model\n",
      "Save model 3806\n",
      "Train Epoch:3807 learning rate:1.0000e-05, Loss_tot:0.1662,\n",
      "save model\n",
      "Save model 3807\n",
      "Train Epoch:3808 learning rate:1.0000e-05, Loss_tot:0.1662,\n",
      "Save model 3808\n",
      "Train Epoch:3809 learning rate:1.0000e-05, Loss_tot:0.1662,\n",
      "Save model 3809\n",
      "Train Epoch:3810 learning rate:1.0000e-05, Loss_tot:0.1661,\n",
      "save model\n",
      "Save model 3810\n",
      "Train Epoch:3811 learning rate:1.0000e-05, Loss_tot:0.1660,\n",
      "save model\n",
      "Save model 3811\n",
      "Train Epoch:3812 learning rate:1.0000e-05, Loss_tot:0.1660,\n",
      "Save model 3812\n",
      "Train Epoch:3813 learning rate:1.0000e-05, Loss_tot:0.1660,\n",
      "save model\n",
      "Save model 3813\n",
      "Train Epoch:3814 learning rate:1.0000e-05, Loss_tot:0.1660,\n",
      "save model\n",
      "Save model 3814\n",
      "Train Epoch:3815 learning rate:1.0000e-05, Loss_tot:0.1659,\n",
      "save model\n",
      "Save model 3815\n",
      "Train Epoch:3816 learning rate:1.0000e-05, Loss_tot:0.1658,\n",
      "save model\n",
      "Save model 3816\n",
      "Train Epoch:3817 learning rate:1.0000e-05, Loss_tot:0.1658,\n",
      "save model\n",
      "Save model 3817\n",
      "Train Epoch:3818 learning rate:1.0000e-05, Loss_tot:0.1658,\n",
      "save model\n",
      "Save model 3818\n",
      "Train Epoch:3819 learning rate:1.0000e-05, Loss_tot:0.1657,\n",
      "save model\n",
      "Save model 3819\n",
      "Train Epoch:3820 learning rate:1.0000e-05, Loss_tot:0.1657,\n",
      "save model\n",
      "Save model 3820\n",
      "Train Epoch:3821 learning rate:1.0000e-05, Loss_tot:0.1656,\n",
      "save model\n",
      "Save model 3821\n",
      "Train Epoch:3822 learning rate:1.0000e-05, Loss_tot:0.1656,\n",
      "save model\n",
      "Save model 3822\n",
      "Train Epoch:3823 learning rate:1.0000e-05, Loss_tot:0.1656,\n",
      "Save model 3823\n",
      "Train Epoch:3824 learning rate:1.0000e-05, Loss_tot:0.1656,\n",
      "save model\n",
      "Save model 3824\n",
      "Train Epoch:3825 learning rate:1.0000e-05, Loss_tot:0.1655,\n",
      "save model\n",
      "Save model 3825\n",
      "Train Epoch:3826 learning rate:1.0000e-05, Loss_tot:0.1655,\n",
      "save model\n",
      "Save model 3826\n",
      "Train Epoch:3827 learning rate:1.0000e-05, Loss_tot:0.1654,\n",
      "save model\n",
      "Save model 3827\n",
      "Train Epoch:3828 learning rate:1.0000e-05, Loss_tot:0.1655,\n",
      "Save model 3828\n",
      "Train Epoch:3829 learning rate:1.0000e-05, Loss_tot:0.1655,\n",
      "Save model 3829\n",
      "Train Epoch:3830 learning rate:1.0000e-05, Loss_tot:0.1654,\n",
      "save model\n",
      "Save model 3830\n",
      "Train Epoch:3831 learning rate:1.0000e-05, Loss_tot:0.1653,\n",
      "save model\n",
      "Save model 3831\n",
      "Train Epoch:3832 learning rate:1.0000e-05, Loss_tot:0.1653,\n",
      "Save model 3832\n",
      "Train Epoch:3833 learning rate:1.0000e-05, Loss_tot:0.1653,\n",
      "save model\n",
      "Save model 3833\n",
      "Train Epoch:3834 learning rate:1.0000e-05, Loss_tot:0.1652,\n",
      "save model\n",
      "Save model 3834\n",
      "Train Epoch:3835 learning rate:1.0000e-05, Loss_tot:0.1652,\n",
      "save model\n",
      "Save model 3835\n",
      "Train Epoch:3836 learning rate:1.0000e-05, Loss_tot:0.1651,\n",
      "save model\n",
      "Save model 3836\n",
      "Train Epoch:3837 learning rate:1.0000e-05, Loss_tot:0.1651,\n",
      "save model\n",
      "Save model 3837\n",
      "Train Epoch:3838 learning rate:1.0000e-05, Loss_tot:0.1650,\n",
      "save model\n",
      "Save model 3838\n",
      "Train Epoch:3839 learning rate:1.0000e-05, Loss_tot:0.1650,\n",
      "save model\n",
      "Save model 3839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3840 learning rate:1.0000e-05, Loss_tot:0.1649,\n",
      "save model\n",
      "Save model 3840\n",
      "Train Epoch:3841 learning rate:1.0000e-05, Loss_tot:0.1650,\n",
      "Save model 3841\n",
      "Train Epoch:3842 learning rate:1.0000e-05, Loss_tot:0.1649,\n",
      "save model\n",
      "Save model 3842\n",
      "Train Epoch:3843 learning rate:1.0000e-05, Loss_tot:0.1649,\n",
      "save model\n",
      "Save model 3843\n",
      "Train Epoch:3844 learning rate:1.0000e-05, Loss_tot:0.1649,\n",
      "save model\n",
      "Save model 3844\n",
      "Train Epoch:3845 learning rate:1.0000e-05, Loss_tot:0.1648,\n",
      "save model\n",
      "Save model 3845\n",
      "Train Epoch:3846 learning rate:1.0000e-05, Loss_tot:0.1648,\n",
      "save model\n",
      "Save model 3846\n",
      "Train Epoch:3847 learning rate:1.0000e-05, Loss_tot:0.1647,\n",
      "save model\n",
      "Save model 3847\n",
      "Train Epoch:3848 learning rate:1.0000e-05, Loss_tot:0.1647,\n",
      "save model\n",
      "Save model 3848\n",
      "Train Epoch:3849 learning rate:1.0000e-05, Loss_tot:0.1647,\n",
      "save model\n",
      "Save model 3849\n",
      "Train Epoch:3850 learning rate:1.0000e-05, Loss_tot:0.1646,\n",
      "save model\n",
      "Save model 3850\n",
      "Train Epoch:3851 learning rate:1.0000e-05, Loss_tot:0.1646,\n",
      "save model\n",
      "Save model 3851\n",
      "Train Epoch:3852 learning rate:1.0000e-05, Loss_tot:0.1645,\n",
      "save model\n",
      "Save model 3852\n",
      "Train Epoch:3853 learning rate:1.0000e-05, Loss_tot:0.1646,\n",
      "Save model 3853\n",
      "Train Epoch:3854 learning rate:1.0000e-05, Loss_tot:0.1646,\n",
      "Save model 3854\n",
      "Train Epoch:3855 learning rate:1.0000e-05, Loss_tot:0.1644,\n",
      "save model\n",
      "Save model 3855\n",
      "Train Epoch:3856 learning rate:1.0000e-05, Loss_tot:0.1644,\n",
      "Save model 3856\n",
      "Train Epoch:3857 learning rate:1.0000e-05, Loss_tot:0.1645,\n",
      "Save model 3857\n",
      "Train Epoch:3858 learning rate:1.0000e-05, Loss_tot:0.1644,\n",
      "Save model 3858\n",
      "Train Epoch:3859 learning rate:1.0000e-05, Loss_tot:0.1644,\n",
      "save model\n",
      "Save model 3859\n",
      "Train Epoch:3860 learning rate:1.0000e-05, Loss_tot:0.1643,\n",
      "save model\n",
      "Save model 3860\n",
      "Train Epoch:3861 learning rate:1.0000e-05, Loss_tot:0.1642,\n",
      "save model\n",
      "Save model 3861\n",
      "Train Epoch:3862 learning rate:1.0000e-05, Loss_tot:0.1642,\n",
      "save model\n",
      "Save model 3862\n",
      "Train Epoch:3863 learning rate:1.0000e-05, Loss_tot:0.1642,\n",
      "save model\n",
      "Save model 3863\n",
      "Train Epoch:3864 learning rate:1.0000e-05, Loss_tot:0.1641,\n",
      "save model\n",
      "Save model 3864\n",
      "Train Epoch:3865 learning rate:1.0000e-05, Loss_tot:0.1641,\n",
      "save model\n",
      "Save model 3865\n",
      "Train Epoch:3866 learning rate:1.0000e-05, Loss_tot:0.1641,\n",
      "Save model 3866\n",
      "Train Epoch:3867 learning rate:1.0000e-05, Loss_tot:0.1641,\n",
      "save model\n",
      "Save model 3867\n",
      "Train Epoch:3868 learning rate:1.0000e-05, Loss_tot:0.1640,\n",
      "save model\n",
      "Save model 3868\n",
      "Train Epoch:3869 learning rate:1.0000e-05, Loss_tot:0.1640,\n",
      "save model\n",
      "Save model 3869\n",
      "Train Epoch:3870 learning rate:1.0000e-05, Loss_tot:0.1639,\n",
      "save model\n",
      "Save model 3870\n",
      "Train Epoch:3871 learning rate:1.0000e-05, Loss_tot:0.1639,\n",
      "save model\n",
      "Save model 3871\n",
      "Train Epoch:3872 learning rate:1.0000e-05, Loss_tot:0.1638,\n",
      "save model\n",
      "Save model 3872\n",
      "Train Epoch:3873 learning rate:1.0000e-05, Loss_tot:0.1638,\n",
      "save model\n",
      "Save model 3873\n",
      "Train Epoch:3874 learning rate:1.0000e-05, Loss_tot:0.1637,\n",
      "save model\n",
      "Save model 3874\n",
      "Train Epoch:3875 learning rate:1.0000e-05, Loss_tot:0.1637,\n",
      "save model\n",
      "Save model 3875\n",
      "Train Epoch:3876 learning rate:1.0000e-05, Loss_tot:0.1637,\n",
      "save model\n",
      "Save model 3876\n",
      "Train Epoch:3877 learning rate:1.0000e-05, Loss_tot:0.1637,\n",
      "save model\n",
      "Save model 3877\n",
      "Train Epoch:3878 learning rate:1.0000e-05, Loss_tot:0.1636,\n",
      "save model\n",
      "Save model 3878\n",
      "Train Epoch:3879 learning rate:1.0000e-05, Loss_tot:0.1636,\n",
      "save model\n",
      "Save model 3879\n",
      "Train Epoch:3880 learning rate:1.0000e-05, Loss_tot:0.1635,\n",
      "save model\n",
      "Save model 3880\n",
      "Train Epoch:3881 learning rate:1.0000e-05, Loss_tot:0.1635,\n",
      "save model\n",
      "Save model 3881\n",
      "Train Epoch:3882 learning rate:1.0000e-05, Loss_tot:0.1635,\n",
      "save model\n",
      "Save model 3882\n",
      "Train Epoch:3883 learning rate:1.0000e-05, Loss_tot:0.1634,\n",
      "save model\n",
      "Save model 3883\n",
      "Train Epoch:3884 learning rate:1.0000e-05, Loss_tot:0.1634,\n",
      "save model\n",
      "Save model 3884\n",
      "Train Epoch:3885 learning rate:1.0000e-05, Loss_tot:0.1634,\n",
      "save model\n",
      "Save model 3885\n",
      "Train Epoch:3886 learning rate:1.0000e-05, Loss_tot:0.1633,\n",
      "save model\n",
      "Save model 3886\n",
      "Train Epoch:3887 learning rate:1.0000e-05, Loss_tot:0.1634,\n",
      "Save model 3887\n",
      "Train Epoch:3888 learning rate:1.0000e-05, Loss_tot:0.1633,\n",
      "Save model 3888\n",
      "Train Epoch:3889 learning rate:1.0000e-05, Loss_tot:0.1632,\n",
      "save model\n",
      "Save model 3889\n",
      "Train Epoch:3890 learning rate:1.0000e-05, Loss_tot:0.1632,\n",
      "Save model 3890\n",
      "Train Epoch:3891 learning rate:1.0000e-05, Loss_tot:0.1632,\n",
      "Save model 3891\n",
      "Train Epoch:3892 learning rate:1.0000e-05, Loss_tot:0.1632,\n",
      "Save model 3892\n",
      "Train Epoch:3893 learning rate:1.0000e-05, Loss_tot:0.1632,\n",
      "save model\n",
      "Save model 3893\n",
      "Train Epoch:3894 learning rate:1.0000e-05, Loss_tot:0.1631,\n",
      "save model\n",
      "Save model 3894\n",
      "Train Epoch:3895 learning rate:1.0000e-05, Loss_tot:0.1630,\n",
      "save model\n",
      "Save model 3895\n",
      "Train Epoch:3896 learning rate:1.0000e-05, Loss_tot:0.1630,\n",
      "save model\n",
      "Save model 3896\n",
      "Train Epoch:3897 learning rate:1.0000e-05, Loss_tot:0.1630,\n",
      "Save model 3897\n",
      "Train Epoch:3898 learning rate:1.0000e-05, Loss_tot:0.1629,\n",
      "save model\n",
      "Save model 3898\n",
      "Train Epoch:3899 learning rate:1.0000e-05, Loss_tot:0.1629,\n",
      "save model\n",
      "Save model 3899\n",
      "Train Epoch:3900 learning rate:1.0000e-05, Loss_tot:0.1629,\n",
      "Save model 3900\n",
      "Train Epoch:3901 learning rate:1.0000e-05, Loss_tot:0.1629,\n",
      "save model\n",
      "Save model 3901\n",
      "Train Epoch:3902 learning rate:1.0000e-05, Loss_tot:0.1628,\n",
      "save model\n",
      "Save model 3902\n",
      "Train Epoch:3903 learning rate:1.0000e-05, Loss_tot:0.1627,\n",
      "save model\n",
      "Save model 3903\n",
      "Train Epoch:3904 learning rate:1.0000e-05, Loss_tot:0.1627,\n",
      "save model\n",
      "Save model 3904\n",
      "Train Epoch:3905 learning rate:1.0000e-05, Loss_tot:0.1627,\n",
      "save model\n",
      "Save model 3905\n",
      "Train Epoch:3906 learning rate:1.0000e-05, Loss_tot:0.1626,\n",
      "save model\n",
      "Save model 3906\n",
      "Train Epoch:3907 learning rate:1.0000e-05, Loss_tot:0.1626,\n",
      "save model\n",
      "Save model 3907\n",
      "Train Epoch:3908 learning rate:1.0000e-05, Loss_tot:0.1625,\n",
      "save model\n",
      "Save model 3908\n",
      "Train Epoch:3909 learning rate:1.0000e-05, Loss_tot:0.1625,\n",
      "Save model 3909\n",
      "Train Epoch:3910 learning rate:1.0000e-05, Loss_tot:0.1625,\n",
      "save model\n",
      "Save model 3910\n",
      "Train Epoch:3911 learning rate:1.0000e-05, Loss_tot:0.1624,\n",
      "save model\n",
      "Save model 3911\n",
      "Train Epoch:3912 learning rate:1.0000e-05, Loss_tot:0.1624,\n",
      "save model\n",
      "Save model 3912\n",
      "Train Epoch:3913 learning rate:1.0000e-05, Loss_tot:0.1624,\n",
      "save model\n",
      "Save model 3913\n",
      "Train Epoch:3914 learning rate:1.0000e-05, Loss_tot:0.1623,\n",
      "save model\n",
      "Save model 3914\n",
      "Train Epoch:3915 learning rate:1.0000e-05, Loss_tot:0.1623,\n",
      "save model\n",
      "Save model 3915\n",
      "Train Epoch:3916 learning rate:1.0000e-05, Loss_tot:0.1623,\n",
      "save model\n",
      "Save model 3916\n",
      "Train Epoch:3917 learning rate:1.0000e-05, Loss_tot:0.1622,\n",
      "save model\n",
      "Save model 3917\n",
      "Train Epoch:3918 learning rate:1.0000e-05, Loss_tot:0.1622,\n",
      "save model\n",
      "Save model 3918\n",
      "Train Epoch:3919 learning rate:1.0000e-05, Loss_tot:0.1622,\n",
      "save model\n",
      "Save model 3919\n",
      "Train Epoch:3920 learning rate:1.0000e-05, Loss_tot:0.1621,\n",
      "save model\n",
      "Save model 3920\n",
      "Train Epoch:3921 learning rate:1.0000e-05, Loss_tot:0.1621,\n",
      "Save model 3921\n",
      "Train Epoch:3922 learning rate:1.0000e-05, Loss_tot:0.1621,\n",
      "Save model 3922\n",
      "Train Epoch:3923 learning rate:1.0000e-05, Loss_tot:0.1620,\n",
      "save model\n",
      "Save model 3923\n",
      "Train Epoch:3924 learning rate:1.0000e-05, Loss_tot:0.1620,\n",
      "Save model 3924\n",
      "Train Epoch:3925 learning rate:1.0000e-05, Loss_tot:0.1620,\n",
      "Save model 3925\n",
      "Train Epoch:3926 learning rate:1.0000e-05, Loss_tot:0.1620,\n",
      "Save model 3926\n",
      "Train Epoch:3927 learning rate:1.0000e-05, Loss_tot:0.1620,\n",
      "save model\n",
      "Save model 3927\n",
      "Train Epoch:3928 learning rate:1.0000e-05, Loss_tot:0.1619,\n",
      "save model\n",
      "Save model 3928\n",
      "Train Epoch:3929 learning rate:1.0000e-05, Loss_tot:0.1618,\n",
      "save model\n",
      "Save model 3929\n",
      "Train Epoch:3930 learning rate:1.0000e-05, Loss_tot:0.1618,\n",
      "save model\n",
      "Save model 3930\n",
      "Train Epoch:3931 learning rate:1.0000e-05, Loss_tot:0.1618,\n",
      "Save model 3931\n",
      "Train Epoch:3932 learning rate:1.0000e-05, Loss_tot:0.1617,\n",
      "save model\n",
      "Save model 3932\n",
      "Train Epoch:3933 learning rate:1.0000e-05, Loss_tot:0.1617,\n",
      "Save model 3933\n",
      "Train Epoch:3934 learning rate:1.0000e-05, Loss_tot:0.1617,\n",
      "Save model 3934\n",
      "Train Epoch:3935 learning rate:1.0000e-05, Loss_tot:0.1617,\n",
      "save model\n",
      "Save model 3935\n",
      "Train Epoch:3936 learning rate:1.0000e-05, Loss_tot:0.1616,\n",
      "save model\n",
      "Save model 3936\n",
      "Train Epoch:3937 learning rate:1.0000e-05, Loss_tot:0.1615,\n",
      "save model\n",
      "Save model 3937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:3938 learning rate:1.0000e-05, Loss_tot:0.1614,\n",
      "save model\n",
      "Save model 3938\n",
      "Train Epoch:3939 learning rate:1.0000e-05, Loss_tot:0.1614,\n",
      "save model\n",
      "Save model 3939\n",
      "Train Epoch:3940 learning rate:1.0000e-05, Loss_tot:0.1614,\n",
      "save model\n",
      "Save model 3940\n",
      "Train Epoch:3941 learning rate:1.0000e-05, Loss_tot:0.1614,\n",
      "save model\n",
      "Save model 3941\n",
      "Train Epoch:3942 learning rate:1.0000e-05, Loss_tot:0.1613,\n",
      "save model\n",
      "Save model 3942\n",
      "Train Epoch:3943 learning rate:1.0000e-05, Loss_tot:0.1613,\n",
      "save model\n",
      "Save model 3943\n",
      "Train Epoch:3944 learning rate:1.0000e-05, Loss_tot:0.1612,\n",
      "save model\n",
      "Save model 3944\n",
      "Train Epoch:3945 learning rate:1.0000e-05, Loss_tot:0.1612,\n",
      "save model\n",
      "Save model 3945\n",
      "Train Epoch:3946 learning rate:1.0000e-05, Loss_tot:0.1612,\n",
      "save model\n",
      "Save model 3946\n",
      "Train Epoch:3947 learning rate:1.0000e-05, Loss_tot:0.1612,\n",
      "save model\n",
      "Save model 3947\n",
      "Train Epoch:3948 learning rate:1.0000e-05, Loss_tot:0.1611,\n",
      "save model\n",
      "Save model 3948\n",
      "Train Epoch:3949 learning rate:1.0000e-05, Loss_tot:0.1611,\n",
      "save model\n",
      "Save model 3949\n",
      "Train Epoch:3950 learning rate:1.0000e-05, Loss_tot:0.1610,\n",
      "save model\n",
      "Save model 3950\n",
      "Train Epoch:3951 learning rate:1.0000e-05, Loss_tot:0.1610,\n",
      "save model\n",
      "Save model 3951\n",
      "Train Epoch:3952 learning rate:1.0000e-05, Loss_tot:0.1610,\n",
      "save model\n",
      "Save model 3952\n",
      "Train Epoch:3953 learning rate:1.0000e-05, Loss_tot:0.1610,\n",
      "save model\n",
      "Save model 3953\n",
      "Train Epoch:3954 learning rate:1.0000e-05, Loss_tot:0.1609,\n",
      "save model\n",
      "Save model 3954\n",
      "Train Epoch:3955 learning rate:1.0000e-05, Loss_tot:0.1610,\n",
      "Save model 3955\n",
      "Train Epoch:3956 learning rate:1.0000e-05, Loss_tot:0.1609,\n",
      "Save model 3956\n",
      "Train Epoch:3957 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "save model\n",
      "Save model 3957\n",
      "Train Epoch:3958 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "Save model 3958\n",
      "Train Epoch:3959 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "Save model 3959\n",
      "Train Epoch:3960 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "Save model 3960\n",
      "Train Epoch:3961 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "save model\n",
      "Save model 3961\n",
      "Train Epoch:3962 learning rate:1.0000e-05, Loss_tot:0.1607,\n",
      "save model\n",
      "Save model 3962\n",
      "Train Epoch:3963 learning rate:1.0000e-05, Loss_tot:0.1606,\n",
      "save model\n",
      "Save model 3963\n",
      "Train Epoch:3964 learning rate:1.0000e-05, Loss_tot:0.1605,\n",
      "save model\n",
      "Save model 3964\n",
      "Train Epoch:3965 learning rate:1.0000e-05, Loss_tot:0.1605,\n",
      "Save model 3965\n",
      "Train Epoch:3966 learning rate:1.0000e-05, Loss_tot:0.1605,\n",
      "save model\n",
      "Save model 3966\n",
      "Train Epoch:3967 learning rate:1.0000e-05, Loss_tot:0.1604,\n",
      "save model\n",
      "Save model 3967\n",
      "Train Epoch:3968 learning rate:1.0000e-05, Loss_tot:0.1604,\n",
      "save model\n",
      "Save model 3968\n",
      "Train Epoch:3969 learning rate:1.0000e-05, Loss_tot:0.1604,\n",
      "save model\n",
      "Save model 3969\n",
      "Train Epoch:3970 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "save model\n",
      "Save model 3970\n",
      "Train Epoch:3971 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "Save model 3971\n",
      "Train Epoch:3972 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "save model\n",
      "Save model 3972\n",
      "Train Epoch:3973 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "Save model 3973\n",
      "Train Epoch:3974 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "Save model 3974\n",
      "Train Epoch:3975 learning rate:1.0000e-05, Loss_tot:0.1602,\n",
      "save model\n",
      "Save model 3975\n",
      "Train Epoch:3976 learning rate:1.0000e-05, Loss_tot:0.1602,\n",
      "save model\n",
      "Save model 3976\n",
      "Train Epoch:3977 learning rate:1.0000e-05, Loss_tot:0.1601,\n",
      "save model\n",
      "Save model 3977\n",
      "Train Epoch:3978 learning rate:1.0000e-05, Loss_tot:0.1601,\n",
      "save model\n",
      "Save model 3978\n",
      "Train Epoch:3979 learning rate:1.0000e-05, Loss_tot:0.1601,\n",
      "save model\n",
      "Save model 3979\n",
      "Train Epoch:3980 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "save model\n",
      "Save model 3980\n",
      "Train Epoch:3981 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "Save model 3981\n",
      "Train Epoch:3982 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "Save model 3982\n",
      "Train Epoch:3983 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "Save model 3983\n",
      "Train Epoch:3984 learning rate:1.0000e-05, Loss_tot:0.1599,\n",
      "save model\n",
      "Save model 3984\n",
      "Train Epoch:3985 learning rate:1.0000e-05, Loss_tot:0.1599,\n",
      "save model\n",
      "Save model 3985\n",
      "Train Epoch:3986 learning rate:1.0000e-05, Loss_tot:0.1598,\n",
      "save model\n",
      "Save model 3986\n",
      "Train Epoch:3987 learning rate:1.0000e-05, Loss_tot:0.1598,\n",
      "Save model 3987\n",
      "Train Epoch:3988 learning rate:1.0000e-05, Loss_tot:0.1598,\n",
      "Save model 3988\n",
      "Train Epoch:3989 learning rate:1.0000e-05, Loss_tot:0.1597,\n",
      "save model\n",
      "Save model 3989\n",
      "Train Epoch:3990 learning rate:1.0000e-05, Loss_tot:0.1596,\n",
      "save model\n",
      "Save model 3990\n",
      "Train Epoch:3991 learning rate:1.0000e-05, Loss_tot:0.1596,\n",
      "Save model 3991\n",
      "Train Epoch:3992 learning rate:1.0000e-05, Loss_tot:0.1596,\n",
      "save model\n",
      "Save model 3992\n",
      "Train Epoch:3993 learning rate:1.0000e-05, Loss_tot:0.1596,\n",
      "save model\n",
      "Save model 3993\n",
      "Train Epoch:3994 learning rate:1.0000e-05, Loss_tot:0.1595,\n",
      "save model\n",
      "Save model 3994\n",
      "Train Epoch:3995 learning rate:1.0000e-05, Loss_tot:0.1595,\n",
      "Save model 3995\n",
      "Train Epoch:3996 learning rate:1.0000e-05, Loss_tot:0.1595,\n",
      "Save model 3996\n",
      "Train Epoch:3997 learning rate:1.0000e-05, Loss_tot:0.1594,\n",
      "save model\n",
      "Save model 3997\n",
      "Train Epoch:3998 learning rate:1.0000e-05, Loss_tot:0.1594,\n",
      "save model\n",
      "Save model 3998\n",
      "Train Epoch:3999 learning rate:1.0000e-05, Loss_tot:0.1594,\n",
      "Save model 3999\n",
      "Train Epoch:4000 learning rate:1.0000e-05, Loss_tot:0.1594,\n",
      "save model\n",
      "Save model 4000\n",
      "Train Epoch:4001 learning rate:1.0000e-05, Loss_tot:0.1593,\n",
      "save model\n",
      "Save model 4001\n",
      "Train Epoch:4002 learning rate:1.0000e-05, Loss_tot:0.1593,\n",
      "save model\n",
      "Save model 4002\n",
      "Train Epoch:4003 learning rate:1.0000e-05, Loss_tot:0.1592,\n",
      "save model\n",
      "Save model 4003\n",
      "Train Epoch:4004 learning rate:1.0000e-05, Loss_tot:0.1592,\n",
      "Save model 4004\n",
      "Train Epoch:4005 learning rate:1.0000e-05, Loss_tot:0.1592,\n",
      "Save model 4005\n",
      "Train Epoch:4006 learning rate:1.0000e-05, Loss_tot:0.1592,\n",
      "save model\n",
      "Save model 4006\n",
      "Train Epoch:4007 learning rate:1.0000e-05, Loss_tot:0.1590,\n",
      "save model\n",
      "Save model 4007\n",
      "Train Epoch:4008 learning rate:1.0000e-05, Loss_tot:0.1590,\n",
      "Save model 4008\n",
      "Train Epoch:4009 learning rate:1.0000e-05, Loss_tot:0.1590,\n",
      "save model\n",
      "Save model 4009\n",
      "Train Epoch:4010 learning rate:1.0000e-05, Loss_tot:0.1589,\n",
      "save model\n",
      "Save model 4010\n",
      "Train Epoch:4011 learning rate:1.0000e-05, Loss_tot:0.1589,\n",
      "save model\n",
      "Save model 4011\n",
      "Train Epoch:4012 learning rate:1.0000e-05, Loss_tot:0.1588,\n",
      "save model\n",
      "Save model 4012\n",
      "Train Epoch:4013 learning rate:1.0000e-05, Loss_tot:0.1588,\n",
      "save model\n",
      "Save model 4013\n",
      "Train Epoch:4014 learning rate:1.0000e-05, Loss_tot:0.1588,\n",
      "save model\n",
      "Save model 4014\n",
      "Train Epoch:4015 learning rate:1.0000e-05, Loss_tot:0.1588,\n",
      "save model\n",
      "Save model 4015\n",
      "Train Epoch:4016 learning rate:1.0000e-05, Loss_tot:0.1587,\n",
      "save model\n",
      "Save model 4016\n",
      "Train Epoch:4017 learning rate:1.0000e-05, Loss_tot:0.1587,\n",
      "save model\n",
      "Save model 4017\n",
      "Train Epoch:4018 learning rate:1.0000e-05, Loss_tot:0.1586,\n",
      "save model\n",
      "Save model 4018\n",
      "Train Epoch:4019 learning rate:1.0000e-05, Loss_tot:0.1586,\n",
      "save model\n",
      "Save model 4019\n",
      "Train Epoch:4020 learning rate:1.0000e-05, Loss_tot:0.1586,\n",
      "save model\n",
      "Save model 4020\n",
      "Train Epoch:4021 learning rate:1.0000e-05, Loss_tot:0.1586,\n",
      "save model\n",
      "Save model 4021\n",
      "Train Epoch:4022 learning rate:1.0000e-05, Loss_tot:0.1585,\n",
      "save model\n",
      "Save model 4022\n",
      "Train Epoch:4023 learning rate:1.0000e-05, Loss_tot:0.1585,\n",
      "save model\n",
      "Save model 4023\n",
      "Train Epoch:4024 learning rate:1.0000e-05, Loss_tot:0.1585,\n",
      "save model\n",
      "Save model 4024\n",
      "Train Epoch:4025 learning rate:1.0000e-05, Loss_tot:0.1584,\n",
      "save model\n",
      "Save model 4025\n",
      "Train Epoch:4026 learning rate:1.0000e-05, Loss_tot:0.1584,\n",
      "save model\n",
      "Save model 4026\n",
      "Train Epoch:4027 learning rate:1.0000e-05, Loss_tot:0.1584,\n",
      "Save model 4027\n",
      "Train Epoch:4028 learning rate:1.0000e-05, Loss_tot:0.1584,\n",
      "save model\n",
      "Save model 4028\n",
      "Train Epoch:4029 learning rate:1.0000e-05, Loss_tot:0.1584,\n",
      "save model\n",
      "Save model 4029\n",
      "Train Epoch:4030 learning rate:1.0000e-05, Loss_tot:0.1583,\n",
      "save model\n",
      "Save model 4030\n",
      "Train Epoch:4031 learning rate:1.0000e-05, Loss_tot:0.1583,\n",
      "save model\n",
      "Save model 4031\n",
      "Train Epoch:4032 learning rate:1.0000e-05, Loss_tot:0.1582,\n",
      "save model\n",
      "Save model 4032\n",
      "Train Epoch:4033 learning rate:1.0000e-05, Loss_tot:0.1582,\n",
      "save model\n",
      "Save model 4033\n",
      "Train Epoch:4034 learning rate:1.0000e-05, Loss_tot:0.1582,\n",
      "save model\n",
      "Save model 4034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4035 learning rate:1.0000e-05, Loss_tot:0.1581,\n",
      "save model\n",
      "Save model 4035\n",
      "Train Epoch:4036 learning rate:1.0000e-05, Loss_tot:0.1581,\n",
      "save model\n",
      "Save model 4036\n",
      "Train Epoch:4037 learning rate:1.0000e-05, Loss_tot:0.1582,\n",
      "Save model 4037\n",
      "Train Epoch:4038 learning rate:1.0000e-05, Loss_tot:0.1581,\n",
      "Save model 4038\n",
      "Train Epoch:4039 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "save model\n",
      "Save model 4039\n",
      "Train Epoch:4040 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "save model\n",
      "Save model 4040\n",
      "Train Epoch:4041 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "Save model 4041\n",
      "Train Epoch:4042 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "Save model 4042\n",
      "Train Epoch:4043 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "save model\n",
      "Save model 4043\n",
      "Train Epoch:4044 learning rate:1.0000e-05, Loss_tot:0.1579,\n",
      "save model\n",
      "Save model 4044\n",
      "Train Epoch:4045 learning rate:1.0000e-05, Loss_tot:0.1578,\n",
      "save model\n",
      "Save model 4045\n",
      "Train Epoch:4046 learning rate:1.0000e-05, Loss_tot:0.1578,\n",
      "save model\n",
      "Save model 4046\n",
      "Train Epoch:4047 learning rate:1.0000e-05, Loss_tot:0.1578,\n",
      "Save model 4047\n",
      "Train Epoch:4048 learning rate:1.0000e-05, Loss_tot:0.1578,\n",
      "save model\n",
      "Save model 4048\n",
      "Train Epoch:4049 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "save model\n",
      "Save model 4049\n",
      "Train Epoch:4050 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "Save model 4050\n",
      "Train Epoch:4051 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "save model\n",
      "Save model 4051\n",
      "Train Epoch:4052 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "save model\n",
      "Save model 4052\n",
      "Train Epoch:4053 learning rate:1.0000e-05, Loss_tot:0.1576,\n",
      "save model\n",
      "Save model 4053\n",
      "Train Epoch:4054 learning rate:1.0000e-05, Loss_tot:0.1576,\n",
      "save model\n",
      "Save model 4054\n",
      "Train Epoch:4055 learning rate:1.0000e-05, Loss_tot:0.1575,\n",
      "save model\n",
      "Save model 4055\n",
      "Train Epoch:4056 learning rate:1.0000e-05, Loss_tot:0.1575,\n",
      "save model\n",
      "Save model 4056\n",
      "Train Epoch:4057 learning rate:1.0000e-05, Loss_tot:0.1574,\n",
      "save model\n",
      "Save model 4057\n",
      "Train Epoch:4058 learning rate:1.0000e-05, Loss_tot:0.1574,\n",
      "save model\n",
      "Save model 4058\n",
      "Train Epoch:4059 learning rate:1.0000e-05, Loss_tot:0.1574,\n",
      "Save model 4059\n",
      "Train Epoch:4060 learning rate:1.0000e-05, Loss_tot:0.1574,\n",
      "save model\n",
      "Save model 4060\n",
      "Train Epoch:4061 learning rate:1.0000e-05, Loss_tot:0.1573,\n",
      "save model\n",
      "Save model 4061\n",
      "Train Epoch:4062 learning rate:1.0000e-05, Loss_tot:0.1573,\n",
      "save model\n",
      "Save model 4062\n",
      "Train Epoch:4063 learning rate:1.0000e-05, Loss_tot:0.1573,\n",
      "save model\n",
      "Save model 4063\n",
      "Train Epoch:4064 learning rate:1.0000e-05, Loss_tot:0.1572,\n",
      "save model\n",
      "Save model 4064\n",
      "Train Epoch:4065 learning rate:1.0000e-05, Loss_tot:0.1572,\n",
      "Save model 4065\n",
      "Train Epoch:4066 learning rate:1.0000e-05, Loss_tot:0.1572,\n",
      "save model\n",
      "Save model 4066\n",
      "Train Epoch:4067 learning rate:1.0000e-05, Loss_tot:0.1571,\n",
      "save model\n",
      "Save model 4067\n",
      "Train Epoch:4068 learning rate:1.0000e-05, Loss_tot:0.1571,\n",
      "save model\n",
      "Save model 4068\n",
      "Train Epoch:4069 learning rate:1.0000e-05, Loss_tot:0.1571,\n",
      "save model\n",
      "Save model 4069\n",
      "Train Epoch:4070 learning rate:1.0000e-05, Loss_tot:0.1570,\n",
      "save model\n",
      "Save model 4070\n",
      "Train Epoch:4071 learning rate:1.0000e-05, Loss_tot:0.1570,\n",
      "save model\n",
      "Save model 4071\n",
      "Train Epoch:4072 learning rate:1.0000e-05, Loss_tot:0.1569,\n",
      "save model\n",
      "Save model 4072\n",
      "Train Epoch:4073 learning rate:1.0000e-05, Loss_tot:0.1569,\n",
      "save model\n",
      "Save model 4073\n",
      "Train Epoch:4074 learning rate:1.0000e-05, Loss_tot:0.1569,\n",
      "save model\n",
      "Save model 4074\n",
      "Train Epoch:4075 learning rate:1.0000e-05, Loss_tot:0.1568,\n",
      "save model\n",
      "Save model 4075\n",
      "Train Epoch:4076 learning rate:1.0000e-05, Loss_tot:0.1568,\n",
      "Save model 4076\n",
      "Train Epoch:4077 learning rate:1.0000e-05, Loss_tot:0.1568,\n",
      "save model\n",
      "Save model 4077\n",
      "Train Epoch:4078 learning rate:1.0000e-05, Loss_tot:0.1568,\n",
      "save model\n",
      "Save model 4078\n",
      "Train Epoch:4079 learning rate:1.0000e-05, Loss_tot:0.1567,\n",
      "save model\n",
      "Save model 4079\n",
      "Train Epoch:4080 learning rate:1.0000e-05, Loss_tot:0.1567,\n",
      "save model\n",
      "Save model 4080\n",
      "Train Epoch:4081 learning rate:1.0000e-05, Loss_tot:0.1566,\n",
      "save model\n",
      "Save model 4081\n",
      "Train Epoch:4082 learning rate:1.0000e-05, Loss_tot:0.1566,\n",
      "Save model 4082\n",
      "Train Epoch:4083 learning rate:1.0000e-05, Loss_tot:0.1566,\n",
      "save model\n",
      "Save model 4083\n",
      "Train Epoch:4084 learning rate:1.0000e-05, Loss_tot:0.1565,\n",
      "save model\n",
      "Save model 4084\n",
      "Train Epoch:4085 learning rate:1.0000e-05, Loss_tot:0.1565,\n",
      "save model\n",
      "Save model 4085\n",
      "Train Epoch:4086 learning rate:1.0000e-05, Loss_tot:0.1565,\n",
      "save model\n",
      "Save model 4086\n",
      "Train Epoch:4087 learning rate:1.0000e-05, Loss_tot:0.1564,\n",
      "save model\n",
      "Save model 4087\n",
      "Train Epoch:4088 learning rate:1.0000e-05, Loss_tot:0.1564,\n",
      "save model\n",
      "Save model 4088\n",
      "Train Epoch:4089 learning rate:1.0000e-05, Loss_tot:0.1564,\n",
      "save model\n",
      "Save model 4089\n",
      "Train Epoch:4090 learning rate:1.0000e-05, Loss_tot:0.1563,\n",
      "save model\n",
      "Save model 4090\n",
      "Train Epoch:4091 learning rate:1.0000e-05, Loss_tot:0.1563,\n",
      "save model\n",
      "Save model 4091\n",
      "Train Epoch:4092 learning rate:1.0000e-05, Loss_tot:0.1563,\n",
      "save model\n",
      "Save model 4092\n",
      "Train Epoch:4093 learning rate:1.0000e-05, Loss_tot:0.1563,\n",
      "Save model 4093\n",
      "Train Epoch:4094 learning rate:1.0000e-05, Loss_tot:0.1562,\n",
      "save model\n",
      "Save model 4094\n",
      "Train Epoch:4095 learning rate:1.0000e-05, Loss_tot:0.1562,\n",
      "save model\n",
      "Save model 4095\n",
      "Train Epoch:4096 learning rate:1.0000e-05, Loss_tot:0.1562,\n",
      "save model\n",
      "Save model 4096\n",
      "Train Epoch:4097 learning rate:1.0000e-05, Loss_tot:0.1561,\n",
      "save model\n",
      "Save model 4097\n",
      "Train Epoch:4098 learning rate:1.0000e-05, Loss_tot:0.1561,\n",
      "save model\n",
      "Save model 4098\n",
      "Train Epoch:4099 learning rate:1.0000e-05, Loss_tot:0.1561,\n",
      "Save model 4099\n",
      "Train Epoch:4100 learning rate:1.0000e-05, Loss_tot:0.1560,\n",
      "save model\n",
      "Save model 4100\n",
      "Train Epoch:4101 learning rate:1.0000e-05, Loss_tot:0.1560,\n",
      "save model\n",
      "Save model 4101\n",
      "Train Epoch:4102 learning rate:1.0000e-05, Loss_tot:0.1559,\n",
      "save model\n",
      "Save model 4102\n",
      "Train Epoch:4103 learning rate:1.0000e-05, Loss_tot:0.1559,\n",
      "save model\n",
      "Save model 4103\n",
      "Train Epoch:4104 learning rate:1.0000e-05, Loss_tot:0.1559,\n",
      "save model\n",
      "Save model 4104\n",
      "Train Epoch:4105 learning rate:1.0000e-05, Loss_tot:0.1558,\n",
      "save model\n",
      "Save model 4105\n",
      "Train Epoch:4106 learning rate:1.0000e-05, Loss_tot:0.1558,\n",
      "save model\n",
      "Save model 4106\n",
      "Train Epoch:4107 learning rate:1.0000e-05, Loss_tot:0.1558,\n",
      "save model\n",
      "Save model 4107\n",
      "Train Epoch:4108 learning rate:1.0000e-05, Loss_tot:0.1557,\n",
      "save model\n",
      "Save model 4108\n",
      "Train Epoch:4109 learning rate:1.0000e-05, Loss_tot:0.1557,\n",
      "save model\n",
      "Save model 4109\n",
      "Train Epoch:4110 learning rate:1.0000e-05, Loss_tot:0.1557,\n",
      "Save model 4110\n",
      "Train Epoch:4111 learning rate:1.0000e-05, Loss_tot:0.1556,\n",
      "save model\n",
      "Save model 4111\n",
      "Train Epoch:4112 learning rate:1.0000e-05, Loss_tot:0.1556,\n",
      "save model\n",
      "Save model 4112\n",
      "Train Epoch:4113 learning rate:1.0000e-05, Loss_tot:0.1556,\n",
      "save model\n",
      "Save model 4113\n",
      "Train Epoch:4114 learning rate:1.0000e-05, Loss_tot:0.1556,\n",
      "save model\n",
      "Save model 4114\n",
      "Train Epoch:4115 learning rate:1.0000e-05, Loss_tot:0.1555,\n",
      "save model\n",
      "Save model 4115\n",
      "Train Epoch:4116 learning rate:1.0000e-05, Loss_tot:0.1555,\n",
      "Save model 4116\n",
      "Train Epoch:4117 learning rate:1.0000e-05, Loss_tot:0.1555,\n",
      "save model\n",
      "Save model 4117\n",
      "Train Epoch:4118 learning rate:1.0000e-05, Loss_tot:0.1554,\n",
      "save model\n",
      "Save model 4118\n",
      "Train Epoch:4119 learning rate:1.0000e-05, Loss_tot:0.1554,\n",
      "save model\n",
      "Save model 4119\n",
      "Train Epoch:4120 learning rate:1.0000e-05, Loss_tot:0.1553,\n",
      "save model\n",
      "Save model 4120\n",
      "Train Epoch:4121 learning rate:1.0000e-05, Loss_tot:0.1553,\n",
      "save model\n",
      "Save model 4121\n",
      "Train Epoch:4122 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "save model\n",
      "Save model 4122\n",
      "Train Epoch:4123 learning rate:1.0000e-05, Loss_tot:0.1553,\n",
      "Save model 4123\n",
      "Train Epoch:4124 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "Save model 4124\n",
      "Train Epoch:4125 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "save model\n",
      "Save model 4125\n",
      "Train Epoch:4126 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "save model\n",
      "Save model 4126\n",
      "Train Epoch:4127 learning rate:1.0000e-05, Loss_tot:0.1551,\n",
      "save model\n",
      "Save model 4127\n",
      "Train Epoch:4128 learning rate:1.0000e-05, Loss_tot:0.1551,\n",
      "Save model 4128\n",
      "Train Epoch:4129 learning rate:1.0000e-05, Loss_tot:0.1551,\n",
      "Save model 4129\n",
      "Train Epoch:4130 learning rate:1.0000e-05, Loss_tot:0.1550,\n",
      "save model\n",
      "Save model 4130\n",
      "Train Epoch:4131 learning rate:1.0000e-05, Loss_tot:0.1550,\n",
      "save model\n",
      "Save model 4131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4132 learning rate:1.0000e-05, Loss_tot:0.1550,\n",
      "Save model 4132\n",
      "Train Epoch:4133 learning rate:1.0000e-05, Loss_tot:0.1550,\n",
      "save model\n",
      "Save model 4133\n",
      "Train Epoch:4134 learning rate:1.0000e-05, Loss_tot:0.1549,\n",
      "save model\n",
      "Save model 4134\n",
      "Train Epoch:4135 learning rate:1.0000e-05, Loss_tot:0.1548,\n",
      "save model\n",
      "Save model 4135\n",
      "Train Epoch:4136 learning rate:1.0000e-05, Loss_tot:0.1548,\n",
      "save model\n",
      "Save model 4136\n",
      "Train Epoch:4137 learning rate:1.0000e-05, Loss_tot:0.1548,\n",
      "save model\n",
      "Save model 4137\n",
      "Train Epoch:4138 learning rate:1.0000e-05, Loss_tot:0.1547,\n",
      "save model\n",
      "Save model 4138\n",
      "Train Epoch:4139 learning rate:1.0000e-05, Loss_tot:0.1547,\n",
      "save model\n",
      "Save model 4139\n",
      "Train Epoch:4140 learning rate:1.0000e-05, Loss_tot:0.1546,\n",
      "save model\n",
      "Save model 4140\n",
      "Train Epoch:4141 learning rate:1.0000e-05, Loss_tot:0.1547,\n",
      "Save model 4141\n",
      "Train Epoch:4142 learning rate:1.0000e-05, Loss_tot:0.1546,\n",
      "save model\n",
      "Save model 4142\n",
      "Train Epoch:4143 learning rate:1.0000e-05, Loss_tot:0.1546,\n",
      "save model\n",
      "Save model 4143\n",
      "Train Epoch:4144 learning rate:1.0000e-05, Loss_tot:0.1546,\n",
      "save model\n",
      "Save model 4144\n",
      "Train Epoch:4145 learning rate:1.0000e-05, Loss_tot:0.1545,\n",
      "save model\n",
      "Save model 4145\n",
      "Train Epoch:4146 learning rate:1.0000e-05, Loss_tot:0.1545,\n",
      "save model\n",
      "Save model 4146\n",
      "Train Epoch:4147 learning rate:1.0000e-05, Loss_tot:0.1544,\n",
      "save model\n",
      "Save model 4147\n",
      "Train Epoch:4148 learning rate:1.0000e-05, Loss_tot:0.1544,\n",
      "save model\n",
      "Save model 4148\n",
      "Train Epoch:4149 learning rate:1.0000e-05, Loss_tot:0.1544,\n",
      "save model\n",
      "Save model 4149\n",
      "Train Epoch:4150 learning rate:1.0000e-05, Loss_tot:0.1543,\n",
      "save model\n",
      "Save model 4150\n",
      "Train Epoch:4151 learning rate:1.0000e-05, Loss_tot:0.1543,\n",
      "save model\n",
      "Save model 4151\n",
      "Train Epoch:4152 learning rate:1.0000e-05, Loss_tot:0.1542,\n",
      "save model\n",
      "Save model 4152\n",
      "Train Epoch:4153 learning rate:1.0000e-05, Loss_tot:0.1542,\n",
      "save model\n",
      "Save model 4153\n",
      "Train Epoch:4154 learning rate:1.0000e-05, Loss_tot:0.1542,\n",
      "save model\n",
      "Save model 4154\n",
      "Train Epoch:4155 learning rate:1.0000e-05, Loss_tot:0.1542,\n",
      "Save model 4155\n",
      "Train Epoch:4156 learning rate:1.0000e-05, Loss_tot:0.1541,\n",
      "save model\n",
      "Save model 4156\n",
      "Train Epoch:4157 learning rate:1.0000e-05, Loss_tot:0.1541,\n",
      "save model\n",
      "Save model 4157\n",
      "Train Epoch:4158 learning rate:1.0000e-05, Loss_tot:0.1541,\n",
      "save model\n",
      "Save model 4158\n",
      "Train Epoch:4159 learning rate:1.0000e-05, Loss_tot:0.1541,\n",
      "save model\n",
      "Save model 4159\n",
      "Train Epoch:4160 learning rate:1.0000e-05, Loss_tot:0.1540,\n",
      "save model\n",
      "Save model 4160\n",
      "Train Epoch:4161 learning rate:1.0000e-05, Loss_tot:0.1539,\n",
      "save model\n",
      "Save model 4161\n",
      "Train Epoch:4162 learning rate:1.0000e-05, Loss_tot:0.1539,\n",
      "save model\n",
      "Save model 4162\n",
      "Train Epoch:4163 learning rate:1.0000e-05, Loss_tot:0.1539,\n",
      "Save model 4163\n",
      "Train Epoch:4164 learning rate:1.0000e-05, Loss_tot:0.1538,\n",
      "save model\n",
      "Save model 4164\n",
      "Train Epoch:4165 learning rate:1.0000e-05, Loss_tot:0.1538,\n",
      "save model\n",
      "Save model 4165\n",
      "Train Epoch:4166 learning rate:1.0000e-05, Loss_tot:0.1538,\n",
      "Save model 4166\n",
      "Train Epoch:4167 learning rate:1.0000e-05, Loss_tot:0.1537,\n",
      "save model\n",
      "Save model 4167\n",
      "Train Epoch:4168 learning rate:1.0000e-05, Loss_tot:0.1537,\n",
      "Save model 4168\n",
      "Train Epoch:4169 learning rate:1.0000e-05, Loss_tot:0.1537,\n",
      "Save model 4169\n",
      "Train Epoch:4170 learning rate:1.0000e-05, Loss_tot:0.1537,\n",
      "save model\n",
      "Save model 4170\n",
      "Train Epoch:4171 learning rate:1.0000e-05, Loss_tot:0.1537,\n",
      "save model\n",
      "Save model 4171\n",
      "Train Epoch:4172 learning rate:1.0000e-05, Loss_tot:0.1536,\n",
      "save model\n",
      "Save model 4172\n",
      "Train Epoch:4173 learning rate:1.0000e-05, Loss_tot:0.1536,\n",
      "save model\n",
      "Save model 4173\n",
      "Train Epoch:4174 learning rate:1.0000e-05, Loss_tot:0.1535,\n",
      "save model\n",
      "Save model 4174\n",
      "Train Epoch:4175 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "save model\n",
      "Save model 4175\n",
      "Train Epoch:4176 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "save model\n",
      "Save model 4176\n",
      "Train Epoch:4177 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "save model\n",
      "Save model 4177\n",
      "Train Epoch:4178 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "save model\n",
      "Save model 4178\n",
      "Train Epoch:4179 learning rate:1.0000e-05, Loss_tot:0.1533,\n",
      "save model\n",
      "Save model 4179\n",
      "Train Epoch:4180 learning rate:1.0000e-05, Loss_tot:0.1533,\n",
      "save model\n",
      "Save model 4180\n",
      "Train Epoch:4181 learning rate:1.0000e-05, Loss_tot:0.1533,\n",
      "save model\n",
      "Save model 4181\n",
      "Train Epoch:4182 learning rate:1.0000e-05, Loss_tot:0.1532,\n",
      "save model\n",
      "Save model 4182\n",
      "Train Epoch:4183 learning rate:1.0000e-05, Loss_tot:0.1532,\n",
      "save model\n",
      "Save model 4183\n",
      "Train Epoch:4184 learning rate:1.0000e-05, Loss_tot:0.1532,\n",
      "save model\n",
      "Save model 4184\n",
      "Train Epoch:4185 learning rate:1.0000e-05, Loss_tot:0.1531,\n",
      "save model\n",
      "Save model 4185\n",
      "Train Epoch:4186 learning rate:1.0000e-05, Loss_tot:0.1531,\n",
      "Save model 4186\n",
      "Train Epoch:4187 learning rate:1.0000e-05, Loss_tot:0.1531,\n",
      "save model\n",
      "Save model 4187\n",
      "Train Epoch:4188 learning rate:1.0000e-05, Loss_tot:0.1531,\n",
      "Save model 4188\n",
      "Train Epoch:4189 learning rate:1.0000e-05, Loss_tot:0.1531,\n",
      "save model\n",
      "Save model 4189\n",
      "Train Epoch:4190 learning rate:1.0000e-05, Loss_tot:0.1530,\n",
      "save model\n",
      "Save model 4190\n",
      "Train Epoch:4191 learning rate:1.0000e-05, Loss_tot:0.1530,\n",
      "save model\n",
      "Save model 4191\n",
      "Train Epoch:4192 learning rate:1.0000e-05, Loss_tot:0.1529,\n",
      "save model\n",
      "Save model 4192\n",
      "Train Epoch:4193 learning rate:1.0000e-05, Loss_tot:0.1529,\n",
      "Save model 4193\n",
      "Train Epoch:4194 learning rate:1.0000e-05, Loss_tot:0.1529,\n",
      "Save model 4194\n",
      "Train Epoch:4195 learning rate:1.0000e-05, Loss_tot:0.1528,\n",
      "save model\n",
      "Save model 4195\n",
      "Train Epoch:4196 learning rate:1.0000e-05, Loss_tot:0.1528,\n",
      "save model\n",
      "Save model 4196\n",
      "Train Epoch:4197 learning rate:1.0000e-05, Loss_tot:0.1528,\n",
      "Save model 4197\n",
      "Train Epoch:4198 learning rate:1.0000e-05, Loss_tot:0.1528,\n",
      "save model\n",
      "Save model 4198\n",
      "Train Epoch:4199 learning rate:1.0000e-05, Loss_tot:0.1527,\n",
      "save model\n",
      "Save model 4199\n",
      "Train Epoch:4200 learning rate:1.0000e-05, Loss_tot:0.1527,\n",
      "save model\n",
      "Save model 4200\n",
      "Train Epoch:4201 learning rate:1.0000e-05, Loss_tot:0.1526,\n",
      "save model\n",
      "Save model 4201\n",
      "Train Epoch:4202 learning rate:1.0000e-05, Loss_tot:0.1527,\n",
      "Save model 4202\n",
      "Train Epoch:4203 learning rate:1.0000e-05, Loss_tot:0.1527,\n",
      "Save model 4203\n",
      "Train Epoch:4204 learning rate:1.0000e-05, Loss_tot:0.1526,\n",
      "Save model 4204\n",
      "Train Epoch:4205 learning rate:1.0000e-05, Loss_tot:0.1524,\n",
      "save model\n",
      "Save model 4205\n",
      "Train Epoch:4206 learning rate:1.0000e-05, Loss_tot:0.1525,\n",
      "Save model 4206\n",
      "Train Epoch:4207 learning rate:1.0000e-05, Loss_tot:0.1525,\n",
      "Save model 4207\n",
      "Train Epoch:4208 learning rate:1.0000e-05, Loss_tot:0.1525,\n",
      "Save model 4208\n",
      "Train Epoch:4209 learning rate:1.0000e-05, Loss_tot:0.1525,\n",
      "Save model 4209\n",
      "Train Epoch:4210 learning rate:1.0000e-05, Loss_tot:0.1525,\n",
      "Save model 4210\n",
      "Train Epoch:4211 learning rate:1.0000e-05, Loss_tot:0.1524,\n",
      "save model\n",
      "Save model 4211\n",
      "Train Epoch:4212 learning rate:1.0000e-05, Loss_tot:0.1523,\n",
      "save model\n",
      "Save model 4212\n",
      "Train Epoch:4213 learning rate:1.0000e-05, Loss_tot:0.1522,\n",
      "save model\n",
      "Save model 4213\n",
      "Train Epoch:4214 learning rate:1.0000e-05, Loss_tot:0.1522,\n",
      "Save model 4214\n",
      "Train Epoch:4215 learning rate:1.0000e-05, Loss_tot:0.1523,\n",
      "Save model 4215\n",
      "Train Epoch:4216 learning rate:1.0000e-05, Loss_tot:0.1522,\n",
      "Save model 4216\n",
      "Train Epoch:4217 learning rate:1.0000e-05, Loss_tot:0.1521,\n",
      "save model\n",
      "Save model 4217\n",
      "Train Epoch:4218 learning rate:1.0000e-05, Loss_tot:0.1520,\n",
      "save model\n",
      "Save model 4218\n",
      "Train Epoch:4219 learning rate:1.0000e-05, Loss_tot:0.1521,\n",
      "Save model 4219\n",
      "Train Epoch:4220 learning rate:1.0000e-05, Loss_tot:0.1521,\n",
      "Save model 4220\n",
      "Train Epoch:4221 learning rate:1.0000e-05, Loss_tot:0.1520,\n",
      "save model\n",
      "Save model 4221\n",
      "Train Epoch:4222 learning rate:1.0000e-05, Loss_tot:0.1520,\n",
      "save model\n",
      "Save model 4222\n",
      "Train Epoch:4223 learning rate:1.0000e-05, Loss_tot:0.1519,\n",
      "save model\n",
      "Save model 4223\n",
      "Train Epoch:4224 learning rate:1.0000e-05, Loss_tot:0.1518,\n",
      "save model\n",
      "Save model 4224\n",
      "Train Epoch:4225 learning rate:1.0000e-05, Loss_tot:0.1519,\n",
      "Save model 4225\n",
      "Train Epoch:4226 learning rate:1.0000e-05, Loss_tot:0.1519,\n",
      "Save model 4226\n",
      "Train Epoch:4227 learning rate:1.0000e-05, Loss_tot:0.1518,\n",
      "Save model 4227\n",
      "Train Epoch:4228 learning rate:1.0000e-05, Loss_tot:0.1517,\n",
      "save model\n",
      "Save model 4228\n",
      "Train Epoch:4229 learning rate:1.0000e-05, Loss_tot:0.1517,\n",
      "Save model 4229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4230 learning rate:1.0000e-05, Loss_tot:0.1517,\n",
      "Save model 4230\n",
      "Train Epoch:4231 learning rate:1.0000e-05, Loss_tot:0.1517,\n",
      "Save model 4231\n",
      "Train Epoch:4232 learning rate:1.0000e-05, Loss_tot:0.1517,\n",
      "Save model 4232\n",
      "Train Epoch:4233 learning rate:1.0000e-05, Loss_tot:0.1516,\n",
      "save model\n",
      "Save model 4233\n",
      "Train Epoch:4234 learning rate:1.0000e-05, Loss_tot:0.1516,\n",
      "save model\n",
      "Save model 4234\n",
      "Train Epoch:4235 learning rate:1.0000e-05, Loss_tot:0.1515,\n",
      "save model\n",
      "Save model 4235\n",
      "Train Epoch:4236 learning rate:1.0000e-05, Loss_tot:0.1514,\n",
      "save model\n",
      "Save model 4236\n",
      "Train Epoch:4237 learning rate:1.0000e-05, Loss_tot:0.1514,\n",
      "Save model 4237\n",
      "Train Epoch:4238 learning rate:1.0000e-05, Loss_tot:0.1513,\n",
      "save model\n",
      "Save model 4238\n",
      "Train Epoch:4239 learning rate:1.0000e-05, Loss_tot:0.1513,\n",
      "save model\n",
      "Save model 4239\n",
      "Train Epoch:4240 learning rate:1.0000e-05, Loss_tot:0.1513,\n",
      "save model\n",
      "Save model 4240\n",
      "Train Epoch:4241 learning rate:1.0000e-05, Loss_tot:0.1513,\n",
      "save model\n",
      "Save model 4241\n",
      "Train Epoch:4242 learning rate:1.0000e-05, Loss_tot:0.1512,\n",
      "save model\n",
      "Save model 4242\n",
      "Train Epoch:4243 learning rate:1.0000e-05, Loss_tot:0.1511,\n",
      "save model\n",
      "Save model 4243\n",
      "Train Epoch:4244 learning rate:1.0000e-05, Loss_tot:0.1512,\n",
      "Save model 4244\n",
      "Train Epoch:4245 learning rate:1.0000e-05, Loss_tot:0.1511,\n",
      "Save model 4245\n",
      "Train Epoch:4246 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "save model\n",
      "Save model 4246\n",
      "Train Epoch:4247 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "save model\n",
      "Save model 4247\n",
      "Train Epoch:4248 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "Save model 4248\n",
      "Train Epoch:4249 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "save model\n",
      "Save model 4249\n",
      "Train Epoch:4250 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "save model\n",
      "Save model 4250\n",
      "Train Epoch:4251 learning rate:1.0000e-05, Loss_tot:0.1509,\n",
      "save model\n",
      "Save model 4251\n",
      "Train Epoch:4252 learning rate:1.0000e-05, Loss_tot:0.1508,\n",
      "save model\n",
      "Save model 4252\n",
      "Train Epoch:4253 learning rate:1.0000e-05, Loss_tot:0.1509,\n",
      "Save model 4253\n",
      "Train Epoch:4254 learning rate:1.0000e-05, Loss_tot:0.1509,\n",
      "Save model 4254\n",
      "Train Epoch:4255 learning rate:1.0000e-05, Loss_tot:0.1508,\n",
      "save model\n",
      "Save model 4255\n",
      "Train Epoch:4256 learning rate:1.0000e-05, Loss_tot:0.1507,\n",
      "save model\n",
      "Save model 4256\n",
      "Train Epoch:4257 learning rate:1.0000e-05, Loss_tot:0.1507,\n",
      "Save model 4257\n",
      "Train Epoch:4258 learning rate:1.0000e-05, Loss_tot:0.1507,\n",
      "save model\n",
      "Save model 4258\n",
      "Train Epoch:4259 learning rate:1.0000e-05, Loss_tot:0.1506,\n",
      "save model\n",
      "Save model 4259\n",
      "Train Epoch:4260 learning rate:1.0000e-05, Loss_tot:0.1505,\n",
      "save model\n",
      "Save model 4260\n",
      "Train Epoch:4261 learning rate:1.0000e-05, Loss_tot:0.1505,\n",
      "save model\n",
      "Save model 4261\n",
      "Train Epoch:4262 learning rate:1.0000e-05, Loss_tot:0.1505,\n",
      "save model\n",
      "Save model 4262\n",
      "Train Epoch:4263 learning rate:1.0000e-05, Loss_tot:0.1505,\n",
      "save model\n",
      "Save model 4263\n",
      "Train Epoch:4264 learning rate:1.0000e-05, Loss_tot:0.1504,\n",
      "save model\n",
      "Save model 4264\n",
      "Train Epoch:4265 learning rate:1.0000e-05, Loss_tot:0.1504,\n",
      "save model\n",
      "Save model 4265\n",
      "Train Epoch:4266 learning rate:1.0000e-05, Loss_tot:0.1504,\n",
      "Save model 4266\n",
      "Train Epoch:4267 learning rate:1.0000e-05, Loss_tot:0.1504,\n",
      "Save model 4267\n",
      "Train Epoch:4268 learning rate:1.0000e-05, Loss_tot:0.1503,\n",
      "save model\n",
      "Save model 4268\n",
      "Train Epoch:4269 learning rate:1.0000e-05, Loss_tot:0.1503,\n",
      "save model\n",
      "Save model 4269\n",
      "Train Epoch:4270 learning rate:1.0000e-05, Loss_tot:0.1502,\n",
      "save model\n",
      "Save model 4270\n",
      "Train Epoch:4271 learning rate:1.0000e-05, Loss_tot:0.1502,\n",
      "save model\n",
      "Save model 4271\n",
      "Train Epoch:4272 learning rate:1.0000e-05, Loss_tot:0.1501,\n",
      "save model\n",
      "Save model 4272\n",
      "Train Epoch:4273 learning rate:1.0000e-05, Loss_tot:0.1501,\n",
      "save model\n",
      "Save model 4273\n",
      "Train Epoch:4274 learning rate:1.0000e-05, Loss_tot:0.1501,\n",
      "Save model 4274\n",
      "Train Epoch:4275 learning rate:1.0000e-05, Loss_tot:0.1501,\n",
      "save model\n",
      "Save model 4275\n",
      "Train Epoch:4276 learning rate:1.0000e-05, Loss_tot:0.1500,\n",
      "save model\n",
      "Save model 4276\n",
      "Train Epoch:4277 learning rate:1.0000e-05, Loss_tot:0.1500,\n",
      "save model\n",
      "Save model 4277\n",
      "Train Epoch:4278 learning rate:1.0000e-05, Loss_tot:0.1500,\n",
      "save model\n",
      "Save model 4278\n",
      "Train Epoch:4279 learning rate:1.0000e-05, Loss_tot:0.1499,\n",
      "save model\n",
      "Save model 4279\n",
      "Train Epoch:4280 learning rate:1.0000e-05, Loss_tot:0.1499,\n",
      "save model\n",
      "Save model 4280\n",
      "Train Epoch:4281 learning rate:1.0000e-05, Loss_tot:0.1499,\n",
      "save model\n",
      "Save model 4281\n",
      "Train Epoch:4282 learning rate:1.0000e-05, Loss_tot:0.1498,\n",
      "save model\n",
      "Save model 4282\n",
      "Train Epoch:4283 learning rate:1.0000e-05, Loss_tot:0.1498,\n",
      "save model\n",
      "Save model 4283\n",
      "Train Epoch:4284 learning rate:1.0000e-05, Loss_tot:0.1498,\n",
      "save model\n",
      "Save model 4284\n",
      "Train Epoch:4285 learning rate:1.0000e-05, Loss_tot:0.1497,\n",
      "save model\n",
      "Save model 4285\n",
      "Train Epoch:4286 learning rate:1.0000e-05, Loss_tot:0.1497,\n",
      "save model\n",
      "Save model 4286\n",
      "Train Epoch:4287 learning rate:1.0000e-05, Loss_tot:0.1497,\n",
      "save model\n",
      "Save model 4287\n",
      "Train Epoch:4288 learning rate:1.0000e-05, Loss_tot:0.1496,\n",
      "save model\n",
      "Save model 4288\n",
      "Train Epoch:4289 learning rate:1.0000e-05, Loss_tot:0.1496,\n",
      "save model\n",
      "Save model 4289\n",
      "Train Epoch:4290 learning rate:1.0000e-05, Loss_tot:0.1495,\n",
      "save model\n",
      "Save model 4290\n",
      "Train Epoch:4291 learning rate:1.0000e-05, Loss_tot:0.1495,\n",
      "Save model 4291\n",
      "Train Epoch:4292 learning rate:1.0000e-05, Loss_tot:0.1495,\n",
      "save model\n",
      "Save model 4292\n",
      "Train Epoch:4293 learning rate:1.0000e-05, Loss_tot:0.1495,\n",
      "save model\n",
      "Save model 4293\n",
      "Train Epoch:4294 learning rate:1.0000e-05, Loss_tot:0.1494,\n",
      "save model\n",
      "Save model 4294\n",
      "Train Epoch:4295 learning rate:1.0000e-05, Loss_tot:0.1494,\n",
      "save model\n",
      "Save model 4295\n",
      "Train Epoch:4296 learning rate:1.0000e-05, Loss_tot:0.1494,\n",
      "save model\n",
      "Save model 4296\n",
      "Train Epoch:4297 learning rate:1.0000e-05, Loss_tot:0.1494,\n",
      "save model\n",
      "Save model 4297\n",
      "Train Epoch:4298 learning rate:1.0000e-05, Loss_tot:0.1493,\n",
      "save model\n",
      "Save model 4298\n",
      "Train Epoch:4299 learning rate:1.0000e-05, Loss_tot:0.1493,\n",
      "save model\n",
      "Save model 4299\n",
      "Train Epoch:4300 learning rate:1.0000e-05, Loss_tot:0.1492,\n",
      "save model\n",
      "Save model 4300\n",
      "Train Epoch:4301 learning rate:1.0000e-05, Loss_tot:0.1492,\n",
      "save model\n",
      "Save model 4301\n",
      "Train Epoch:4302 learning rate:1.0000e-05, Loss_tot:0.1492,\n",
      "save model\n",
      "Save model 4302\n",
      "Train Epoch:4303 learning rate:1.0000e-05, Loss_tot:0.1491,\n",
      "save model\n",
      "Save model 4303\n",
      "Train Epoch:4304 learning rate:1.0000e-05, Loss_tot:0.1491,\n",
      "save model\n",
      "Save model 4304\n",
      "Train Epoch:4305 learning rate:1.0000e-05, Loss_tot:0.1490,\n",
      "save model\n",
      "Save model 4305\n",
      "Train Epoch:4306 learning rate:1.0000e-05, Loss_tot:0.1490,\n",
      "save model\n",
      "Save model 4306\n",
      "Train Epoch:4307 learning rate:1.0000e-05, Loss_tot:0.1490,\n",
      "save model\n",
      "Save model 4307\n",
      "Train Epoch:4308 learning rate:1.0000e-05, Loss_tot:0.1490,\n",
      "save model\n",
      "Save model 4308\n",
      "Train Epoch:4309 learning rate:1.0000e-05, Loss_tot:0.1489,\n",
      "save model\n",
      "Save model 4309\n",
      "Train Epoch:4310 learning rate:1.0000e-05, Loss_tot:0.1489,\n",
      "save model\n",
      "Save model 4310\n",
      "Train Epoch:4311 learning rate:1.0000e-05, Loss_tot:0.1489,\n",
      "save model\n",
      "Save model 4311\n",
      "Train Epoch:4312 learning rate:1.0000e-05, Loss_tot:0.1488,\n",
      "save model\n",
      "Save model 4312\n",
      "Train Epoch:4313 learning rate:1.0000e-05, Loss_tot:0.1488,\n",
      "save model\n",
      "Save model 4313\n",
      "Train Epoch:4314 learning rate:1.0000e-05, Loss_tot:0.1488,\n",
      "save model\n",
      "Save model 4314\n",
      "Train Epoch:4315 learning rate:1.0000e-05, Loss_tot:0.1488,\n",
      "save model\n",
      "Save model 4315\n",
      "Train Epoch:4316 learning rate:1.0000e-05, Loss_tot:0.1487,\n",
      "save model\n",
      "Save model 4316\n",
      "Train Epoch:4317 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "save model\n",
      "Save model 4317\n",
      "Train Epoch:4318 learning rate:1.0000e-05, Loss_tot:0.1487,\n",
      "Save model 4318\n",
      "Train Epoch:4319 learning rate:1.0000e-05, Loss_tot:0.1486,\n",
      "Save model 4319\n",
      "Train Epoch:4320 learning rate:1.0000e-05, Loss_tot:0.1486,\n",
      "Save model 4320\n",
      "Train Epoch:4321 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "save model\n",
      "Save model 4321\n",
      "Train Epoch:4322 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "save model\n",
      "Save model 4322\n",
      "Train Epoch:4323 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "Save model 4323\n",
      "Train Epoch:4324 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "Save model 4324\n",
      "Train Epoch:4325 learning rate:1.0000e-05, Loss_tot:0.1484,\n",
      "save model\n",
      "Save model 4325\n",
      "Train Epoch:4326 learning rate:1.0000e-05, Loss_tot:0.1484,\n",
      "Save model 4326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4327 learning rate:1.0000e-05, Loss_tot:0.1484,\n",
      "Save model 4327\n",
      "Train Epoch:4328 learning rate:1.0000e-05, Loss_tot:0.1484,\n",
      "Save model 4328\n",
      "Train Epoch:4329 learning rate:1.0000e-05, Loss_tot:0.1484,\n",
      "Save model 4329\n",
      "Train Epoch:4330 learning rate:1.0000e-05, Loss_tot:0.1483,\n",
      "save model\n",
      "Save model 4330\n",
      "Train Epoch:4331 learning rate:1.0000e-05, Loss_tot:0.1482,\n",
      "save model\n",
      "Save model 4331\n",
      "Train Epoch:4332 learning rate:1.0000e-05, Loss_tot:0.1482,\n",
      "save model\n",
      "Save model 4332\n",
      "Train Epoch:4333 learning rate:1.0000e-05, Loss_tot:0.1482,\n",
      "Save model 4333\n",
      "Train Epoch:4334 learning rate:1.0000e-05, Loss_tot:0.1481,\n",
      "save model\n",
      "Save model 4334\n",
      "Train Epoch:4335 learning rate:1.0000e-05, Loss_tot:0.1480,\n",
      "save model\n",
      "Save model 4335\n",
      "Train Epoch:4336 learning rate:1.0000e-05, Loss_tot:0.1481,\n",
      "Save model 4336\n",
      "Train Epoch:4337 learning rate:1.0000e-05, Loss_tot:0.1480,\n",
      "save model\n",
      "Save model 4337\n",
      "Train Epoch:4338 learning rate:1.0000e-05, Loss_tot:0.1479,\n",
      "save model\n",
      "Save model 4338\n",
      "Train Epoch:4339 learning rate:1.0000e-05, Loss_tot:0.1480,\n",
      "Save model 4339\n",
      "Train Epoch:4340 learning rate:1.0000e-05, Loss_tot:0.1479,\n",
      "save model\n",
      "Save model 4340\n",
      "Train Epoch:4341 learning rate:1.0000e-05, Loss_tot:0.1479,\n",
      "save model\n",
      "Save model 4341\n",
      "Train Epoch:4342 learning rate:1.0000e-05, Loss_tot:0.1478,\n",
      "save model\n",
      "Save model 4342\n",
      "Train Epoch:4343 learning rate:1.0000e-05, Loss_tot:0.1479,\n",
      "Save model 4343\n",
      "Train Epoch:4344 learning rate:1.0000e-05, Loss_tot:0.1478,\n",
      "Save model 4344\n",
      "Train Epoch:4345 learning rate:1.0000e-05, Loss_tot:0.1477,\n",
      "save model\n",
      "Save model 4345\n",
      "Train Epoch:4346 learning rate:1.0000e-05, Loss_tot:0.1477,\n",
      "Save model 4346\n",
      "Train Epoch:4347 learning rate:1.0000e-05, Loss_tot:0.1477,\n",
      "Save model 4347\n",
      "Train Epoch:4348 learning rate:1.0000e-05, Loss_tot:0.1477,\n",
      "Save model 4348\n",
      "Train Epoch:4349 learning rate:1.0000e-05, Loss_tot:0.1477,\n",
      "save model\n",
      "Save model 4349\n",
      "Train Epoch:4350 learning rate:1.0000e-05, Loss_tot:0.1476,\n",
      "save model\n",
      "Save model 4350\n",
      "Train Epoch:4351 learning rate:1.0000e-05, Loss_tot:0.1475,\n",
      "save model\n",
      "Save model 4351\n",
      "Train Epoch:4352 learning rate:1.0000e-05, Loss_tot:0.1475,\n",
      "save model\n",
      "Save model 4352\n",
      "Train Epoch:4353 learning rate:1.0000e-05, Loss_tot:0.1475,\n",
      "Save model 4353\n",
      "Train Epoch:4354 learning rate:1.0000e-05, Loss_tot:0.1475,\n",
      "save model\n",
      "Save model 4354\n",
      "Train Epoch:4355 learning rate:1.0000e-05, Loss_tot:0.1474,\n",
      "save model\n",
      "Save model 4355\n",
      "Train Epoch:4356 learning rate:1.0000e-05, Loss_tot:0.1474,\n",
      "Save model 4356\n",
      "Train Epoch:4357 learning rate:1.0000e-05, Loss_tot:0.1474,\n",
      "save model\n",
      "Save model 4357\n",
      "Train Epoch:4358 learning rate:1.0000e-05, Loss_tot:0.1473,\n",
      "save model\n",
      "Save model 4358\n",
      "Train Epoch:4359 learning rate:1.0000e-05, Loss_tot:0.1473,\n",
      "save model\n",
      "Save model 4359\n",
      "Train Epoch:4360 learning rate:1.0000e-05, Loss_tot:0.1473,\n",
      "save model\n",
      "Save model 4360\n",
      "Train Epoch:4361 learning rate:1.0000e-05, Loss_tot:0.1472,\n",
      "save model\n",
      "Save model 4361\n",
      "Train Epoch:4362 learning rate:1.0000e-05, Loss_tot:0.1471,\n",
      "save model\n",
      "Save model 4362\n",
      "Train Epoch:4363 learning rate:1.0000e-05, Loss_tot:0.1472,\n",
      "Save model 4363\n",
      "Train Epoch:4364 learning rate:1.0000e-05, Loss_tot:0.1472,\n",
      "Save model 4364\n",
      "Train Epoch:4365 learning rate:1.0000e-05, Loss_tot:0.1472,\n",
      "Save model 4365\n",
      "Train Epoch:4366 learning rate:1.0000e-05, Loss_tot:0.1471,\n",
      "save model\n",
      "Save model 4366\n",
      "Train Epoch:4367 learning rate:1.0000e-05, Loss_tot:0.1471,\n",
      "save model\n",
      "Save model 4367\n",
      "Train Epoch:4368 learning rate:1.0000e-05, Loss_tot:0.1470,\n",
      "save model\n",
      "Save model 4368\n",
      "Train Epoch:4369 learning rate:1.0000e-05, Loss_tot:0.1470,\n",
      "Save model 4369\n",
      "Train Epoch:4370 learning rate:1.0000e-05, Loss_tot:0.1470,\n",
      "Save model 4370\n",
      "Train Epoch:4371 learning rate:1.0000e-05, Loss_tot:0.1469,\n",
      "save model\n",
      "Save model 4371\n",
      "Train Epoch:4372 learning rate:1.0000e-05, Loss_tot:0.1468,\n",
      "save model\n",
      "Save model 4372\n",
      "Train Epoch:4373 learning rate:1.0000e-05, Loss_tot:0.1468,\n",
      "Save model 4373\n",
      "Train Epoch:4374 learning rate:1.0000e-05, Loss_tot:0.1468,\n",
      "save model\n",
      "Save model 4374\n",
      "Train Epoch:4375 learning rate:1.0000e-05, Loss_tot:0.1468,\n",
      "save model\n",
      "Save model 4375\n",
      "Train Epoch:4376 learning rate:1.0000e-05, Loss_tot:0.1467,\n",
      "save model\n",
      "Save model 4376\n",
      "Train Epoch:4377 learning rate:1.0000e-05, Loss_tot:0.1467,\n",
      "Save model 4377\n",
      "Train Epoch:4378 learning rate:1.0000e-05, Loss_tot:0.1467,\n",
      "Save model 4378\n",
      "Train Epoch:4379 learning rate:1.0000e-05, Loss_tot:0.1466,\n",
      "save model\n",
      "Save model 4379\n",
      "Train Epoch:4380 learning rate:1.0000e-05, Loss_tot:0.1466,\n",
      "Save model 4380\n",
      "Train Epoch:4381 learning rate:1.0000e-05, Loss_tot:0.1466,\n",
      "Save model 4381\n",
      "Train Epoch:4382 learning rate:1.0000e-05, Loss_tot:0.1466,\n",
      "Save model 4382\n",
      "Train Epoch:4383 learning rate:1.0000e-05, Loss_tot:0.1466,\n",
      "save model\n",
      "Save model 4383\n",
      "Train Epoch:4384 learning rate:1.0000e-05, Loss_tot:0.1465,\n",
      "save model\n",
      "Save model 4384\n",
      "Train Epoch:4385 learning rate:1.0000e-05, Loss_tot:0.1464,\n",
      "save model\n",
      "Save model 4385\n",
      "Train Epoch:4386 learning rate:1.0000e-05, Loss_tot:0.1464,\n",
      "Save model 4386\n",
      "Train Epoch:4387 learning rate:1.0000e-05, Loss_tot:0.1464,\n",
      "Save model 4387\n",
      "Train Epoch:4388 learning rate:1.0000e-05, Loss_tot:0.1464,\n",
      "save model\n",
      "Save model 4388\n",
      "Train Epoch:4389 learning rate:1.0000e-05, Loss_tot:0.1463,\n",
      "save model\n",
      "Save model 4389\n",
      "Train Epoch:4390 learning rate:1.0000e-05, Loss_tot:0.1463,\n",
      "Save model 4390\n",
      "Train Epoch:4391 learning rate:1.0000e-05, Loss_tot:0.1462,\n",
      "save model\n",
      "Save model 4391\n",
      "Train Epoch:4392 learning rate:1.0000e-05, Loss_tot:0.1462,\n",
      "save model\n",
      "Save model 4392\n",
      "Train Epoch:4393 learning rate:1.0000e-05, Loss_tot:0.1461,\n",
      "save model\n",
      "Save model 4393\n",
      "Train Epoch:4394 learning rate:1.0000e-05, Loss_tot:0.1462,\n",
      "Save model 4394\n",
      "Train Epoch:4395 learning rate:1.0000e-05, Loss_tot:0.1462,\n",
      "Save model 4395\n",
      "Train Epoch:4396 learning rate:1.0000e-05, Loss_tot:0.1461,\n",
      "save model\n",
      "Save model 4396\n",
      "Train Epoch:4397 learning rate:1.0000e-05, Loss_tot:0.1460,\n",
      "save model\n",
      "Save model 4397\n",
      "Train Epoch:4398 learning rate:1.0000e-05, Loss_tot:0.1460,\n",
      "Save model 4398\n",
      "Train Epoch:4399 learning rate:1.0000e-05, Loss_tot:0.1460,\n",
      "save model\n",
      "Save model 4399\n",
      "Train Epoch:4400 learning rate:1.0000e-05, Loss_tot:0.1460,\n",
      "save model\n",
      "Save model 4400\n",
      "Train Epoch:4401 learning rate:1.0000e-05, Loss_tot:0.1459,\n",
      "save model\n",
      "Save model 4401\n",
      "Train Epoch:4402 learning rate:1.0000e-05, Loss_tot:0.1458,\n",
      "save model\n",
      "Save model 4402\n",
      "Train Epoch:4403 learning rate:1.0000e-05, Loss_tot:0.1459,\n",
      "Save model 4403\n",
      "Train Epoch:4404 learning rate:1.0000e-05, Loss_tot:0.1459,\n",
      "Save model 4404\n",
      "Train Epoch:4405 learning rate:1.0000e-05, Loss_tot:0.1458,\n",
      "Save model 4405\n",
      "Train Epoch:4406 learning rate:1.0000e-05, Loss_tot:0.1457,\n",
      "save model\n",
      "Save model 4406\n",
      "Train Epoch:4407 learning rate:1.0000e-05, Loss_tot:0.1457,\n",
      "Save model 4407\n",
      "Train Epoch:4408 learning rate:1.0000e-05, Loss_tot:0.1457,\n",
      "save model\n",
      "Save model 4408\n",
      "Train Epoch:4409 learning rate:1.0000e-05, Loss_tot:0.1456,\n",
      "save model\n",
      "Save model 4409\n",
      "Train Epoch:4410 learning rate:1.0000e-05, Loss_tot:0.1455,\n",
      "save model\n",
      "Save model 4410\n",
      "Train Epoch:4411 learning rate:1.0000e-05, Loss_tot:0.1455,\n",
      "save model\n",
      "Save model 4411\n",
      "Train Epoch:4412 learning rate:1.0000e-05, Loss_tot:0.1455,\n",
      "save model\n",
      "Save model 4412\n",
      "Train Epoch:4413 learning rate:1.0000e-05, Loss_tot:0.1455,\n",
      "save model\n",
      "Save model 4413\n",
      "Train Epoch:4414 learning rate:1.0000e-05, Loss_tot:0.1454,\n",
      "save model\n",
      "Save model 4414\n",
      "Train Epoch:4415 learning rate:1.0000e-05, Loss_tot:0.1454,\n",
      "save model\n",
      "Save model 4415\n",
      "Train Epoch:4416 learning rate:1.0000e-05, Loss_tot:0.1454,\n",
      "Save model 4416\n",
      "Train Epoch:4417 learning rate:1.0000e-05, Loss_tot:0.1454,\n",
      "Save model 4417\n",
      "Train Epoch:4418 learning rate:1.0000e-05, Loss_tot:0.1453,\n",
      "save model\n",
      "Save model 4418\n",
      "Train Epoch:4419 learning rate:1.0000e-05, Loss_tot:0.1453,\n",
      "save model\n",
      "Save model 4419\n",
      "Train Epoch:4420 learning rate:1.0000e-05, Loss_tot:0.1452,\n",
      "save model\n",
      "Save model 4420\n",
      "Train Epoch:4421 learning rate:1.0000e-05, Loss_tot:0.1452,\n",
      "save model\n",
      "Save model 4421\n",
      "Train Epoch:4422 learning rate:1.0000e-05, Loss_tot:0.1452,\n",
      "save model\n",
      "Save model 4422\n",
      "Train Epoch:4423 learning rate:1.0000e-05, Loss_tot:0.1451,\n",
      "save model\n",
      "Save model 4423\n",
      "Train Epoch:4424 learning rate:1.0000e-05, Loss_tot:0.1451,\n",
      "Save model 4424\n",
      "Train Epoch:4425 learning rate:1.0000e-05, Loss_tot:0.1451,\n",
      "save model\n",
      "Save model 4425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4426 learning rate:1.0000e-05, Loss_tot:0.1451,\n",
      "save model\n",
      "Save model 4426\n",
      "Train Epoch:4427 learning rate:1.0000e-05, Loss_tot:0.1451,\n",
      "save model\n",
      "Save model 4427\n",
      "Train Epoch:4428 learning rate:1.0000e-05, Loss_tot:0.1450,\n",
      "save model\n",
      "Save model 4428\n",
      "Train Epoch:4429 learning rate:1.0000e-05, Loss_tot:0.1450,\n",
      "save model\n",
      "Save model 4429\n",
      "Train Epoch:4430 learning rate:1.0000e-05, Loss_tot:0.1449,\n",
      "save model\n",
      "Save model 4430\n",
      "Train Epoch:4431 learning rate:1.0000e-05, Loss_tot:0.1450,\n",
      "Save model 4431\n",
      "Train Epoch:4432 learning rate:1.0000e-05, Loss_tot:0.1450,\n",
      "Save model 4432\n",
      "Train Epoch:4433 learning rate:1.0000e-05, Loss_tot:0.1449,\n",
      "save model\n",
      "Save model 4433\n",
      "Train Epoch:4434 learning rate:1.0000e-05, Loss_tot:0.1448,\n",
      "save model\n",
      "Save model 4434\n",
      "Train Epoch:4435 learning rate:1.0000e-05, Loss_tot:0.1448,\n",
      "Save model 4435\n",
      "Train Epoch:4436 learning rate:1.0000e-05, Loss_tot:0.1448,\n",
      "save model\n",
      "Save model 4436\n",
      "Train Epoch:4437 learning rate:1.0000e-05, Loss_tot:0.1447,\n",
      "save model\n",
      "Save model 4437\n",
      "Train Epoch:4438 learning rate:1.0000e-05, Loss_tot:0.1446,\n",
      "save model\n",
      "Save model 4438\n",
      "Train Epoch:4439 learning rate:1.0000e-05, Loss_tot:0.1446,\n",
      "save model\n",
      "Save model 4439\n",
      "Train Epoch:4440 learning rate:1.0000e-05, Loss_tot:0.1446,\n",
      "save model\n",
      "Save model 4440\n",
      "Train Epoch:4441 learning rate:1.0000e-05, Loss_tot:0.1445,\n",
      "save model\n",
      "Save model 4441\n",
      "Train Epoch:4442 learning rate:1.0000e-05, Loss_tot:0.1445,\n",
      "save model\n",
      "Save model 4442\n",
      "Train Epoch:4443 learning rate:1.0000e-05, Loss_tot:0.1445,\n",
      "save model\n",
      "Save model 4443\n",
      "Train Epoch:4444 learning rate:1.0000e-05, Loss_tot:0.1445,\n",
      "Save model 4444\n",
      "Train Epoch:4445 learning rate:1.0000e-05, Loss_tot:0.1444,\n",
      "save model\n",
      "Save model 4445\n",
      "Train Epoch:4446 learning rate:1.0000e-05, Loss_tot:0.1444,\n",
      "save model\n",
      "Save model 4446\n",
      "Train Epoch:4447 learning rate:1.0000e-05, Loss_tot:0.1444,\n",
      "save model\n",
      "Save model 4447\n",
      "Train Epoch:4448 learning rate:1.0000e-05, Loss_tot:0.1443,\n",
      "save model\n",
      "Save model 4448\n",
      "Train Epoch:4449 learning rate:1.0000e-05, Loss_tot:0.1443,\n",
      "save model\n",
      "Save model 4449\n",
      "Train Epoch:4450 learning rate:1.0000e-05, Loss_tot:0.1443,\n",
      "Save model 4450\n",
      "Train Epoch:4451 learning rate:1.0000e-05, Loss_tot:0.1443,\n",
      "Save model 4451\n",
      "Train Epoch:4452 learning rate:1.0000e-05, Loss_tot:0.1442,\n",
      "save model\n",
      "Save model 4452\n",
      "Train Epoch:4453 learning rate:1.0000e-05, Loss_tot:0.1442,\n",
      "save model\n",
      "Save model 4453\n",
      "Train Epoch:4454 learning rate:1.0000e-05, Loss_tot:0.1442,\n",
      "save model\n",
      "Save model 4454\n",
      "Train Epoch:4455 learning rate:1.0000e-05, Loss_tot:0.1441,\n",
      "save model\n",
      "Save model 4455\n",
      "Train Epoch:4456 learning rate:1.0000e-05, Loss_tot:0.1441,\n",
      "save model\n",
      "Save model 4456\n",
      "Train Epoch:4457 learning rate:1.0000e-05, Loss_tot:0.1440,\n",
      "save model\n",
      "Save model 4457\n",
      "Train Epoch:4458 learning rate:1.0000e-05, Loss_tot:0.1441,\n",
      "Save model 4458\n",
      "Train Epoch:4459 learning rate:1.0000e-05, Loss_tot:0.1440,\n",
      "save model\n",
      "Save model 4459\n",
      "Train Epoch:4460 learning rate:1.0000e-05, Loss_tot:0.1440,\n",
      "save model\n",
      "Save model 4460\n",
      "Train Epoch:4461 learning rate:1.0000e-05, Loss_tot:0.1439,\n",
      "save model\n",
      "Save model 4461\n",
      "Train Epoch:4462 learning rate:1.0000e-05, Loss_tot:0.1439,\n",
      "save model\n",
      "Save model 4462\n",
      "Train Epoch:4463 learning rate:1.0000e-05, Loss_tot:0.1439,\n",
      "save model\n",
      "Save model 4463\n",
      "Train Epoch:4464 learning rate:1.0000e-05, Loss_tot:0.1438,\n",
      "save model\n",
      "Save model 4464\n",
      "Train Epoch:4465 learning rate:1.0000e-05, Loss_tot:0.1437,\n",
      "save model\n",
      "Save model 4465\n",
      "Train Epoch:4466 learning rate:1.0000e-05, Loss_tot:0.1438,\n",
      "Save model 4466\n",
      "Train Epoch:4467 learning rate:1.0000e-05, Loss_tot:0.1437,\n",
      "save model\n",
      "Save model 4467\n",
      "Train Epoch:4468 learning rate:1.0000e-05, Loss_tot:0.1437,\n",
      "save model\n",
      "Save model 4468\n",
      "Train Epoch:4469 learning rate:1.0000e-05, Loss_tot:0.1436,\n",
      "save model\n",
      "Save model 4469\n",
      "Train Epoch:4470 learning rate:1.0000e-05, Loss_tot:0.1436,\n",
      "save model\n",
      "Save model 4470\n",
      "Train Epoch:4471 learning rate:1.0000e-05, Loss_tot:0.1436,\n",
      "save model\n",
      "Save model 4471\n",
      "Train Epoch:4472 learning rate:1.0000e-05, Loss_tot:0.1435,\n",
      "save model\n",
      "Save model 4472\n",
      "Train Epoch:4473 learning rate:1.0000e-05, Loss_tot:0.1435,\n",
      "save model\n",
      "Save model 4473\n",
      "Train Epoch:4474 learning rate:1.0000e-05, Loss_tot:0.1434,\n",
      "save model\n",
      "Save model 4474\n",
      "Train Epoch:4475 learning rate:1.0000e-05, Loss_tot:0.1435,\n",
      "Save model 4475\n",
      "Train Epoch:4476 learning rate:1.0000e-05, Loss_tot:0.1434,\n",
      "save model\n",
      "Save model 4476\n",
      "Train Epoch:4477 learning rate:1.0000e-05, Loss_tot:0.1434,\n",
      "save model\n",
      "Save model 4477\n",
      "Train Epoch:4478 learning rate:1.0000e-05, Loss_tot:0.1434,\n",
      "save model\n",
      "Save model 4478\n",
      "Train Epoch:4479 learning rate:1.0000e-05, Loss_tot:0.1433,\n",
      "save model\n",
      "Save model 4479\n",
      "Train Epoch:4480 learning rate:1.0000e-05, Loss_tot:0.1433,\n",
      "save model\n",
      "Save model 4480\n",
      "Train Epoch:4481 learning rate:1.0000e-05, Loss_tot:0.1432,\n",
      "save model\n",
      "Save model 4481\n",
      "Train Epoch:4482 learning rate:1.0000e-05, Loss_tot:0.1432,\n",
      "save model\n",
      "Save model 4482\n",
      "Train Epoch:4483 learning rate:1.0000e-05, Loss_tot:0.1432,\n",
      "Save model 4483\n",
      "Train Epoch:4484 learning rate:1.0000e-05, Loss_tot:0.1432,\n",
      "save model\n",
      "Save model 4484\n",
      "Train Epoch:4485 learning rate:1.0000e-05, Loss_tot:0.1431,\n",
      "save model\n",
      "Save model 4485\n",
      "Train Epoch:4486 learning rate:1.0000e-05, Loss_tot:0.1431,\n",
      "save model\n",
      "Save model 4486\n",
      "Train Epoch:4487 learning rate:1.0000e-05, Loss_tot:0.1431,\n",
      "Save model 4487\n",
      "Train Epoch:4488 learning rate:1.0000e-05, Loss_tot:0.1430,\n",
      "save model\n",
      "Save model 4488\n",
      "Train Epoch:4489 learning rate:1.0000e-05, Loss_tot:0.1429,\n",
      "save model\n",
      "Save model 4489\n",
      "Train Epoch:4490 learning rate:1.0000e-05, Loss_tot:0.1429,\n",
      "save model\n",
      "Save model 4490\n",
      "Train Epoch:4491 learning rate:1.0000e-05, Loss_tot:0.1429,\n",
      "save model\n",
      "Save model 4491\n",
      "Train Epoch:4492 learning rate:1.0000e-05, Loss_tot:0.1429,\n",
      "Save model 4492\n",
      "Train Epoch:4493 learning rate:1.0000e-05, Loss_tot:0.1428,\n",
      "save model\n",
      "Save model 4493\n",
      "Train Epoch:4494 learning rate:1.0000e-05, Loss_tot:0.1428,\n",
      "save model\n",
      "Save model 4494\n",
      "Train Epoch:4495 learning rate:1.0000e-05, Loss_tot:0.1428,\n",
      "save model\n",
      "Save model 4495\n",
      "Train Epoch:4496 learning rate:1.0000e-05, Loss_tot:0.1428,\n",
      "save model\n",
      "Save model 4496\n",
      "Train Epoch:4497 learning rate:1.0000e-05, Loss_tot:0.1427,\n",
      "save model\n",
      "Save model 4497\n",
      "Train Epoch:4498 learning rate:1.0000e-05, Loss_tot:0.1426,\n",
      "save model\n",
      "Save model 4498\n",
      "Train Epoch:4499 learning rate:1.0000e-05, Loss_tot:0.1426,\n",
      "save model\n",
      "Save model 4499\n",
      "Train Epoch:4500 learning rate:1.0000e-05, Loss_tot:0.1426,\n",
      "Save model 4500\n",
      "Train Epoch:4501 learning rate:1.0000e-05, Loss_tot:0.1426,\n",
      "save model\n",
      "Save model 4501\n",
      "Train Epoch:4502 learning rate:1.0000e-05, Loss_tot:0.1425,\n",
      "save model\n",
      "Save model 4502\n",
      "Train Epoch:4503 learning rate:1.0000e-05, Loss_tot:0.1425,\n",
      "save model\n",
      "Save model 4503\n",
      "Train Epoch:4504 learning rate:1.0000e-05, Loss_tot:0.1425,\n",
      "Save model 4504\n",
      "Train Epoch:4505 learning rate:1.0000e-05, Loss_tot:0.1425,\n",
      "save model\n",
      "Save model 4505\n",
      "Train Epoch:4506 learning rate:1.0000e-05, Loss_tot:0.1424,\n",
      "save model\n",
      "Save model 4506\n",
      "Train Epoch:4507 learning rate:1.0000e-05, Loss_tot:0.1423,\n",
      "save model\n",
      "Save model 4507\n",
      "Train Epoch:4508 learning rate:1.0000e-05, Loss_tot:0.1423,\n",
      "save model\n",
      "Save model 4508\n",
      "Train Epoch:4509 learning rate:1.0000e-05, Loss_tot:0.1423,\n",
      "Save model 4509\n",
      "Train Epoch:4510 learning rate:1.0000e-05, Loss_tot:0.1423,\n",
      "save model\n",
      "Save model 4510\n",
      "Train Epoch:4511 learning rate:1.0000e-05, Loss_tot:0.1422,\n",
      "save model\n",
      "Save model 4511\n",
      "Train Epoch:4512 learning rate:1.0000e-05, Loss_tot:0.1422,\n",
      "save model\n",
      "Save model 4512\n",
      "Train Epoch:4513 learning rate:1.0000e-05, Loss_tot:0.1422,\n",
      "save model\n",
      "Save model 4513\n",
      "Train Epoch:4514 learning rate:1.0000e-05, Loss_tot:0.1421,\n",
      "save model\n",
      "Save model 4514\n",
      "Train Epoch:4515 learning rate:1.0000e-05, Loss_tot:0.1421,\n",
      "save model\n",
      "Save model 4515\n",
      "Train Epoch:4516 learning rate:1.0000e-05, Loss_tot:0.1420,\n",
      "save model\n",
      "Save model 4516\n",
      "Train Epoch:4517 learning rate:1.0000e-05, Loss_tot:0.1420,\n",
      "save model\n",
      "Save model 4517\n",
      "Train Epoch:4518 learning rate:1.0000e-05, Loss_tot:0.1420,\n",
      "save model\n",
      "Save model 4518\n",
      "Train Epoch:4519 learning rate:1.0000e-05, Loss_tot:0.1420,\n",
      "save model\n",
      "Save model 4519\n",
      "Train Epoch:4520 learning rate:1.0000e-05, Loss_tot:0.1419,\n",
      "save model\n",
      "Save model 4520\n",
      "Train Epoch:4521 learning rate:1.0000e-05, Loss_tot:0.1419,\n",
      "Save model 4521\n",
      "Train Epoch:4522 learning rate:1.0000e-05, Loss_tot:0.1419,\n",
      "save model\n",
      "Save model 4522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4523 learning rate:1.0000e-05, Loss_tot:0.1418,\n",
      "save model\n",
      "Save model 4523\n",
      "Train Epoch:4524 learning rate:1.0000e-05, Loss_tot:0.1418,\n",
      "save model\n",
      "Save model 4524\n",
      "Train Epoch:4525 learning rate:1.0000e-05, Loss_tot:0.1417,\n",
      "save model\n",
      "Save model 4525\n",
      "Train Epoch:4526 learning rate:1.0000e-05, Loss_tot:0.1417,\n",
      "Save model 4526\n",
      "Train Epoch:4527 learning rate:1.0000e-05, Loss_tot:0.1417,\n",
      "save model\n",
      "Save model 4527\n",
      "Train Epoch:4528 learning rate:1.0000e-05, Loss_tot:0.1416,\n",
      "save model\n",
      "Save model 4528\n",
      "Train Epoch:4529 learning rate:1.0000e-05, Loss_tot:0.1416,\n",
      "save model\n",
      "Save model 4529\n",
      "Train Epoch:4530 learning rate:1.0000e-05, Loss_tot:0.1416,\n",
      "save model\n",
      "Save model 4530\n",
      "Train Epoch:4531 learning rate:1.0000e-05, Loss_tot:0.1415,\n",
      "save model\n",
      "Save model 4531\n",
      "Train Epoch:4532 learning rate:1.0000e-05, Loss_tot:0.1415,\n",
      "save model\n",
      "Save model 4532\n",
      "Train Epoch:4533 learning rate:1.0000e-05, Loss_tot:0.1415,\n",
      "save model\n",
      "Save model 4533\n",
      "Train Epoch:4534 learning rate:1.0000e-05, Loss_tot:0.1414,\n",
      "save model\n",
      "Save model 4534\n",
      "Train Epoch:4535 learning rate:1.0000e-05, Loss_tot:0.1414,\n",
      "save model\n",
      "Save model 4535\n",
      "Train Epoch:4536 learning rate:1.0000e-05, Loss_tot:0.1414,\n",
      "save model\n",
      "Save model 4536\n",
      "Train Epoch:4537 learning rate:1.0000e-05, Loss_tot:0.1413,\n",
      "save model\n",
      "Save model 4537\n",
      "Train Epoch:4538 learning rate:1.0000e-05, Loss_tot:0.1414,\n",
      "Save model 4538\n",
      "Train Epoch:4539 learning rate:1.0000e-05, Loss_tot:0.1413,\n",
      "save model\n",
      "Save model 4539\n",
      "Train Epoch:4540 learning rate:1.0000e-05, Loss_tot:0.1412,\n",
      "save model\n",
      "Save model 4540\n",
      "Train Epoch:4541 learning rate:1.0000e-05, Loss_tot:0.1412,\n",
      "save model\n",
      "Save model 4541\n",
      "Train Epoch:4542 learning rate:1.0000e-05, Loss_tot:0.1411,\n",
      "save model\n",
      "Save model 4542\n",
      "Train Epoch:4543 learning rate:1.0000e-05, Loss_tot:0.1412,\n",
      "Save model 4543\n",
      "Train Epoch:4544 learning rate:1.0000e-05, Loss_tot:0.1411,\n",
      "save model\n",
      "Save model 4544\n",
      "Train Epoch:4545 learning rate:1.0000e-05, Loss_tot:0.1411,\n",
      "save model\n",
      "Save model 4545\n",
      "Train Epoch:4546 learning rate:1.0000e-05, Loss_tot:0.1411,\n",
      "save model\n",
      "Save model 4546\n",
      "Train Epoch:4547 learning rate:1.0000e-05, Loss_tot:0.1410,\n",
      "save model\n",
      "Save model 4547\n",
      "Train Epoch:4548 learning rate:1.0000e-05, Loss_tot:0.1410,\n",
      "save model\n",
      "Save model 4548\n",
      "Train Epoch:4549 learning rate:1.0000e-05, Loss_tot:0.1409,\n",
      "save model\n",
      "Save model 4549\n",
      "Train Epoch:4550 learning rate:1.0000e-05, Loss_tot:0.1409,\n",
      "save model\n",
      "Save model 4550\n",
      "Train Epoch:4551 learning rate:1.0000e-05, Loss_tot:0.1409,\n",
      "save model\n",
      "Save model 4551\n",
      "Train Epoch:4552 learning rate:1.0000e-05, Loss_tot:0.1409,\n",
      "save model\n",
      "Save model 4552\n",
      "Train Epoch:4553 learning rate:1.0000e-05, Loss_tot:0.1408,\n",
      "save model\n",
      "Save model 4553\n",
      "Train Epoch:4554 learning rate:1.0000e-05, Loss_tot:0.1408,\n",
      "save model\n",
      "Save model 4554\n",
      "Train Epoch:4555 learning rate:1.0000e-05, Loss_tot:0.1408,\n",
      "Save model 4555\n",
      "Train Epoch:4556 learning rate:1.0000e-05, Loss_tot:0.1408,\n",
      "Save model 4556\n",
      "Train Epoch:4557 learning rate:1.0000e-05, Loss_tot:0.1406,\n",
      "save model\n",
      "Save model 4557\n",
      "Train Epoch:4558 learning rate:1.0000e-05, Loss_tot:0.1407,\n",
      "Save model 4558\n",
      "Train Epoch:4559 learning rate:1.0000e-05, Loss_tot:0.1407,\n",
      "Save model 4559\n",
      "Train Epoch:4560 learning rate:1.0000e-05, Loss_tot:0.1407,\n",
      "Save model 4560\n",
      "Train Epoch:4561 learning rate:1.0000e-05, Loss_tot:0.1406,\n",
      "Save model 4561\n",
      "Train Epoch:4562 learning rate:1.0000e-05, Loss_tot:0.1406,\n",
      "save model\n",
      "Save model 4562\n",
      "Train Epoch:4563 learning rate:1.0000e-05, Loss_tot:0.1405,\n",
      "save model\n",
      "Save model 4563\n",
      "Train Epoch:4564 learning rate:1.0000e-05, Loss_tot:0.1404,\n",
      "save model\n",
      "Save model 4564\n",
      "Train Epoch:4565 learning rate:1.0000e-05, Loss_tot:0.1404,\n",
      "Save model 4565\n",
      "Train Epoch:4566 learning rate:1.0000e-05, Loss_tot:0.1403,\n",
      "save model\n",
      "Save model 4566\n",
      "Train Epoch:4567 learning rate:1.0000e-05, Loss_tot:0.1403,\n",
      "save model\n",
      "Save model 4567\n",
      "Train Epoch:4568 learning rate:1.0000e-05, Loss_tot:0.1403,\n",
      "save model\n",
      "Save model 4568\n",
      "Train Epoch:4569 learning rate:1.0000e-05, Loss_tot:0.1402,\n",
      "save model\n",
      "Save model 4569\n",
      "Train Epoch:4570 learning rate:1.0000e-05, Loss_tot:0.1402,\n",
      "save model\n",
      "Save model 4570\n",
      "Train Epoch:4571 learning rate:1.0000e-05, Loss_tot:0.1402,\n",
      "save model\n",
      "Save model 4571\n",
      "Train Epoch:4572 learning rate:1.0000e-05, Loss_tot:0.1401,\n",
      "save model\n",
      "Save model 4572\n",
      "Train Epoch:4573 learning rate:1.0000e-05, Loss_tot:0.1401,\n",
      "save model\n",
      "Save model 4573\n",
      "Train Epoch:4574 learning rate:1.0000e-05, Loss_tot:0.1401,\n",
      "save model\n",
      "Save model 4574\n",
      "Train Epoch:4575 learning rate:1.0000e-05, Loss_tot:0.1401,\n",
      "save model\n",
      "Save model 4575\n",
      "Train Epoch:4576 learning rate:1.0000e-05, Loss_tot:0.1400,\n",
      "save model\n",
      "Save model 4576\n",
      "Train Epoch:4577 learning rate:1.0000e-05, Loss_tot:0.1400,\n",
      "save model\n",
      "Save model 4577\n",
      "Train Epoch:4578 learning rate:1.0000e-05, Loss_tot:0.1399,\n",
      "save model\n",
      "Save model 4578\n",
      "Train Epoch:4579 learning rate:1.0000e-05, Loss_tot:0.1399,\n",
      "save model\n",
      "Save model 4579\n",
      "Train Epoch:4580 learning rate:1.0000e-05, Loss_tot:0.1399,\n",
      "save model\n",
      "Save model 4580\n",
      "Train Epoch:4581 learning rate:1.0000e-05, Loss_tot:0.1398,\n",
      "save model\n",
      "Save model 4581\n",
      "Train Epoch:4582 learning rate:1.0000e-05, Loss_tot:0.1398,\n",
      "save model\n",
      "Save model 4582\n",
      "Train Epoch:4583 learning rate:1.0000e-05, Loss_tot:0.1398,\n",
      "Save model 4583\n",
      "Train Epoch:4584 learning rate:1.0000e-05, Loss_tot:0.1398,\n",
      "save model\n",
      "Save model 4584\n",
      "Train Epoch:4585 learning rate:1.0000e-05, Loss_tot:0.1397,\n",
      "save model\n",
      "Save model 4585\n",
      "Train Epoch:4586 learning rate:1.0000e-05, Loss_tot:0.1397,\n",
      "save model\n",
      "Save model 4586\n",
      "Train Epoch:4587 learning rate:1.0000e-05, Loss_tot:0.1396,\n",
      "save model\n",
      "Save model 4587\n",
      "Train Epoch:4588 learning rate:1.0000e-05, Loss_tot:0.1396,\n",
      "save model\n",
      "Save model 4588\n",
      "Train Epoch:4589 learning rate:1.0000e-05, Loss_tot:0.1396,\n",
      "Save model 4589\n",
      "Train Epoch:4590 learning rate:1.0000e-05, Loss_tot:0.1396,\n",
      "Save model 4590\n",
      "Train Epoch:4591 learning rate:1.0000e-05, Loss_tot:0.1395,\n",
      "save model\n",
      "Save model 4591\n",
      "Train Epoch:4592 learning rate:1.0000e-05, Loss_tot:0.1395,\n",
      "Save model 4592\n",
      "Train Epoch:4593 learning rate:1.0000e-05, Loss_tot:0.1395,\n",
      "Save model 4593\n",
      "Train Epoch:4594 learning rate:1.0000e-05, Loss_tot:0.1395,\n",
      "Save model 4594\n",
      "Train Epoch:4595 learning rate:1.0000e-05, Loss_tot:0.1395,\n",
      "Save model 4595\n",
      "Train Epoch:4596 learning rate:1.0000e-05, Loss_tot:0.1394,\n",
      "save model\n",
      "Save model 4596\n",
      "Train Epoch:4597 learning rate:1.0000e-05, Loss_tot:0.1393,\n",
      "save model\n",
      "Save model 4597\n",
      "Train Epoch:4598 learning rate:1.0000e-05, Loss_tot:0.1392,\n",
      "save model\n",
      "Save model 4598\n",
      "Train Epoch:4599 learning rate:1.0000e-05, Loss_tot:0.1394,\n",
      "Save model 4599\n",
      "Train Epoch:4600 learning rate:1.0000e-05, Loss_tot:0.1394,\n",
      "Save model 4600\n",
      "Train Epoch:4601 learning rate:1.0000e-05, Loss_tot:0.1393,\n",
      "Save model 4601\n",
      "Train Epoch:4602 learning rate:1.0000e-05, Loss_tot:0.1392,\n",
      "save model\n",
      "Save model 4602\n",
      "Train Epoch:4603 learning rate:1.0000e-05, Loss_tot:0.1391,\n",
      "save model\n",
      "Save model 4603\n",
      "Train Epoch:4604 learning rate:1.0000e-05, Loss_tot:0.1391,\n",
      "Save model 4604\n",
      "Train Epoch:4605 learning rate:1.0000e-05, Loss_tot:0.1391,\n",
      "Save model 4605\n",
      "Train Epoch:4606 learning rate:1.0000e-05, Loss_tot:0.1391,\n",
      "Save model 4606\n",
      "Train Epoch:4607 learning rate:1.0000e-05, Loss_tot:0.1391,\n",
      "save model\n",
      "Save model 4607\n",
      "Train Epoch:4608 learning rate:1.0000e-05, Loss_tot:0.1390,\n",
      "save model\n",
      "Save model 4608\n",
      "Train Epoch:4609 learning rate:1.0000e-05, Loss_tot:0.1389,\n",
      "save model\n",
      "Save model 4609\n",
      "Train Epoch:4610 learning rate:1.0000e-05, Loss_tot:0.1389,\n",
      "Save model 4610\n",
      "Train Epoch:4611 learning rate:1.0000e-05, Loss_tot:0.1389,\n",
      "Save model 4611\n",
      "Train Epoch:4612 learning rate:1.0000e-05, Loss_tot:0.1389,\n",
      "save model\n",
      "Save model 4612\n",
      "Train Epoch:4613 learning rate:1.0000e-05, Loss_tot:0.1387,\n",
      "save model\n",
      "Save model 4613\n",
      "Train Epoch:4614 learning rate:1.0000e-05, Loss_tot:0.1387,\n",
      "save model\n",
      "Save model 4614\n",
      "Train Epoch:4615 learning rate:1.0000e-05, Loss_tot:0.1387,\n",
      "save model\n",
      "Save model 4615\n",
      "Train Epoch:4616 learning rate:1.0000e-05, Loss_tot:0.1386,\n",
      "save model\n",
      "Save model 4616\n",
      "Train Epoch:4617 learning rate:1.0000e-05, Loss_tot:0.1386,\n",
      "save model\n",
      "Save model 4617\n",
      "Train Epoch:4618 learning rate:1.0000e-05, Loss_tot:0.1386,\n",
      "save model\n",
      "Save model 4618\n",
      "Train Epoch:4619 learning rate:1.0000e-05, Loss_tot:0.1385,\n",
      "save model\n",
      "Save model 4619\n",
      "Train Epoch:4620 learning rate:1.0000e-05, Loss_tot:0.1385,\n",
      "save model\n",
      "Save model 4620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4621 learning rate:1.0000e-05, Loss_tot:0.1385,\n",
      "save model\n",
      "Save model 4621\n",
      "Train Epoch:4622 learning rate:1.0000e-05, Loss_tot:0.1384,\n",
      "save model\n",
      "Save model 4622\n",
      "Train Epoch:4623 learning rate:1.0000e-05, Loss_tot:0.1384,\n",
      "Save model 4623\n",
      "Train Epoch:4624 learning rate:1.0000e-05, Loss_tot:0.1384,\n",
      "save model\n",
      "Save model 4624\n",
      "Train Epoch:4625 learning rate:1.0000e-05, Loss_tot:0.1383,\n",
      "save model\n",
      "Save model 4625\n",
      "Train Epoch:4626 learning rate:1.0000e-05, Loss_tot:0.1383,\n",
      "save model\n",
      "Save model 4626\n",
      "Train Epoch:4627 learning rate:1.0000e-05, Loss_tot:0.1382,\n",
      "save model\n",
      "Save model 4627\n",
      "Train Epoch:4628 learning rate:1.0000e-05, Loss_tot:0.1382,\n",
      "save model\n",
      "Save model 4628\n",
      "Train Epoch:4629 learning rate:1.0000e-05, Loss_tot:0.1382,\n",
      "save model\n",
      "Save model 4629\n",
      "Train Epoch:4630 learning rate:1.0000e-05, Loss_tot:0.1382,\n",
      "save model\n",
      "Save model 4630\n",
      "Train Epoch:4631 learning rate:1.0000e-05, Loss_tot:0.1382,\n",
      "save model\n",
      "Save model 4631\n",
      "Train Epoch:4632 learning rate:1.0000e-05, Loss_tot:0.1381,\n",
      "save model\n",
      "Save model 4632\n",
      "Train Epoch:4633 learning rate:1.0000e-05, Loss_tot:0.1381,\n",
      "save model\n",
      "Save model 4633\n",
      "Train Epoch:4634 learning rate:1.0000e-05, Loss_tot:0.1380,\n",
      "save model\n",
      "Save model 4634\n",
      "Train Epoch:4635 learning rate:1.0000e-05, Loss_tot:0.1381,\n",
      "Save model 4635\n",
      "Train Epoch:4636 learning rate:1.0000e-05, Loss_tot:0.1381,\n",
      "Save model 4636\n",
      "Train Epoch:4637 learning rate:1.0000e-05, Loss_tot:0.1380,\n",
      "save model\n",
      "Save model 4637\n",
      "Train Epoch:4638 learning rate:1.0000e-05, Loss_tot:0.1379,\n",
      "save model\n",
      "Save model 4638\n",
      "Train Epoch:4639 learning rate:1.0000e-05, Loss_tot:0.1379,\n",
      "Save model 4639\n",
      "Train Epoch:4640 learning rate:1.0000e-05, Loss_tot:0.1379,\n",
      "save model\n",
      "Save model 4640\n",
      "Train Epoch:4641 learning rate:1.0000e-05, Loss_tot:0.1378,\n",
      "save model\n",
      "Save model 4641\n",
      "Train Epoch:4642 learning rate:1.0000e-05, Loss_tot:0.1378,\n",
      "save model\n",
      "Save model 4642\n",
      "Train Epoch:4643 learning rate:1.0000e-05, Loss_tot:0.1378,\n",
      "Save model 4643\n",
      "Train Epoch:4644 learning rate:1.0000e-05, Loss_tot:0.1377,\n",
      "save model\n",
      "Save model 4644\n",
      "Train Epoch:4645 learning rate:1.0000e-05, Loss_tot:0.1376,\n",
      "save model\n",
      "Save model 4645\n",
      "Train Epoch:4646 learning rate:1.0000e-05, Loss_tot:0.1376,\n",
      "save model\n",
      "Save model 4646\n",
      "Train Epoch:4647 learning rate:1.0000e-05, Loss_tot:0.1376,\n",
      "save model\n",
      "Save model 4647\n",
      "Train Epoch:4648 learning rate:1.0000e-05, Loss_tot:0.1376,\n",
      "save model\n",
      "Save model 4648\n",
      "Train Epoch:4649 learning rate:1.0000e-05, Loss_tot:0.1375,\n",
      "save model\n",
      "Save model 4649\n",
      "Train Epoch:4650 learning rate:1.0000e-05, Loss_tot:0.1375,\n",
      "save model\n",
      "Save model 4650\n",
      "Train Epoch:4651 learning rate:1.0000e-05, Loss_tot:0.1375,\n",
      "Save model 4651\n",
      "Train Epoch:4652 learning rate:1.0000e-05, Loss_tot:0.1375,\n",
      "save model\n",
      "Save model 4652\n",
      "Train Epoch:4653 learning rate:1.0000e-05, Loss_tot:0.1374,\n",
      "save model\n",
      "Save model 4653\n",
      "Train Epoch:4654 learning rate:1.0000e-05, Loss_tot:0.1374,\n",
      "save model\n",
      "Save model 4654\n",
      "Train Epoch:4655 learning rate:1.0000e-05, Loss_tot:0.1373,\n",
      "save model\n",
      "Save model 4655\n",
      "Train Epoch:4656 learning rate:1.0000e-05, Loss_tot:0.1373,\n",
      "save model\n",
      "Save model 4656\n",
      "Train Epoch:4657 learning rate:1.0000e-05, Loss_tot:0.1373,\n",
      "Save model 4657\n",
      "Train Epoch:4658 learning rate:1.0000e-05, Loss_tot:0.1373,\n",
      "Save model 4658\n",
      "Train Epoch:4659 learning rate:1.0000e-05, Loss_tot:0.1372,\n",
      "save model\n",
      "Save model 4659\n",
      "Train Epoch:4660 learning rate:1.0000e-05, Loss_tot:0.1372,\n",
      "Save model 4660\n",
      "Train Epoch:4661 learning rate:1.0000e-05, Loss_tot:0.1372,\n",
      "Save model 4661\n",
      "Train Epoch:4662 learning rate:1.0000e-05, Loss_tot:0.1372,\n",
      "Save model 4662\n",
      "Train Epoch:4663 learning rate:1.0000e-05, Loss_tot:0.1372,\n",
      "Save model 4663\n",
      "Train Epoch:4664 learning rate:1.0000e-05, Loss_tot:0.1371,\n",
      "save model\n",
      "Save model 4664\n",
      "Train Epoch:4665 learning rate:1.0000e-05, Loss_tot:0.1370,\n",
      "save model\n",
      "Save model 4665\n",
      "Train Epoch:4666 learning rate:1.0000e-05, Loss_tot:0.1369,\n",
      "save model\n",
      "Save model 4666\n",
      "Train Epoch:4667 learning rate:1.0000e-05, Loss_tot:0.1371,\n",
      "Save model 4667\n",
      "Train Epoch:4668 learning rate:1.0000e-05, Loss_tot:0.1371,\n",
      "Save model 4668\n",
      "Train Epoch:4669 learning rate:1.0000e-05, Loss_tot:0.1370,\n",
      "Save model 4669\n",
      "Train Epoch:4670 learning rate:1.0000e-05, Loss_tot:0.1369,\n",
      "save model\n",
      "Save model 4670\n",
      "Train Epoch:4671 learning rate:1.0000e-05, Loss_tot:0.1368,\n",
      "save model\n",
      "Save model 4671\n",
      "Train Epoch:4672 learning rate:1.0000e-05, Loss_tot:0.1368,\n",
      "Save model 4672\n",
      "Train Epoch:4673 learning rate:1.0000e-05, Loss_tot:0.1368,\n",
      "Save model 4673\n",
      "Train Epoch:4674 learning rate:1.0000e-05, Loss_tot:0.1368,\n",
      "Save model 4674\n",
      "Train Epoch:4675 learning rate:1.0000e-05, Loss_tot:0.1368,\n",
      "save model\n",
      "Save model 4675\n",
      "Train Epoch:4676 learning rate:1.0000e-05, Loss_tot:0.1367,\n",
      "save model\n",
      "Save model 4676\n",
      "Train Epoch:4677 learning rate:1.0000e-05, Loss_tot:0.1366,\n",
      "save model\n",
      "Save model 4677\n",
      "Train Epoch:4678 learning rate:1.0000e-05, Loss_tot:0.1367,\n",
      "Save model 4678\n",
      "Train Epoch:4679 learning rate:1.0000e-05, Loss_tot:0.1367,\n",
      "Save model 4679\n",
      "Train Epoch:4680 learning rate:1.0000e-05, Loss_tot:0.1366,\n",
      "Save model 4680\n",
      "Train Epoch:4681 learning rate:1.0000e-05, Loss_tot:0.1365,\n",
      "save model\n",
      "Save model 4681\n",
      "Train Epoch:4682 learning rate:1.0000e-05, Loss_tot:0.1365,\n",
      "Save model 4682\n",
      "Train Epoch:4683 learning rate:1.0000e-05, Loss_tot:0.1365,\n",
      "Save model 4683\n",
      "Train Epoch:4684 learning rate:1.0000e-05, Loss_tot:0.1365,\n",
      "Save model 4684\n",
      "Train Epoch:4685 learning rate:1.0000e-05, Loss_tot:0.1365,\n",
      "Save model 4685\n",
      "Train Epoch:4686 learning rate:1.0000e-05, Loss_tot:0.1364,\n",
      "save model\n",
      "Save model 4686\n",
      "Train Epoch:4687 learning rate:1.0000e-05, Loss_tot:0.1364,\n",
      "save model\n",
      "Save model 4687\n",
      "Train Epoch:4688 learning rate:1.0000e-05, Loss_tot:0.1363,\n",
      "save model\n",
      "Save model 4688\n",
      "Train Epoch:4689 learning rate:1.0000e-05, Loss_tot:0.1362,\n",
      "save model\n",
      "Save model 4689\n",
      "Train Epoch:4690 learning rate:1.0000e-05, Loss_tot:0.1362,\n",
      "Save model 4690\n",
      "Train Epoch:4691 learning rate:1.0000e-05, Loss_tot:0.1361,\n",
      "save model\n",
      "Save model 4691\n",
      "Train Epoch:4692 learning rate:1.0000e-05, Loss_tot:0.1361,\n",
      "save model\n",
      "Save model 4692\n",
      "Train Epoch:4693 learning rate:1.0000e-05, Loss_tot:0.1361,\n",
      "save model\n",
      "Save model 4693\n",
      "Train Epoch:4694 learning rate:1.0000e-05, Loss_tot:0.1361,\n",
      "save model\n",
      "Save model 4694\n",
      "Train Epoch:4695 learning rate:1.0000e-05, Loss_tot:0.1360,\n",
      "save model\n",
      "Save model 4695\n",
      "Train Epoch:4696 learning rate:1.0000e-05, Loss_tot:0.1359,\n",
      "save model\n",
      "Save model 4696\n",
      "Train Epoch:4697 learning rate:1.0000e-05, Loss_tot:0.1360,\n",
      "Save model 4697\n",
      "Train Epoch:4698 learning rate:1.0000e-05, Loss_tot:0.1360,\n",
      "Save model 4698\n",
      "Train Epoch:4699 learning rate:1.0000e-05, Loss_tot:0.1359,\n",
      "save model\n",
      "Save model 4699\n",
      "Train Epoch:4700 learning rate:1.0000e-05, Loss_tot:0.1358,\n",
      "save model\n",
      "Save model 4700\n",
      "Train Epoch:4701 learning rate:1.0000e-05, Loss_tot:0.1359,\n",
      "Save model 4701\n",
      "Train Epoch:4702 learning rate:1.0000e-05, Loss_tot:0.1358,\n",
      "save model\n",
      "Save model 4702\n",
      "Train Epoch:4703 learning rate:1.0000e-05, Loss_tot:0.1358,\n",
      "save model\n",
      "Save model 4703\n",
      "Train Epoch:4704 learning rate:1.0000e-05, Loss_tot:0.1357,\n",
      "save model\n",
      "Save model 4704\n",
      "Train Epoch:4705 learning rate:1.0000e-05, Loss_tot:0.1356,\n",
      "save model\n",
      "Save model 4705\n",
      "Train Epoch:4706 learning rate:1.0000e-05, Loss_tot:0.1357,\n",
      "Save model 4706\n",
      "Train Epoch:4707 learning rate:1.0000e-05, Loss_tot:0.1357,\n",
      "Save model 4707\n",
      "Train Epoch:4708 learning rate:1.0000e-05, Loss_tot:0.1356,\n",
      "save model\n",
      "Save model 4708\n",
      "Train Epoch:4709 learning rate:1.0000e-05, Loss_tot:0.1355,\n",
      "save model\n",
      "Save model 4709\n",
      "Train Epoch:4710 learning rate:1.0000e-05, Loss_tot:0.1355,\n",
      "Save model 4710\n",
      "Train Epoch:4711 learning rate:1.0000e-05, Loss_tot:0.1355,\n",
      "save model\n",
      "Save model 4711\n",
      "Train Epoch:4712 learning rate:1.0000e-05, Loss_tot:0.1354,\n",
      "save model\n",
      "Save model 4712\n",
      "Train Epoch:4713 learning rate:1.0000e-05, Loss_tot:0.1353,\n",
      "save model\n",
      "Save model 4713\n",
      "Train Epoch:4714 learning rate:1.0000e-05, Loss_tot:0.1354,\n",
      "Save model 4714\n",
      "Train Epoch:4715 learning rate:1.0000e-05, Loss_tot:0.1354,\n",
      "Save model 4715\n",
      "Train Epoch:4716 learning rate:1.0000e-05, Loss_tot:0.1353,\n",
      "save model\n",
      "Save model 4716\n",
      "Train Epoch:4717 learning rate:1.0000e-05, Loss_tot:0.1353,\n",
      "save model\n",
      "Save model 4717\n",
      "Train Epoch:4718 learning rate:1.0000e-05, Loss_tot:0.1353,\n",
      "Save model 4718\n",
      "Train Epoch:4719 learning rate:1.0000e-05, Loss_tot:0.1353,\n",
      "save model\n",
      "Save model 4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4720 learning rate:1.0000e-05, Loss_tot:0.1352,\n",
      "save model\n",
      "Save model 4720\n",
      "Train Epoch:4721 learning rate:1.0000e-05, Loss_tot:0.1351,\n",
      "save model\n",
      "Save model 4721\n",
      "Train Epoch:4722 learning rate:1.0000e-05, Loss_tot:0.1350,\n",
      "save model\n",
      "Save model 4722\n",
      "Train Epoch:4723 learning rate:1.0000e-05, Loss_tot:0.1351,\n",
      "Save model 4723\n",
      "Train Epoch:4724 learning rate:1.0000e-05, Loss_tot:0.1351,\n",
      "Save model 4724\n",
      "Train Epoch:4725 learning rate:1.0000e-05, Loss_tot:0.1351,\n",
      "Save model 4725\n",
      "Train Epoch:4726 learning rate:1.0000e-05, Loss_tot:0.1349,\n",
      "save model\n",
      "Save model 4726\n",
      "Train Epoch:4727 learning rate:1.0000e-05, Loss_tot:0.1349,\n",
      "Save model 4727\n",
      "Train Epoch:4728 learning rate:1.0000e-05, Loss_tot:0.1349,\n",
      "save model\n",
      "Save model 4728\n",
      "Train Epoch:4729 learning rate:1.0000e-05, Loss_tot:0.1348,\n",
      "save model\n",
      "Save model 4729\n",
      "Train Epoch:4730 learning rate:1.0000e-05, Loss_tot:0.1348,\n",
      "save model\n",
      "Save model 4730\n",
      "Train Epoch:4731 learning rate:1.0000e-05, Loss_tot:0.1347,\n",
      "save model\n",
      "Save model 4731\n",
      "Train Epoch:4732 learning rate:1.0000e-05, Loss_tot:0.1347,\n",
      "save model\n",
      "Save model 4732\n",
      "Train Epoch:4733 learning rate:1.0000e-05, Loss_tot:0.1347,\n",
      "save model\n",
      "Save model 4733\n",
      "Train Epoch:4734 learning rate:1.0000e-05, Loss_tot:0.1347,\n",
      "save model\n",
      "Save model 4734\n",
      "Train Epoch:4735 learning rate:1.0000e-05, Loss_tot:0.1346,\n",
      "save model\n",
      "Save model 4735\n",
      "Train Epoch:4736 learning rate:1.0000e-05, Loss_tot:0.1346,\n",
      "Save model 4736\n",
      "Train Epoch:4737 learning rate:1.0000e-05, Loss_tot:0.1346,\n",
      "save model\n",
      "Save model 4737\n",
      "Train Epoch:4738 learning rate:1.0000e-05, Loss_tot:0.1345,\n",
      "save model\n",
      "Save model 4738\n",
      "Train Epoch:4739 learning rate:1.0000e-05, Loss_tot:0.1345,\n",
      "save model\n",
      "Save model 4739\n",
      "Train Epoch:4740 learning rate:1.0000e-05, Loss_tot:0.1344,\n",
      "save model\n",
      "Save model 4740\n",
      "Train Epoch:4741 learning rate:1.0000e-05, Loss_tot:0.1344,\n",
      "save model\n",
      "Save model 4741\n",
      "Train Epoch:4742 learning rate:1.0000e-05, Loss_tot:0.1345,\n",
      "Save model 4742\n",
      "Train Epoch:4743 learning rate:1.0000e-05, Loss_tot:0.1344,\n",
      "Save model 4743\n",
      "Train Epoch:4744 learning rate:1.0000e-05, Loss_tot:0.1343,\n",
      "save model\n",
      "Save model 4744\n",
      "Train Epoch:4745 learning rate:1.0000e-05, Loss_tot:0.1343,\n",
      "Save model 4745\n",
      "Train Epoch:4746 learning rate:1.0000e-05, Loss_tot:0.1343,\n",
      "Save model 4746\n",
      "Train Epoch:4747 learning rate:1.0000e-05, Loss_tot:0.1343,\n",
      "Save model 4747\n",
      "Train Epoch:4748 learning rate:1.0000e-05, Loss_tot:0.1343,\n",
      "save model\n",
      "Save model 4748\n",
      "Train Epoch:4749 learning rate:1.0000e-05, Loss_tot:0.1342,\n",
      "save model\n",
      "Save model 4749\n",
      "Train Epoch:4750 learning rate:1.0000e-05, Loss_tot:0.1341,\n",
      "save model\n",
      "Save model 4750\n",
      "Train Epoch:4751 learning rate:1.0000e-05, Loss_tot:0.1341,\n",
      "save model\n",
      "Save model 4751\n",
      "Train Epoch:4752 learning rate:1.0000e-05, Loss_tot:0.1341,\n",
      "Save model 4752\n",
      "Train Epoch:4753 learning rate:1.0000e-05, Loss_tot:0.1340,\n",
      "save model\n",
      "Save model 4753\n",
      "Train Epoch:4754 learning rate:1.0000e-05, Loss_tot:0.1340,\n",
      "Save model 4754\n",
      "Train Epoch:4755 learning rate:1.0000e-05, Loss_tot:0.1340,\n",
      "Save model 4755\n",
      "Train Epoch:4756 learning rate:1.0000e-05, Loss_tot:0.1340,\n",
      "Save model 4756\n",
      "Train Epoch:4757 learning rate:1.0000e-05, Loss_tot:0.1339,\n",
      "save model\n",
      "Save model 4757\n",
      "Train Epoch:4758 learning rate:1.0000e-05, Loss_tot:0.1339,\n",
      "save model\n",
      "Save model 4758\n",
      "Train Epoch:4759 learning rate:1.0000e-05, Loss_tot:0.1338,\n",
      "save model\n",
      "Save model 4759\n",
      "Train Epoch:4760 learning rate:1.0000e-05, Loss_tot:0.1339,\n",
      "Save model 4760\n",
      "Train Epoch:4761 learning rate:1.0000e-05, Loss_tot:0.1339,\n",
      "Save model 4761\n",
      "Train Epoch:4762 learning rate:1.0000e-05, Loss_tot:0.1338,\n",
      "Save model 4762\n",
      "Train Epoch:4763 learning rate:1.0000e-05, Loss_tot:0.1336,\n",
      "save model\n",
      "Save model 4763\n",
      "Train Epoch:4764 learning rate:1.0000e-05, Loss_tot:0.1337,\n",
      "Save model 4764\n",
      "Train Epoch:4765 learning rate:1.0000e-05, Loss_tot:0.1337,\n",
      "Save model 4765\n",
      "Train Epoch:4766 learning rate:1.0000e-05, Loss_tot:0.1337,\n",
      "Save model 4766\n",
      "Train Epoch:4767 learning rate:1.0000e-05, Loss_tot:0.1337,\n",
      "Save model 4767\n",
      "Train Epoch:4768 learning rate:1.0000e-05, Loss_tot:0.1337,\n",
      "Save model 4768\n",
      "Train Epoch:4769 learning rate:1.0000e-05, Loss_tot:0.1336,\n",
      "save model\n",
      "Save model 4769\n",
      "Train Epoch:4770 learning rate:1.0000e-05, Loss_tot:0.1335,\n",
      "save model\n",
      "Save model 4770\n",
      "Train Epoch:4771 learning rate:1.0000e-05, Loss_tot:0.1334,\n",
      "save model\n",
      "Save model 4771\n",
      "Train Epoch:4772 learning rate:1.0000e-05, Loss_tot:0.1334,\n",
      "Save model 4772\n",
      "Train Epoch:4773 learning rate:1.0000e-05, Loss_tot:0.1335,\n",
      "Save model 4773\n",
      "Train Epoch:4774 learning rate:1.0000e-05, Loss_tot:0.1334,\n",
      "Save model 4774\n",
      "Train Epoch:4775 learning rate:1.0000e-05, Loss_tot:0.1333,\n",
      "save model\n",
      "Save model 4775\n",
      "Train Epoch:4776 learning rate:1.0000e-05, Loss_tot:0.1332,\n",
      "save model\n",
      "Save model 4776\n",
      "Train Epoch:4777 learning rate:1.0000e-05, Loss_tot:0.1332,\n",
      "Save model 4777\n",
      "Train Epoch:4778 learning rate:1.0000e-05, Loss_tot:0.1332,\n",
      "Save model 4778\n",
      "Train Epoch:4779 learning rate:1.0000e-05, Loss_tot:0.1332,\n",
      "save model\n",
      "Save model 4779\n",
      "Train Epoch:4780 learning rate:1.0000e-05, Loss_tot:0.1331,\n",
      "save model\n",
      "Save model 4780\n",
      "Train Epoch:4781 learning rate:1.0000e-05, Loss_tot:0.1331,\n",
      "save model\n",
      "Save model 4781\n",
      "Train Epoch:4782 learning rate:1.0000e-05, Loss_tot:0.1330,\n",
      "save model\n",
      "Save model 4782\n",
      "Train Epoch:4783 learning rate:1.0000e-05, Loss_tot:0.1331,\n",
      "Save model 4783\n",
      "Train Epoch:4784 learning rate:1.0000e-05, Loss_tot:0.1331,\n",
      "Save model 4784\n",
      "Train Epoch:4785 learning rate:1.0000e-05, Loss_tot:0.1330,\n",
      "Save model 4785\n",
      "Train Epoch:4786 learning rate:1.0000e-05, Loss_tot:0.1329,\n",
      "save model\n",
      "Save model 4786\n",
      "Train Epoch:4787 learning rate:1.0000e-05, Loss_tot:0.1328,\n",
      "save model\n",
      "Save model 4787\n",
      "Train Epoch:4788 learning rate:1.0000e-05, Loss_tot:0.1328,\n",
      "Save model 4788\n",
      "Train Epoch:4789 learning rate:1.0000e-05, Loss_tot:0.1329,\n",
      "Save model 4789\n",
      "Train Epoch:4790 learning rate:1.0000e-05, Loss_tot:0.1329,\n",
      "Save model 4790\n",
      "Train Epoch:4791 learning rate:1.0000e-05, Loss_tot:0.1329,\n",
      "Save model 4791\n",
      "Train Epoch:4792 learning rate:1.0000e-05, Loss_tot:0.1328,\n",
      "save model\n",
      "Save model 4792\n",
      "Train Epoch:4793 learning rate:1.0000e-05, Loss_tot:0.1327,\n",
      "save model\n",
      "Save model 4793\n",
      "Train Epoch:4794 learning rate:1.0000e-05, Loss_tot:0.1326,\n",
      "save model\n",
      "Save model 4794\n",
      "Train Epoch:4795 learning rate:1.0000e-05, Loss_tot:0.1327,\n",
      "Save model 4795\n",
      "Train Epoch:4796 learning rate:1.0000e-05, Loss_tot:0.1326,\n",
      "save model\n",
      "Save model 4796\n",
      "Train Epoch:4797 learning rate:1.0000e-05, Loss_tot:0.1325,\n",
      "save model\n",
      "Save model 4797\n",
      "Train Epoch:4798 learning rate:1.0000e-05, Loss_tot:0.1325,\n",
      "Save model 4798\n",
      "Train Epoch:4799 learning rate:1.0000e-05, Loss_tot:0.1325,\n",
      "save model\n",
      "Save model 4799\n",
      "Train Epoch:4800 learning rate:1.0000e-05, Loss_tot:0.1324,\n",
      "save model\n",
      "Save model 4800\n",
      "Train Epoch:4801 learning rate:1.0000e-05, Loss_tot:0.1324,\n",
      "save model\n",
      "Save model 4801\n",
      "Train Epoch:4802 learning rate:1.0000e-05, Loss_tot:0.1324,\n",
      "Save model 4802\n",
      "Train Epoch:4803 learning rate:1.0000e-05, Loss_tot:0.1324,\n",
      "Save model 4803\n",
      "Train Epoch:4804 learning rate:1.0000e-05, Loss_tot:0.1323,\n",
      "save model\n",
      "Save model 4804\n",
      "Train Epoch:4805 learning rate:1.0000e-05, Loss_tot:0.1323,\n",
      "save model\n",
      "Save model 4805\n",
      "Train Epoch:4806 learning rate:1.0000e-05, Loss_tot:0.1323,\n",
      "Save model 4806\n",
      "Train Epoch:4807 learning rate:1.0000e-05, Loss_tot:0.1323,\n",
      "save model\n",
      "Save model 4807\n",
      "Train Epoch:4808 learning rate:1.0000e-05, Loss_tot:0.1322,\n",
      "save model\n",
      "Save model 4808\n",
      "Train Epoch:4809 learning rate:1.0000e-05, Loss_tot:0.1321,\n",
      "save model\n",
      "Save model 4809\n",
      "Train Epoch:4810 learning rate:1.0000e-05, Loss_tot:0.1321,\n",
      "save model\n",
      "Save model 4810\n",
      "Train Epoch:4811 learning rate:1.0000e-05, Loss_tot:0.1320,\n",
      "save model\n",
      "Save model 4811\n",
      "Train Epoch:4812 learning rate:1.0000e-05, Loss_tot:0.1320,\n",
      "save model\n",
      "Save model 4812\n",
      "Train Epoch:4813 learning rate:1.0000e-05, Loss_tot:0.1320,\n",
      "save model\n",
      "Save model 4813\n",
      "Train Epoch:4814 learning rate:1.0000e-05, Loss_tot:0.1319,\n",
      "save model\n",
      "Save model 4814\n",
      "Train Epoch:4815 learning rate:1.0000e-05, Loss_tot:0.1319,\n",
      "save model\n",
      "Save model 4815\n",
      "Train Epoch:4816 learning rate:1.0000e-05, Loss_tot:0.1319,\n",
      "save model\n",
      "Save model 4816\n",
      "Train Epoch:4817 learning rate:1.0000e-05, Loss_tot:0.1318,\n",
      "save model\n",
      "Save model 4817\n",
      "Train Epoch:4818 learning rate:1.0000e-05, Loss_tot:0.1318,\n",
      "save model\n",
      "Save model 4818\n",
      "Train Epoch:4819 learning rate:1.0000e-05, Loss_tot:0.1318,\n",
      "save model\n",
      "Save model 4819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4820 learning rate:1.0000e-05, Loss_tot:0.1317,\n",
      "save model\n",
      "Save model 4820\n",
      "Train Epoch:4821 learning rate:1.0000e-05, Loss_tot:0.1318,\n",
      "Save model 4821\n",
      "Train Epoch:4822 learning rate:1.0000e-05, Loss_tot:0.1317,\n",
      "save model\n",
      "Save model 4822\n",
      "Train Epoch:4823 learning rate:1.0000e-05, Loss_tot:0.1316,\n",
      "save model\n",
      "Save model 4823\n",
      "Train Epoch:4824 learning rate:1.0000e-05, Loss_tot:0.1316,\n",
      "save model\n",
      "Save model 4824\n",
      "Train Epoch:4825 learning rate:1.0000e-05, Loss_tot:0.1316,\n",
      "save model\n",
      "Save model 4825\n",
      "Train Epoch:4826 learning rate:1.0000e-05, Loss_tot:0.1315,\n",
      "save model\n",
      "Save model 4826\n",
      "Train Epoch:4827 learning rate:1.0000e-05, Loss_tot:0.1316,\n",
      "Save model 4827\n",
      "Train Epoch:4828 learning rate:1.0000e-05, Loss_tot:0.1315,\n",
      "Save model 4828\n",
      "Train Epoch:4829 learning rate:1.0000e-05, Loss_tot:0.1314,\n",
      "save model\n",
      "Save model 4829\n",
      "Train Epoch:4830 learning rate:1.0000e-05, Loss_tot:0.1314,\n",
      "save model\n",
      "Save model 4830\n",
      "Train Epoch:4831 learning rate:1.0000e-05, Loss_tot:0.1313,\n",
      "save model\n",
      "Save model 4831\n",
      "Train Epoch:4832 learning rate:1.0000e-05, Loss_tot:0.1313,\n",
      "Save model 4832\n",
      "Train Epoch:4833 learning rate:1.0000e-05, Loss_tot:0.1313,\n",
      "save model\n",
      "Save model 4833\n",
      "Train Epoch:4834 learning rate:1.0000e-05, Loss_tot:0.1313,\n",
      "save model\n",
      "Save model 4834\n",
      "Train Epoch:4835 learning rate:1.0000e-05, Loss_tot:0.1313,\n",
      "save model\n",
      "Save model 4835\n",
      "Train Epoch:4836 learning rate:1.0000e-05, Loss_tot:0.1312,\n",
      "save model\n",
      "Save model 4836\n",
      "Train Epoch:4837 learning rate:1.0000e-05, Loss_tot:0.1312,\n",
      "save model\n",
      "Save model 4837\n",
      "Train Epoch:4838 learning rate:1.0000e-05, Loss_tot:0.1311,\n",
      "save model\n",
      "Save model 4838\n",
      "Train Epoch:4839 learning rate:1.0000e-05, Loss_tot:0.1312,\n",
      "Save model 4839\n",
      "Train Epoch:4840 learning rate:1.0000e-05, Loss_tot:0.1312,\n",
      "Save model 4840\n",
      "Train Epoch:4841 learning rate:1.0000e-05, Loss_tot:0.1311,\n",
      "save model\n",
      "Save model 4841\n",
      "Train Epoch:4842 learning rate:1.0000e-05, Loss_tot:0.1310,\n",
      "save model\n",
      "Save model 4842\n",
      "Train Epoch:4843 learning rate:1.0000e-05, Loss_tot:0.1310,\n",
      "Save model 4843\n",
      "Train Epoch:4844 learning rate:1.0000e-05, Loss_tot:0.1310,\n",
      "save model\n",
      "Save model 4844\n",
      "Train Epoch:4845 learning rate:1.0000e-05, Loss_tot:0.1309,\n",
      "save model\n",
      "Save model 4845\n",
      "Train Epoch:4846 learning rate:1.0000e-05, Loss_tot:0.1309,\n",
      "save model\n",
      "Save model 4846\n",
      "Train Epoch:4847 learning rate:1.0000e-05, Loss_tot:0.1308,\n",
      "save model\n",
      "Save model 4847\n",
      "Train Epoch:4848 learning rate:1.0000e-05, Loss_tot:0.1308,\n",
      "save model\n",
      "Save model 4848\n",
      "Train Epoch:4849 learning rate:1.0000e-05, Loss_tot:0.1307,\n",
      "save model\n",
      "Save model 4849\n",
      "Train Epoch:4850 learning rate:1.0000e-05, Loss_tot:0.1307,\n",
      "save model\n",
      "Save model 4850\n",
      "Train Epoch:4851 learning rate:1.0000e-05, Loss_tot:0.1307,\n",
      "save model\n",
      "Save model 4851\n",
      "Train Epoch:4852 learning rate:1.0000e-05, Loss_tot:0.1306,\n",
      "save model\n",
      "Save model 4852\n",
      "Train Epoch:4853 learning rate:1.0000e-05, Loss_tot:0.1306,\n",
      "save model\n",
      "Save model 4853\n",
      "Train Epoch:4854 learning rate:1.0000e-05, Loss_tot:0.1306,\n",
      "save model\n",
      "Save model 4854\n",
      "Train Epoch:4855 learning rate:1.0000e-05, Loss_tot:0.1306,\n",
      "save model\n",
      "Save model 4855\n",
      "Train Epoch:4856 learning rate:1.0000e-05, Loss_tot:0.1305,\n",
      "save model\n",
      "Save model 4856\n",
      "Train Epoch:4857 learning rate:1.0000e-05, Loss_tot:0.1305,\n",
      "save model\n",
      "Save model 4857\n",
      "Train Epoch:4858 learning rate:1.0000e-05, Loss_tot:0.1304,\n",
      "save model\n",
      "Save model 4858\n",
      "Train Epoch:4859 learning rate:1.0000e-05, Loss_tot:0.1304,\n",
      "save model\n",
      "Save model 4859\n",
      "Train Epoch:4860 learning rate:1.0000e-05, Loss_tot:0.1304,\n",
      "save model\n",
      "Save model 4860\n",
      "Train Epoch:4861 learning rate:1.0000e-05, Loss_tot:0.1304,\n",
      "save model\n",
      "Save model 4861\n",
      "Train Epoch:4862 learning rate:1.0000e-05, Loss_tot:0.1303,\n",
      "save model\n",
      "Save model 4862\n",
      "Train Epoch:4863 learning rate:1.0000e-05, Loss_tot:0.1302,\n",
      "save model\n",
      "Save model 4863\n",
      "Train Epoch:4864 learning rate:1.0000e-05, Loss_tot:0.1303,\n",
      "Save model 4864\n",
      "Train Epoch:4865 learning rate:1.0000e-05, Loss_tot:0.1302,\n",
      "save model\n",
      "Save model 4865\n",
      "Train Epoch:4866 learning rate:1.0000e-05, Loss_tot:0.1301,\n",
      "save model\n",
      "Save model 4866\n",
      "Train Epoch:4867 learning rate:1.0000e-05, Loss_tot:0.1301,\n",
      "save model\n",
      "Save model 4867\n",
      "Train Epoch:4868 learning rate:1.0000e-05, Loss_tot:0.1301,\n",
      "save model\n",
      "Save model 4868\n",
      "Train Epoch:4869 learning rate:1.0000e-05, Loss_tot:0.1301,\n",
      "Save model 4869\n",
      "Train Epoch:4870 learning rate:1.0000e-05, Loss_tot:0.1300,\n",
      "save model\n",
      "Save model 4870\n",
      "Train Epoch:4871 learning rate:1.0000e-05, Loss_tot:0.1300,\n",
      "save model\n",
      "Save model 4871\n",
      "Train Epoch:4872 learning rate:1.0000e-05, Loss_tot:0.1300,\n",
      "save model\n",
      "Save model 4872\n",
      "Train Epoch:4873 learning rate:1.0000e-05, Loss_tot:0.1299,\n",
      "save model\n",
      "Save model 4873\n",
      "Train Epoch:4874 learning rate:1.0000e-05, Loss_tot:0.1299,\n",
      "save model\n",
      "Save model 4874\n",
      "Train Epoch:4875 learning rate:1.0000e-05, Loss_tot:0.1298,\n",
      "save model\n",
      "Save model 4875\n",
      "Train Epoch:4876 learning rate:1.0000e-05, Loss_tot:0.1298,\n",
      "save model\n",
      "Save model 4876\n",
      "Train Epoch:4877 learning rate:1.0000e-05, Loss_tot:0.1298,\n",
      "Save model 4877\n",
      "Train Epoch:4878 learning rate:1.0000e-05, Loss_tot:0.1297,\n",
      "save model\n",
      "Save model 4878\n",
      "Train Epoch:4879 learning rate:1.0000e-05, Loss_tot:0.1297,\n",
      "save model\n",
      "Save model 4879\n",
      "Train Epoch:4880 learning rate:1.0000e-05, Loss_tot:0.1297,\n",
      "Save model 4880\n",
      "Train Epoch:4881 learning rate:1.0000e-05, Loss_tot:0.1296,\n",
      "save model\n",
      "Save model 4881\n",
      "Train Epoch:4882 learning rate:1.0000e-05, Loss_tot:0.1296,\n",
      "Save model 4882\n",
      "Train Epoch:4883 learning rate:1.0000e-05, Loss_tot:0.1296,\n",
      "Save model 4883\n",
      "Train Epoch:4884 learning rate:1.0000e-05, Loss_tot:0.1296,\n",
      "save model\n",
      "Save model 4884\n",
      "Train Epoch:4885 learning rate:1.0000e-05, Loss_tot:0.1295,\n",
      "save model\n",
      "Save model 4885\n",
      "Train Epoch:4886 learning rate:1.0000e-05, Loss_tot:0.1295,\n",
      "save model\n",
      "Save model 4886\n",
      "Train Epoch:4887 learning rate:1.0000e-05, Loss_tot:0.1294,\n",
      "save model\n",
      "Save model 4887\n",
      "Train Epoch:4888 learning rate:1.0000e-05, Loss_tot:0.1294,\n",
      "save model\n",
      "Save model 4888\n",
      "Train Epoch:4889 learning rate:1.0000e-05, Loss_tot:0.1293,\n",
      "save model\n",
      "Save model 4889\n",
      "Train Epoch:4890 learning rate:1.0000e-05, Loss_tot:0.1293,\n",
      "save model\n",
      "Save model 4890\n",
      "Train Epoch:4891 learning rate:1.0000e-05, Loss_tot:0.1293,\n",
      "save model\n",
      "Save model 4891\n",
      "Train Epoch:4892 learning rate:1.0000e-05, Loss_tot:0.1292,\n",
      "save model\n",
      "Save model 4892\n",
      "Train Epoch:4893 learning rate:1.0000e-05, Loss_tot:0.1292,\n",
      "save model\n",
      "Save model 4893\n",
      "Train Epoch:4894 learning rate:1.0000e-05, Loss_tot:0.1291,\n",
      "save model\n",
      "Save model 4894\n",
      "Train Epoch:4895 learning rate:1.0000e-05, Loss_tot:0.1291,\n",
      "save model\n",
      "Save model 4895\n",
      "Train Epoch:4896 learning rate:1.0000e-05, Loss_tot:0.1291,\n",
      "save model\n",
      "Save model 4896\n",
      "Train Epoch:4897 learning rate:1.0000e-05, Loss_tot:0.1290,\n",
      "save model\n",
      "Save model 4897\n",
      "Train Epoch:4898 learning rate:1.0000e-05, Loss_tot:0.1290,\n",
      "save model\n",
      "Save model 4898\n",
      "Train Epoch:4899 learning rate:1.0000e-05, Loss_tot:0.1290,\n",
      "save model\n",
      "Save model 4899\n",
      "Train Epoch:4900 learning rate:1.0000e-05, Loss_tot:0.1290,\n",
      "Save model 4900\n",
      "Train Epoch:4901 learning rate:1.0000e-05, Loss_tot:0.1289,\n",
      "save model\n",
      "Save model 4901\n",
      "Train Epoch:4902 learning rate:1.0000e-05, Loss_tot:0.1289,\n",
      "Save model 4902\n",
      "Train Epoch:4903 learning rate:1.0000e-05, Loss_tot:0.1289,\n",
      "Save model 4903\n",
      "Train Epoch:4904 learning rate:1.0000e-05, Loss_tot:0.1289,\n",
      "save model\n",
      "Save model 4904\n",
      "Train Epoch:4905 learning rate:1.0000e-05, Loss_tot:0.1288,\n",
      "save model\n",
      "Save model 4905\n",
      "Train Epoch:4906 learning rate:1.0000e-05, Loss_tot:0.1287,\n",
      "save model\n",
      "Save model 4906\n",
      "Train Epoch:4907 learning rate:1.0000e-05, Loss_tot:0.1288,\n",
      "Save model 4907\n",
      "Train Epoch:4908 learning rate:1.0000e-05, Loss_tot:0.1287,\n",
      "Save model 4908\n",
      "Train Epoch:4909 learning rate:1.0000e-05, Loss_tot:0.1286,\n",
      "save model\n",
      "Save model 4909\n",
      "Train Epoch:4910 learning rate:1.0000e-05, Loss_tot:0.1286,\n",
      "save model\n",
      "Save model 4910\n",
      "Train Epoch:4911 learning rate:1.0000e-05, Loss_tot:0.1286,\n",
      "Save model 4911\n",
      "Train Epoch:4912 learning rate:1.0000e-05, Loss_tot:0.1286,\n",
      "save model\n",
      "Save model 4912\n",
      "Train Epoch:4913 learning rate:1.0000e-05, Loss_tot:0.1286,\n",
      "save model\n",
      "Save model 4913\n",
      "Train Epoch:4914 learning rate:1.0000e-05, Loss_tot:0.1285,\n",
      "save model\n",
      "Save model 4914\n",
      "Train Epoch:4915 learning rate:1.0000e-05, Loss_tot:0.1284,\n",
      "save model\n",
      "Save model 4915\n",
      "Train Epoch:4916 learning rate:1.0000e-05, Loss_tot:0.1285,\n",
      "Save model 4916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:4917 learning rate:1.0000e-05, Loss_tot:0.1285,\n",
      "Save model 4917\n",
      "Train Epoch:4918 learning rate:1.0000e-05, Loss_tot:0.1284,\n",
      "save model\n",
      "Save model 4918\n",
      "Train Epoch:4919 learning rate:1.0000e-05, Loss_tot:0.1283,\n",
      "save model\n",
      "Save model 4919\n",
      "Train Epoch:4920 learning rate:1.0000e-05, Loss_tot:0.1283,\n",
      "Save model 4920\n",
      "Train Epoch:4921 learning rate:1.0000e-05, Loss_tot:0.1282,\n",
      "save model\n",
      "Save model 4921\n",
      "Train Epoch:4922 learning rate:1.0000e-05, Loss_tot:0.1282,\n",
      "save model\n",
      "Save model 4922\n",
      "Train Epoch:4923 learning rate:1.0000e-05, Loss_tot:0.1281,\n",
      "save model\n",
      "Save model 4923\n",
      "Train Epoch:4924 learning rate:1.0000e-05, Loss_tot:0.1282,\n",
      "Save model 4924\n",
      "Train Epoch:4925 learning rate:1.0000e-05, Loss_tot:0.1282,\n",
      "Save model 4925\n",
      "Train Epoch:4926 learning rate:1.0000e-05, Loss_tot:0.1281,\n",
      "save model\n",
      "Save model 4926\n",
      "Train Epoch:4927 learning rate:1.0000e-05, Loss_tot:0.1280,\n",
      "save model\n",
      "Save model 4927\n",
      "Train Epoch:4928 learning rate:1.0000e-05, Loss_tot:0.1280,\n",
      "Save model 4928\n",
      "Train Epoch:4929 learning rate:1.0000e-05, Loss_tot:0.1280,\n",
      "save model\n",
      "Save model 4929\n",
      "Train Epoch:4930 learning rate:1.0000e-05, Loss_tot:0.1280,\n",
      "save model\n",
      "Save model 4930\n",
      "Train Epoch:4931 learning rate:1.0000e-05, Loss_tot:0.1279,\n",
      "save model\n",
      "Save model 4931\n",
      "Train Epoch:4932 learning rate:1.0000e-05, Loss_tot:0.1278,\n",
      "save model\n",
      "Save model 4932\n",
      "Train Epoch:4933 learning rate:1.0000e-05, Loss_tot:0.1279,\n",
      "Save model 4933\n",
      "Train Epoch:4934 learning rate:1.0000e-05, Loss_tot:0.1279,\n",
      "Save model 4934\n",
      "Train Epoch:4935 learning rate:1.0000e-05, Loss_tot:0.1278,\n",
      "save model\n",
      "Save model 4935\n",
      "Train Epoch:4936 learning rate:1.0000e-05, Loss_tot:0.1277,\n",
      "save model\n",
      "Save model 4936\n",
      "Train Epoch:4937 learning rate:1.0000e-05, Loss_tot:0.1277,\n",
      "Save model 4937\n",
      "Train Epoch:4938 learning rate:1.0000e-05, Loss_tot:0.1276,\n",
      "save model\n",
      "Save model 4938\n",
      "Train Epoch:4939 learning rate:1.0000e-05, Loss_tot:0.1276,\n",
      "save model\n",
      "Save model 4939\n",
      "Train Epoch:4940 learning rate:1.0000e-05, Loss_tot:0.1275,\n",
      "save model\n",
      "Save model 4940\n",
      "Train Epoch:4941 learning rate:1.0000e-05, Loss_tot:0.1276,\n",
      "Save model 4941\n",
      "Train Epoch:4942 learning rate:1.0000e-05, Loss_tot:0.1275,\n",
      "Save model 4942\n",
      "Train Epoch:4943 learning rate:1.0000e-05, Loss_tot:0.1274,\n",
      "save model\n",
      "Save model 4943\n",
      "Train Epoch:4944 learning rate:1.0000e-05, Loss_tot:0.1274,\n",
      "save model\n",
      "Save model 4944\n",
      "Train Epoch:4945 learning rate:1.0000e-05, Loss_tot:0.1274,\n",
      "Save model 4945\n",
      "Train Epoch:4946 learning rate:1.0000e-05, Loss_tot:0.1274,\n",
      "save model\n",
      "Save model 4946\n",
      "Train Epoch:4947 learning rate:1.0000e-05, Loss_tot:0.1274,\n",
      "save model\n",
      "Save model 4947\n",
      "Train Epoch:4948 learning rate:1.0000e-05, Loss_tot:0.1273,\n",
      "save model\n",
      "Save model 4948\n",
      "Train Epoch:4949 learning rate:1.0000e-05, Loss_tot:0.1272,\n",
      "save model\n",
      "Save model 4949\n",
      "Train Epoch:4950 learning rate:1.0000e-05, Loss_tot:0.1272,\n",
      "Save model 4950\n",
      "Train Epoch:4951 learning rate:1.0000e-05, Loss_tot:0.1272,\n",
      "Save model 4951\n",
      "Train Epoch:4952 learning rate:1.0000e-05, Loss_tot:0.1271,\n",
      "save model\n",
      "Save model 4952\n",
      "Train Epoch:4953 learning rate:1.0000e-05, Loss_tot:0.1271,\n",
      "save model\n",
      "Save model 4953\n",
      "Train Epoch:4954 learning rate:1.0000e-05, Loss_tot:0.1271,\n",
      "Save model 4954\n",
      "Train Epoch:4955 learning rate:1.0000e-05, Loss_tot:0.1270,\n",
      "save model\n",
      "Save model 4955\n",
      "Train Epoch:4956 learning rate:1.0000e-05, Loss_tot:0.1270,\n",
      "save model\n",
      "Save model 4956\n",
      "Train Epoch:4957 learning rate:1.0000e-05, Loss_tot:0.1269,\n",
      "save model\n",
      "Save model 4957\n",
      "Train Epoch:4958 learning rate:1.0000e-05, Loss_tot:0.1269,\n",
      "Save model 4958\n",
      "Train Epoch:4959 learning rate:1.0000e-05, Loss_tot:0.1269,\n",
      "save model\n",
      "Save model 4959\n",
      "Train Epoch:4960 learning rate:1.0000e-05, Loss_tot:0.1268,\n",
      "save model\n",
      "Save model 4960\n",
      "Train Epoch:4961 learning rate:1.0000e-05, Loss_tot:0.1268,\n",
      "Save model 4961\n",
      "Train Epoch:4962 learning rate:1.0000e-05, Loss_tot:0.1268,\n",
      "Save model 4962\n",
      "Train Epoch:4963 learning rate:1.0000e-05, Loss_tot:0.1268,\n",
      "Save model 4963\n",
      "Train Epoch:4964 learning rate:1.0000e-05, Loss_tot:0.1268,\n",
      "save model\n",
      "Save model 4964\n",
      "Train Epoch:4965 learning rate:1.0000e-05, Loss_tot:0.1267,\n",
      "save model\n",
      "Save model 4965\n",
      "Train Epoch:4966 learning rate:1.0000e-05, Loss_tot:0.1266,\n",
      "save model\n",
      "Save model 4966\n",
      "Train Epoch:4967 learning rate:1.0000e-05, Loss_tot:0.1266,\n",
      "save model\n",
      "Save model 4967\n",
      "Train Epoch:4968 learning rate:1.0000e-05, Loss_tot:0.1266,\n",
      "Save model 4968\n",
      "Train Epoch:4969 learning rate:1.0000e-05, Loss_tot:0.1265,\n",
      "save model\n",
      "Save model 4969\n",
      "Train Epoch:4970 learning rate:1.0000e-05, Loss_tot:0.1264,\n",
      "save model\n",
      "Save model 4970\n",
      "Train Epoch:4971 learning rate:1.0000e-05, Loss_tot:0.1264,\n",
      "Save model 4971\n",
      "Train Epoch:4972 learning rate:1.0000e-05, Loss_tot:0.1264,\n",
      "save model\n",
      "Save model 4972\n",
      "Train Epoch:4973 learning rate:1.0000e-05, Loss_tot:0.1264,\n",
      "save model\n",
      "Save model 4973\n",
      "Train Epoch:4974 learning rate:1.0000e-05, Loss_tot:0.1263,\n",
      "save model\n",
      "Save model 4974\n",
      "Train Epoch:4975 learning rate:1.0000e-05, Loss_tot:0.1262,\n",
      "save model\n",
      "Save model 4975\n",
      "Train Epoch:4976 learning rate:1.0000e-05, Loss_tot:0.1262,\n",
      "save model\n",
      "Save model 4976\n",
      "Train Epoch:4977 learning rate:1.0000e-05, Loss_tot:0.1261,\n",
      "save model\n",
      "Save model 4977\n",
      "Train Epoch:4978 learning rate:1.0000e-05, Loss_tot:0.1261,\n",
      "save model\n",
      "Save model 4978\n",
      "Train Epoch:4979 learning rate:1.0000e-05, Loss_tot:0.1260,\n",
      "save model\n",
      "Save model 4979\n",
      "Train Epoch:4980 learning rate:1.0000e-05, Loss_tot:0.1260,\n",
      "save model\n",
      "Save model 4980\n",
      "Train Epoch:4981 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "save model\n",
      "Save model 4981\n",
      "Train Epoch:4982 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "save model\n",
      "Save model 4982\n",
      "Train Epoch:4983 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "save model\n",
      "Save model 4983\n",
      "Train Epoch:4984 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "save model\n",
      "Save model 4984\n",
      "Train Epoch:4985 learning rate:1.0000e-05, Loss_tot:0.1258,\n",
      "save model\n",
      "Save model 4985\n",
      "Train Epoch:4986 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "save model\n",
      "Save model 4986\n",
      "Train Epoch:4987 learning rate:1.0000e-05, Loss_tot:0.1258,\n",
      "Save model 4987\n",
      "Train Epoch:4988 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "Save model 4988\n",
      "Train Epoch:4989 learning rate:1.0000e-05, Loss_tot:0.1256,\n",
      "save model\n",
      "Save model 4989\n",
      "Train Epoch:4990 learning rate:1.0000e-05, Loss_tot:0.1255,\n",
      "save model\n",
      "Save model 4990\n",
      "Train Epoch:4991 learning rate:1.0000e-05, Loss_tot:0.1254,\n",
      "save model\n",
      "Save model 4991\n",
      "Train Epoch:4992 learning rate:1.0000e-05, Loss_tot:0.1253,\n",
      "save model\n",
      "Save model 4992\n",
      "Train Epoch:4993 learning rate:1.0000e-05, Loss_tot:0.1252,\n",
      "save model\n",
      "Save model 4993\n",
      "Train Epoch:4994 learning rate:1.0000e-05, Loss_tot:0.1251,\n",
      "save model\n",
      "Save model 4994\n",
      "Train Epoch:4995 learning rate:1.0000e-05, Loss_tot:0.1249,\n",
      "save model\n",
      "Save model 4995\n",
      "Train Epoch:4996 learning rate:1.0000e-05, Loss_tot:0.1247,\n",
      "save model\n",
      "Save model 4996\n",
      "Train Epoch:4997 learning rate:1.0000e-05, Loss_tot:0.1245,\n",
      "save model\n",
      "Save model 4997\n",
      "Train Epoch:4998 learning rate:1.0000e-05, Loss_tot:0.1261,\n",
      "Save model 4998\n",
      "Train Epoch:4999 learning rate:1.0000e-05, Loss_tot:0.1260,\n",
      "Save model 4999\n",
      "Train Epoch:5000 learning rate:1.0000e-05, Loss_tot:0.1260,\n",
      "Save model 5000\n",
      "Train Epoch:5001 learning rate:1.0000e-05, Loss_tot:0.1260,\n",
      "Save model 5001\n",
      "Train Epoch:5002 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "Save model 5002\n",
      "Train Epoch:5003 learning rate:1.0000e-05, Loss_tot:0.1259,\n",
      "Save model 5003\n",
      "Train Epoch:5004 learning rate:1.0000e-05, Loss_tot:0.1258,\n",
      "Save model 5004\n",
      "Train Epoch:5005 learning rate:1.0000e-05, Loss_tot:0.1258,\n",
      "Save model 5005\n",
      "Train Epoch:5006 learning rate:1.0000e-05, Loss_tot:0.1258,\n",
      "Save model 5006\n",
      "Train Epoch:5007 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "Save model 5007\n",
      "Train Epoch:5008 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "Save model 5008\n",
      "Train Epoch:5009 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "Save model 5009\n",
      "Train Epoch:5010 learning rate:1.0000e-05, Loss_tot:0.1257,\n",
      "Save model 5010\n",
      "Train Epoch:5011 learning rate:1.0000e-05, Loss_tot:0.1256,\n",
      "Save model 5011\n",
      "Train Epoch:5012 learning rate:1.0000e-05, Loss_tot:0.1256,\n",
      "Save model 5012\n",
      "Train Epoch:5013 learning rate:1.0000e-05, Loss_tot:0.1255,\n",
      "Save model 5013\n",
      "Train Epoch:5014 learning rate:1.0000e-05, Loss_tot:0.1255,\n",
      "Save model 5014\n",
      "Train Epoch:5015 learning rate:1.0000e-05, Loss_tot:0.1255,\n",
      "Save model 5015\n",
      "Train Epoch:5016 learning rate:1.0000e-05, Loss_tot:0.1254,\n",
      "Save model 5016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5017 learning rate:1.0000e-05, Loss_tot:0.1254,\n",
      "Save model 5017\n",
      "Train Epoch:5018 learning rate:1.0000e-05, Loss_tot:0.1254,\n",
      "Save model 5018\n",
      "Train Epoch:5019 learning rate:1.0000e-05, Loss_tot:0.1253,\n",
      "Save model 5019\n",
      "Train Epoch:5020 learning rate:1.0000e-05, Loss_tot:0.1254,\n",
      "Save model 5020\n",
      "Train Epoch:5021 learning rate:1.0000e-05, Loss_tot:0.1253,\n",
      "Save model 5021\n",
      "Train Epoch:5022 learning rate:1.0000e-05, Loss_tot:0.1252,\n",
      "Save model 5022\n",
      "Train Epoch:5023 learning rate:1.0000e-05, Loss_tot:0.1252,\n",
      "Save model 5023\n",
      "Train Epoch:5024 learning rate:1.0000e-05, Loss_tot:0.1253,\n",
      "Save model 5024\n",
      "Train Epoch:5025 learning rate:1.0000e-05, Loss_tot:0.1253,\n",
      "Save model 5025\n",
      "Train Epoch:5026 learning rate:1.0000e-05, Loss_tot:0.1252,\n",
      "Save model 5026\n",
      "Train Epoch:5027 learning rate:1.0000e-05, Loss_tot:0.1251,\n",
      "Save model 5027\n",
      "Train Epoch:5028 learning rate:1.0000e-05, Loss_tot:0.1251,\n",
      "Save model 5028\n",
      "Train Epoch:5029 learning rate:1.0000e-05, Loss_tot:0.1250,\n",
      "Save model 5029\n",
      "Train Epoch:5030 learning rate:1.0000e-05, Loss_tot:0.1250,\n",
      "Save model 5030\n",
      "Train Epoch:5031 learning rate:1.0000e-05, Loss_tot:0.1249,\n",
      "Save model 5031\n",
      "Train Epoch:5032 learning rate:1.0000e-05, Loss_tot:0.1249,\n",
      "Save model 5032\n",
      "Train Epoch:5033 learning rate:1.0000e-05, Loss_tot:0.1249,\n",
      "Save model 5033\n",
      "Train Epoch:5034 learning rate:1.0000e-05, Loss_tot:0.1249,\n",
      "Save model 5034\n",
      "Train Epoch:5035 learning rate:1.0000e-05, Loss_tot:0.1248,\n",
      "Save model 5035\n",
      "Train Epoch:5036 learning rate:1.0000e-05, Loss_tot:0.1248,\n",
      "Save model 5036\n",
      "Train Epoch:5037 learning rate:1.0000e-05, Loss_tot:0.1247,\n",
      "Save model 5037\n",
      "Train Epoch:5038 learning rate:1.0000e-05, Loss_tot:0.1247,\n",
      "Save model 5038\n",
      "Train Epoch:5039 learning rate:1.0000e-05, Loss_tot:0.1246,\n",
      "Save model 5039\n",
      "Train Epoch:5040 learning rate:1.0000e-05, Loss_tot:0.1246,\n",
      "Save model 5040\n",
      "Train Epoch:5041 learning rate:1.0000e-05, Loss_tot:0.1246,\n",
      "Save model 5041\n",
      "Train Epoch:5042 learning rate:1.0000e-05, Loss_tot:0.1246,\n",
      "Save model 5042\n",
      "Train Epoch:5043 learning rate:1.0000e-05, Loss_tot:0.1245,\n",
      "save model\n",
      "Save model 5043\n",
      "Train Epoch:5044 learning rate:1.0000e-05, Loss_tot:0.1245,\n",
      "save model\n",
      "Save model 5044\n",
      "Train Epoch:5045 learning rate:1.0000e-05, Loss_tot:0.1245,\n",
      "save model\n",
      "Save model 5045\n",
      "Train Epoch:5046 learning rate:1.0000e-05, Loss_tot:0.1244,\n",
      "save model\n",
      "Save model 5046\n",
      "Train Epoch:5047 learning rate:1.0000e-05, Loss_tot:0.1244,\n",
      "save model\n",
      "Save model 5047\n",
      "Train Epoch:5048 learning rate:1.0000e-05, Loss_tot:0.1243,\n",
      "save model\n",
      "Save model 5048\n",
      "Train Epoch:5049 learning rate:1.0000e-05, Loss_tot:0.1243,\n",
      "save model\n",
      "Save model 5049\n",
      "Train Epoch:5050 learning rate:1.0000e-05, Loss_tot:0.1242,\n",
      "save model\n",
      "Save model 5050\n",
      "Train Epoch:5051 learning rate:1.0000e-05, Loss_tot:0.1242,\n",
      "save model\n",
      "Save model 5051\n",
      "Train Epoch:5052 learning rate:1.0000e-05, Loss_tot:0.1242,\n",
      "save model\n",
      "Save model 5052\n",
      "Train Epoch:5053 learning rate:1.0000e-05, Loss_tot:0.1241,\n",
      "save model\n",
      "Save model 5053\n",
      "Train Epoch:5054 learning rate:1.0000e-05, Loss_tot:0.1242,\n",
      "Save model 5054\n",
      "Train Epoch:5055 learning rate:1.0000e-05, Loss_tot:0.1241,\n",
      "Save model 5055\n",
      "Train Epoch:5056 learning rate:1.0000e-05, Loss_tot:0.1240,\n",
      "save model\n",
      "Save model 5056\n",
      "Train Epoch:5057 learning rate:1.0000e-05, Loss_tot:0.1240,\n",
      "Save model 5057\n",
      "Train Epoch:5058 learning rate:1.0000e-05, Loss_tot:0.1240,\n",
      "Save model 5058\n",
      "Train Epoch:5059 learning rate:1.0000e-05, Loss_tot:0.1240,\n",
      "Save model 5059\n",
      "Train Epoch:5060 learning rate:1.0000e-05, Loss_tot:0.1240,\n",
      "save model\n",
      "Save model 5060\n",
      "Train Epoch:5061 learning rate:1.0000e-05, Loss_tot:0.1239,\n",
      "save model\n",
      "Save model 5061\n",
      "Train Epoch:5062 learning rate:1.0000e-05, Loss_tot:0.1238,\n",
      "save model\n",
      "Save model 5062\n",
      "Train Epoch:5063 learning rate:1.0000e-05, Loss_tot:0.1237,\n",
      "save model\n",
      "Save model 5063\n",
      "Train Epoch:5064 learning rate:1.0000e-05, Loss_tot:0.1237,\n",
      "save model\n",
      "Save model 5064\n",
      "Train Epoch:5065 learning rate:1.0000e-05, Loss_tot:0.1237,\n",
      "save model\n",
      "Save model 5065\n",
      "Train Epoch:5066 learning rate:1.0000e-05, Loss_tot:0.1236,\n",
      "save model\n",
      "Save model 5066\n",
      "Train Epoch:5067 learning rate:1.0000e-05, Loss_tot:0.1236,\n",
      "save model\n",
      "Save model 5067\n",
      "Train Epoch:5068 learning rate:1.0000e-05, Loss_tot:0.1235,\n",
      "save model\n",
      "Save model 5068\n",
      "Train Epoch:5069 learning rate:1.0000e-05, Loss_tot:0.1235,\n",
      "save model\n",
      "Save model 5069\n",
      "Train Epoch:5070 learning rate:1.0000e-05, Loss_tot:0.1235,\n",
      "save model\n",
      "Save model 5070\n",
      "Train Epoch:5071 learning rate:1.0000e-05, Loss_tot:0.1234,\n",
      "save model\n",
      "Save model 5071\n",
      "Train Epoch:5072 learning rate:1.0000e-05, Loss_tot:0.1234,\n",
      "save model\n",
      "Save model 5072\n",
      "Train Epoch:5073 learning rate:1.0000e-05, Loss_tot:0.1234,\n",
      "save model\n",
      "Save model 5073\n",
      "Train Epoch:5074 learning rate:1.0000e-05, Loss_tot:0.1234,\n",
      "save model\n",
      "Save model 5074\n",
      "Train Epoch:5075 learning rate:1.0000e-05, Loss_tot:0.1233,\n",
      "save model\n",
      "Save model 5075\n",
      "Train Epoch:5076 learning rate:1.0000e-05, Loss_tot:0.1232,\n",
      "save model\n",
      "Save model 5076\n",
      "Train Epoch:5077 learning rate:1.0000e-05, Loss_tot:0.1232,\n",
      "save model\n",
      "Save model 5077\n",
      "Train Epoch:5078 learning rate:1.0000e-05, Loss_tot:0.1232,\n",
      "save model\n",
      "Save model 5078\n",
      "Train Epoch:5079 learning rate:1.0000e-05, Loss_tot:0.1232,\n",
      "save model\n",
      "Save model 5079\n",
      "Train Epoch:5080 learning rate:1.0000e-05, Loss_tot:0.1231,\n",
      "save model\n",
      "Save model 5080\n",
      "Train Epoch:5081 learning rate:1.0000e-05, Loss_tot:0.1231,\n",
      "save model\n",
      "Save model 5081\n",
      "Train Epoch:5082 learning rate:1.0000e-05, Loss_tot:0.1231,\n",
      "Save model 5082\n",
      "Train Epoch:5083 learning rate:1.0000e-05, Loss_tot:0.1230,\n",
      "save model\n",
      "Save model 5083\n",
      "Train Epoch:5084 learning rate:1.0000e-05, Loss_tot:0.1230,\n",
      "save model\n",
      "Save model 5084\n",
      "Train Epoch:5085 learning rate:1.0000e-05, Loss_tot:0.1230,\n",
      "save model\n",
      "Save model 5085\n",
      "Train Epoch:5086 learning rate:1.0000e-05, Loss_tot:0.1229,\n",
      "save model\n",
      "Save model 5086\n",
      "Train Epoch:5087 learning rate:1.0000e-05, Loss_tot:0.1228,\n",
      "save model\n",
      "Save model 5087\n",
      "Train Epoch:5088 learning rate:1.0000e-05, Loss_tot:0.1228,\n",
      "save model\n",
      "Save model 5088\n",
      "Train Epoch:5089 learning rate:1.0000e-05, Loss_tot:0.1228,\n",
      "save model\n",
      "Save model 5089\n",
      "Train Epoch:5090 learning rate:1.0000e-05, Loss_tot:0.1227,\n",
      "save model\n",
      "Save model 5090\n",
      "Train Epoch:5091 learning rate:1.0000e-05, Loss_tot:0.1227,\n",
      "save model\n",
      "Save model 5091\n",
      "Train Epoch:5092 learning rate:1.0000e-05, Loss_tot:0.1227,\n",
      "save model\n",
      "Save model 5092\n",
      "Train Epoch:5093 learning rate:1.0000e-05, Loss_tot:0.1226,\n",
      "save model\n",
      "Save model 5093\n",
      "Train Epoch:5094 learning rate:1.0000e-05, Loss_tot:0.1226,\n",
      "save model\n",
      "Save model 5094\n",
      "Train Epoch:5095 learning rate:1.0000e-05, Loss_tot:0.1225,\n",
      "save model\n",
      "Save model 5095\n",
      "Train Epoch:5096 learning rate:1.0000e-05, Loss_tot:0.1225,\n",
      "save model\n",
      "Save model 5096\n",
      "Train Epoch:5097 learning rate:1.0000e-05, Loss_tot:0.1225,\n",
      "save model\n",
      "Save model 5097\n",
      "Train Epoch:5098 learning rate:1.0000e-05, Loss_tot:0.1224,\n",
      "save model\n",
      "Save model 5098\n",
      "Train Epoch:5099 learning rate:1.0000e-05, Loss_tot:0.1224,\n",
      "save model\n",
      "Save model 5099\n",
      "Train Epoch:5100 learning rate:1.0000e-05, Loss_tot:0.1223,\n",
      "save model\n",
      "Save model 5100\n",
      "Train Epoch:5101 learning rate:1.0000e-05, Loss_tot:0.1223,\n",
      "Save model 5101\n",
      "Train Epoch:5102 learning rate:1.0000e-05, Loss_tot:0.1223,\n",
      "Save model 5102\n",
      "Train Epoch:5103 learning rate:1.0000e-05, Loss_tot:0.1223,\n",
      "save model\n",
      "Save model 5103\n",
      "Train Epoch:5104 learning rate:1.0000e-05, Loss_tot:0.1222,\n",
      "save model\n",
      "Save model 5104\n",
      "Train Epoch:5105 learning rate:1.0000e-05, Loss_tot:0.1222,\n",
      "save model\n",
      "Save model 5105\n",
      "Train Epoch:5106 learning rate:1.0000e-05, Loss_tot:0.1221,\n",
      "save model\n",
      "Save model 5106\n",
      "Train Epoch:5107 learning rate:1.0000e-05, Loss_tot:0.1221,\n",
      "save model\n",
      "Save model 5107\n",
      "Train Epoch:5108 learning rate:1.0000e-05, Loss_tot:0.1221,\n",
      "save model\n",
      "Save model 5108\n",
      "Train Epoch:5109 learning rate:1.0000e-05, Loss_tot:0.1220,\n",
      "save model\n",
      "Save model 5109\n",
      "Train Epoch:5110 learning rate:1.0000e-05, Loss_tot:0.1220,\n",
      "save model\n",
      "Save model 5110\n",
      "Train Epoch:5111 learning rate:1.0000e-05, Loss_tot:0.1220,\n",
      "Save model 5111\n",
      "Train Epoch:5112 learning rate:1.0000e-05, Loss_tot:0.1220,\n",
      "save model\n",
      "Save model 5112\n",
      "Train Epoch:5113 learning rate:1.0000e-05, Loss_tot:0.1218,\n",
      "save model\n",
      "Save model 5113\n",
      "Train Epoch:5114 learning rate:1.0000e-05, Loss_tot:0.1219,\n",
      "Save model 5114\n",
      "Train Epoch:5115 learning rate:1.0000e-05, Loss_tot:0.1219,\n",
      "Save model 5115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5116 learning rate:1.0000e-05, Loss_tot:0.1219,\n",
      "Save model 5116\n",
      "Train Epoch:5117 learning rate:1.0000e-05, Loss_tot:0.1218,\n",
      "Save model 5117\n",
      "Train Epoch:5118 learning rate:1.0000e-05, Loss_tot:0.1218,\n",
      "save model\n",
      "Save model 5118\n",
      "Train Epoch:5119 learning rate:1.0000e-05, Loss_tot:0.1217,\n",
      "save model\n",
      "Save model 5119\n",
      "Train Epoch:5120 learning rate:1.0000e-05, Loss_tot:0.1216,\n",
      "save model\n",
      "Save model 5120\n",
      "Train Epoch:5121 learning rate:1.0000e-05, Loss_tot:0.1216,\n",
      "save model\n",
      "Save model 5121\n",
      "Train Epoch:5122 learning rate:1.0000e-05, Loss_tot:0.1215,\n",
      "save model\n",
      "Save model 5122\n",
      "Train Epoch:5123 learning rate:1.0000e-05, Loss_tot:0.1215,\n",
      "save model\n",
      "Save model 5123\n",
      "Train Epoch:5124 learning rate:1.0000e-05, Loss_tot:0.1214,\n",
      "save model\n",
      "Save model 5124\n",
      "Train Epoch:5125 learning rate:1.0000e-05, Loss_tot:0.1214,\n",
      "save model\n",
      "Save model 5125\n",
      "Train Epoch:5126 learning rate:1.0000e-05, Loss_tot:0.1214,\n",
      "save model\n",
      "Save model 5126\n",
      "Train Epoch:5127 learning rate:1.0000e-05, Loss_tot:0.1213,\n",
      "save model\n",
      "Save model 5127\n",
      "Train Epoch:5128 learning rate:1.0000e-05, Loss_tot:0.1213,\n",
      "save model\n",
      "Save model 5128\n",
      "Train Epoch:5129 learning rate:1.0000e-05, Loss_tot:0.1212,\n",
      "save model\n",
      "Save model 5129\n",
      "Train Epoch:5130 learning rate:1.0000e-05, Loss_tot:0.1212,\n",
      "Save model 5130\n",
      "Train Epoch:5131 learning rate:1.0000e-05, Loss_tot:0.1212,\n",
      "save model\n",
      "Save model 5131\n",
      "Train Epoch:5132 learning rate:1.0000e-05, Loss_tot:0.1212,\n",
      "save model\n",
      "Save model 5132\n",
      "Train Epoch:5133 learning rate:1.0000e-05, Loss_tot:0.1212,\n",
      "save model\n",
      "Save model 5133\n",
      "Train Epoch:5134 learning rate:1.0000e-05, Loss_tot:0.1211,\n",
      "save model\n",
      "Save model 5134\n",
      "Train Epoch:5135 learning rate:1.0000e-05, Loss_tot:0.1211,\n",
      "save model\n",
      "Save model 5135\n",
      "Train Epoch:5136 learning rate:1.0000e-05, Loss_tot:0.1210,\n",
      "save model\n",
      "Save model 5136\n",
      "Train Epoch:5137 learning rate:1.0000e-05, Loss_tot:0.1210,\n",
      "Save model 5137\n",
      "Train Epoch:5138 learning rate:1.0000e-05, Loss_tot:0.1210,\n",
      "save model\n",
      "Save model 5138\n",
      "Train Epoch:5139 learning rate:1.0000e-05, Loss_tot:0.1209,\n",
      "save model\n",
      "Save model 5139\n",
      "Train Epoch:5140 learning rate:1.0000e-05, Loss_tot:0.1209,\n",
      "save model\n",
      "Save model 5140\n",
      "Train Epoch:5141 learning rate:1.0000e-05, Loss_tot:0.1209,\n",
      "Save model 5141\n",
      "Train Epoch:5142 learning rate:1.0000e-05, Loss_tot:0.1209,\n",
      "save model\n",
      "Save model 5142\n",
      "Train Epoch:5143 learning rate:1.0000e-05, Loss_tot:0.1208,\n",
      "save model\n",
      "Save model 5143\n",
      "Train Epoch:5144 learning rate:1.0000e-05, Loss_tot:0.1207,\n",
      "save model\n",
      "Save model 5144\n",
      "Train Epoch:5145 learning rate:1.0000e-05, Loss_tot:0.1206,\n",
      "save model\n",
      "Save model 5145\n",
      "Train Epoch:5146 learning rate:1.0000e-05, Loss_tot:0.1207,\n",
      "Save model 5146\n",
      "Train Epoch:5147 learning rate:1.0000e-05, Loss_tot:0.1207,\n",
      "Save model 5147\n",
      "Train Epoch:5148 learning rate:1.0000e-05, Loss_tot:0.1206,\n",
      "save model\n",
      "Save model 5148\n",
      "Train Epoch:5149 learning rate:1.0000e-05, Loss_tot:0.1205,\n",
      "save model\n",
      "Save model 5149\n",
      "Train Epoch:5150 learning rate:1.0000e-05, Loss_tot:0.1205,\n",
      "save model\n",
      "Save model 5150\n",
      "Train Epoch:5151 learning rate:1.0000e-05, Loss_tot:0.1205,\n",
      "save model\n",
      "Save model 5151\n",
      "Train Epoch:5152 learning rate:1.0000e-05, Loss_tot:0.1204,\n",
      "save model\n",
      "Save model 5152\n",
      "Train Epoch:5153 learning rate:1.0000e-05, Loss_tot:0.1203,\n",
      "save model\n",
      "Save model 5153\n",
      "Train Epoch:5154 learning rate:1.0000e-05, Loss_tot:0.1204,\n",
      "Save model 5154\n",
      "Train Epoch:5155 learning rate:1.0000e-05, Loss_tot:0.1203,\n",
      "Save model 5155\n",
      "Train Epoch:5156 learning rate:1.0000e-05, Loss_tot:0.1202,\n",
      "save model\n",
      "Save model 5156\n",
      "Train Epoch:5157 learning rate:1.0000e-05, Loss_tot:0.1202,\n",
      "save model\n",
      "Save model 5157\n",
      "Train Epoch:5158 learning rate:1.0000e-05, Loss_tot:0.1202,\n",
      "Save model 5158\n",
      "Train Epoch:5159 learning rate:1.0000e-05, Loss_tot:0.1202,\n",
      "save model\n",
      "Save model 5159\n",
      "Train Epoch:5160 learning rate:1.0000e-05, Loss_tot:0.1202,\n",
      "save model\n",
      "Save model 5160\n",
      "Train Epoch:5161 learning rate:1.0000e-05, Loss_tot:0.1201,\n",
      "save model\n",
      "Save model 5161\n",
      "Train Epoch:5162 learning rate:1.0000e-05, Loss_tot:0.1200,\n",
      "save model\n",
      "Save model 5162\n",
      "Train Epoch:5163 learning rate:1.0000e-05, Loss_tot:0.1200,\n",
      "save model\n",
      "Save model 5163\n",
      "Train Epoch:5164 learning rate:1.0000e-05, Loss_tot:0.1200,\n",
      "Save model 5164\n",
      "Train Epoch:5165 learning rate:1.0000e-05, Loss_tot:0.1199,\n",
      "save model\n",
      "Save model 5165\n",
      "Train Epoch:5166 learning rate:1.0000e-05, Loss_tot:0.1199,\n",
      "save model\n",
      "Save model 5166\n",
      "Train Epoch:5167 learning rate:1.0000e-05, Loss_tot:0.1199,\n",
      "Save model 5167\n",
      "Train Epoch:5168 learning rate:1.0000e-05, Loss_tot:0.1198,\n",
      "save model\n",
      "Save model 5168\n",
      "Train Epoch:5169 learning rate:1.0000e-05, Loss_tot:0.1198,\n",
      "save model\n",
      "Save model 5169\n",
      "Train Epoch:5170 learning rate:1.0000e-05, Loss_tot:0.1197,\n",
      "save model\n",
      "Save model 5170\n",
      "Train Epoch:5171 learning rate:1.0000e-05, Loss_tot:0.1196,\n",
      "save model\n",
      "Save model 5171\n",
      "Train Epoch:5172 learning rate:1.0000e-05, Loss_tot:0.1196,\n",
      "save model\n",
      "Save model 5172\n",
      "Train Epoch:5173 learning rate:1.0000e-05, Loss_tot:0.1195,\n",
      "save model\n",
      "Save model 5173\n",
      "Train Epoch:5174 learning rate:1.0000e-05, Loss_tot:0.1195,\n",
      "save model\n",
      "Save model 5174\n",
      "Train Epoch:5175 learning rate:1.0000e-05, Loss_tot:0.1195,\n",
      "save model\n",
      "Save model 5175\n",
      "Train Epoch:5176 learning rate:1.0000e-05, Loss_tot:0.1195,\n",
      "save model\n",
      "Save model 5176\n",
      "Train Epoch:5177 learning rate:1.0000e-05, Loss_tot:0.1194,\n",
      "save model\n",
      "Save model 5177\n",
      "Train Epoch:5178 learning rate:1.0000e-05, Loss_tot:0.1194,\n",
      "save model\n",
      "Save model 5178\n",
      "Train Epoch:5179 learning rate:1.0000e-05, Loss_tot:0.1194,\n",
      "save model\n",
      "Save model 5179\n",
      "Train Epoch:5180 learning rate:1.0000e-05, Loss_tot:0.1193,\n",
      "save model\n",
      "Save model 5180\n",
      "Train Epoch:5181 learning rate:1.0000e-05, Loss_tot:0.1193,\n",
      "save model\n",
      "Save model 5181\n",
      "Train Epoch:5182 learning rate:1.0000e-05, Loss_tot:0.1192,\n",
      "save model\n",
      "Save model 5182\n",
      "Train Epoch:5183 learning rate:1.0000e-05, Loss_tot:0.1191,\n",
      "save model\n",
      "Save model 5183\n",
      "Train Epoch:5184 learning rate:1.0000e-05, Loss_tot:0.1191,\n",
      "Save model 5184\n",
      "Train Epoch:5185 learning rate:1.0000e-05, Loss_tot:0.1191,\n",
      "save model\n",
      "Save model 5185\n",
      "Train Epoch:5186 learning rate:1.0000e-05, Loss_tot:0.1191,\n",
      "save model\n",
      "Save model 5186\n",
      "Train Epoch:5187 learning rate:1.0000e-05, Loss_tot:0.1190,\n",
      "save model\n",
      "Save model 5187\n",
      "Train Epoch:5188 learning rate:1.0000e-05, Loss_tot:0.1190,\n",
      "save model\n",
      "Save model 5188\n",
      "Train Epoch:5189 learning rate:1.0000e-05, Loss_tot:0.1190,\n",
      "save model\n",
      "Save model 5189\n",
      "Train Epoch:5190 learning rate:1.0000e-05, Loss_tot:0.1189,\n",
      "save model\n",
      "Save model 5190\n",
      "Train Epoch:5191 learning rate:1.0000e-05, Loss_tot:0.1189,\n",
      "save model\n",
      "Save model 5191\n",
      "Train Epoch:5192 learning rate:1.0000e-05, Loss_tot:0.1188,\n",
      "save model\n",
      "Save model 5192\n",
      "Train Epoch:5193 learning rate:1.0000e-05, Loss_tot:0.1188,\n",
      "save model\n",
      "Save model 5193\n",
      "Train Epoch:5194 learning rate:1.0000e-05, Loss_tot:0.1187,\n",
      "save model\n",
      "Save model 5194\n",
      "Train Epoch:5195 learning rate:1.0000e-05, Loss_tot:0.1187,\n",
      "Save model 5195\n",
      "Train Epoch:5196 learning rate:1.0000e-05, Loss_tot:0.1187,\n",
      "save model\n",
      "Save model 5196\n",
      "Train Epoch:5197 learning rate:1.0000e-05, Loss_tot:0.1187,\n",
      "save model\n",
      "Save model 5197\n",
      "Train Epoch:5198 learning rate:1.0000e-05, Loss_tot:0.1186,\n",
      "save model\n",
      "Save model 5198\n",
      "Train Epoch:5199 learning rate:1.0000e-05, Loss_tot:0.1185,\n",
      "save model\n",
      "Save model 5199\n",
      "Train Epoch:5200 learning rate:1.0000e-05, Loss_tot:0.1186,\n",
      "Save model 5200\n",
      "Train Epoch:5201 learning rate:1.0000e-05, Loss_tot:0.1186,\n",
      "Save model 5201\n",
      "Train Epoch:5202 learning rate:1.0000e-05, Loss_tot:0.1185,\n",
      "save model\n",
      "Save model 5202\n",
      "Train Epoch:5203 learning rate:1.0000e-05, Loss_tot:0.1184,\n",
      "save model\n",
      "Save model 5203\n",
      "Train Epoch:5204 learning rate:1.0000e-05, Loss_tot:0.1184,\n",
      "Save model 5204\n",
      "Train Epoch:5205 learning rate:1.0000e-05, Loss_tot:0.1184,\n",
      "save model\n",
      "Save model 5205\n",
      "Train Epoch:5206 learning rate:1.0000e-05, Loss_tot:0.1183,\n",
      "save model\n",
      "Save model 5206\n",
      "Train Epoch:5207 learning rate:1.0000e-05, Loss_tot:0.1183,\n",
      "save model\n",
      "Save model 5207\n",
      "Train Epoch:5208 learning rate:1.0000e-05, Loss_tot:0.1182,\n",
      "save model\n",
      "Save model 5208\n",
      "Train Epoch:5209 learning rate:1.0000e-05, Loss_tot:0.1181,\n",
      "save model\n",
      "Save model 5209\n",
      "Train Epoch:5210 learning rate:1.0000e-05, Loss_tot:0.1181,\n",
      "save model\n",
      "Save model 5210\n",
      "Train Epoch:5211 learning rate:1.0000e-05, Loss_tot:0.1181,\n",
      "save model\n",
      "Save model 5211\n",
      "Train Epoch:5212 learning rate:1.0000e-05, Loss_tot:0.1180,\n",
      "save model\n",
      "Save model 5212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5213 learning rate:1.0000e-05, Loss_tot:0.1180,\n",
      "save model\n",
      "Save model 5213\n",
      "Train Epoch:5214 learning rate:1.0000e-05, Loss_tot:0.1179,\n",
      "save model\n",
      "Save model 5214\n",
      "Train Epoch:5215 learning rate:1.0000e-05, Loss_tot:0.1179,\n",
      "save model\n",
      "Save model 5215\n",
      "Train Epoch:5216 learning rate:1.0000e-05, Loss_tot:0.1179,\n",
      "save model\n",
      "Save model 5216\n",
      "Train Epoch:5217 learning rate:1.0000e-05, Loss_tot:0.1178,\n",
      "save model\n",
      "Save model 5217\n",
      "Train Epoch:5218 learning rate:1.0000e-05, Loss_tot:0.1178,\n",
      "save model\n",
      "Save model 5218\n",
      "Train Epoch:5219 learning rate:1.0000e-05, Loss_tot:0.1178,\n",
      "save model\n",
      "Save model 5219\n",
      "Train Epoch:5220 learning rate:1.0000e-05, Loss_tot:0.1177,\n",
      "save model\n",
      "Save model 5220\n",
      "Train Epoch:5221 learning rate:1.0000e-05, Loss_tot:0.1177,\n",
      "save model\n",
      "Save model 5221\n",
      "Train Epoch:5222 learning rate:1.0000e-05, Loss_tot:0.1177,\n",
      "save model\n",
      "Save model 5222\n",
      "Train Epoch:5223 learning rate:1.0000e-05, Loss_tot:0.1176,\n",
      "save model\n",
      "Save model 5223\n",
      "Train Epoch:5224 learning rate:1.0000e-05, Loss_tot:0.1175,\n",
      "save model\n",
      "Save model 5224\n",
      "Train Epoch:5225 learning rate:1.0000e-05, Loss_tot:0.1175,\n",
      "save model\n",
      "Save model 5225\n",
      "Train Epoch:5226 learning rate:1.0000e-05, Loss_tot:0.1175,\n",
      "save model\n",
      "Save model 5226\n",
      "Train Epoch:5227 learning rate:1.0000e-05, Loss_tot:0.1174,\n",
      "save model\n",
      "Save model 5227\n",
      "Train Epoch:5228 learning rate:1.0000e-05, Loss_tot:0.1174,\n",
      "save model\n",
      "Save model 5228\n",
      "Train Epoch:5229 learning rate:1.0000e-05, Loss_tot:0.1174,\n",
      "save model\n",
      "Save model 5229\n",
      "Train Epoch:5230 learning rate:1.0000e-05, Loss_tot:0.1173,\n",
      "save model\n",
      "Save model 5230\n",
      "Train Epoch:5231 learning rate:1.0000e-05, Loss_tot:0.1173,\n",
      "Save model 5231\n",
      "Train Epoch:5232 learning rate:1.0000e-05, Loss_tot:0.1173,\n",
      "Save model 5232\n",
      "Train Epoch:5233 learning rate:1.0000e-05, Loss_tot:0.1172,\n",
      "save model\n",
      "Save model 5233\n",
      "Train Epoch:5234 learning rate:1.0000e-05, Loss_tot:0.1172,\n",
      "Save model 5234\n",
      "Train Epoch:5235 learning rate:1.0000e-05, Loss_tot:0.1172,\n",
      "Save model 5235\n",
      "Train Epoch:5236 learning rate:1.0000e-05, Loss_tot:0.1172,\n",
      "Save model 5236\n",
      "Train Epoch:5237 learning rate:1.0000e-05, Loss_tot:0.1171,\n",
      "save model\n",
      "Save model 5237\n",
      "Train Epoch:5238 learning rate:1.0000e-05, Loss_tot:0.1171,\n",
      "save model\n",
      "Save model 5238\n",
      "Train Epoch:5239 learning rate:1.0000e-05, Loss_tot:0.1170,\n",
      "save model\n",
      "Save model 5239\n",
      "Train Epoch:5240 learning rate:1.0000e-05, Loss_tot:0.1169,\n",
      "save model\n",
      "Save model 5240\n",
      "Train Epoch:5241 learning rate:1.0000e-05, Loss_tot:0.1169,\n",
      "save model\n",
      "Save model 5241\n",
      "Train Epoch:5242 learning rate:1.0000e-05, Loss_tot:0.1168,\n",
      "save model\n",
      "Save model 5242\n",
      "Train Epoch:5243 learning rate:1.0000e-05, Loss_tot:0.1168,\n",
      "save model\n",
      "Save model 5243\n",
      "Train Epoch:5244 learning rate:1.0000e-05, Loss_tot:0.1167,\n",
      "save model\n",
      "Save model 5244\n",
      "Train Epoch:5245 learning rate:1.0000e-05, Loss_tot:0.1167,\n",
      "save model\n",
      "Save model 5245\n",
      "Train Epoch:5246 learning rate:1.0000e-05, Loss_tot:0.1166,\n",
      "save model\n",
      "Save model 5246\n",
      "Train Epoch:5247 learning rate:1.0000e-05, Loss_tot:0.1166,\n",
      "save model\n",
      "Save model 5247\n",
      "Train Epoch:5248 learning rate:1.0000e-05, Loss_tot:0.1165,\n",
      "save model\n",
      "Save model 5248\n",
      "Train Epoch:5249 learning rate:1.0000e-05, Loss_tot:0.1165,\n",
      "save model\n",
      "Save model 5249\n",
      "Train Epoch:5250 learning rate:1.0000e-05, Loss_tot:0.1165,\n",
      "Save model 5250\n",
      "Train Epoch:5251 learning rate:1.0000e-05, Loss_tot:0.1164,\n",
      "save model\n",
      "Save model 5251\n",
      "Train Epoch:5252 learning rate:1.0000e-05, Loss_tot:0.1165,\n",
      "Save model 5252\n",
      "Train Epoch:5253 learning rate:1.0000e-05, Loss_tot:0.1164,\n",
      "Save model 5253\n",
      "Train Epoch:5254 learning rate:1.0000e-05, Loss_tot:0.1164,\n",
      "Save model 5254\n",
      "Train Epoch:5255 learning rate:1.0000e-05, Loss_tot:0.1163,\n",
      "save model\n",
      "Save model 5255\n",
      "Train Epoch:5256 learning rate:1.0000e-05, Loss_tot:0.1163,\n",
      "save model\n",
      "Save model 5256\n",
      "Train Epoch:5257 learning rate:1.0000e-05, Loss_tot:0.1155,\n",
      "save model\n",
      "Save model 5257\n",
      "Train Epoch:5258 learning rate:1.0000e-05, Loss_tot:0.1162,\n",
      "Save model 5258\n",
      "Train Epoch:5259 learning rate:1.0000e-05, Loss_tot:0.1161,\n",
      "Save model 5259\n",
      "Train Epoch:5260 learning rate:1.0000e-05, Loss_tot:0.1161,\n",
      "Save model 5260\n",
      "Train Epoch:5261 learning rate:1.0000e-05, Loss_tot:0.1160,\n",
      "Save model 5261\n",
      "Train Epoch:5262 learning rate:1.0000e-05, Loss_tot:0.1160,\n",
      "Save model 5262\n",
      "Train Epoch:5263 learning rate:1.0000e-05, Loss_tot:0.1159,\n",
      "Save model 5263\n",
      "Train Epoch:5264 learning rate:1.0000e-05, Loss_tot:0.1159,\n",
      "Save model 5264\n",
      "Train Epoch:5265 learning rate:1.0000e-05, Loss_tot:0.1159,\n",
      "Save model 5265\n",
      "Train Epoch:5266 learning rate:1.0000e-05, Loss_tot:0.1159,\n",
      "Save model 5266\n",
      "Train Epoch:5267 learning rate:1.0000e-05, Loss_tot:0.1158,\n",
      "Save model 5267\n",
      "Train Epoch:5268 learning rate:1.0000e-05, Loss_tot:0.1157,\n",
      "Save model 5268\n",
      "Train Epoch:5269 learning rate:1.0000e-05, Loss_tot:0.1158,\n",
      "Save model 5269\n",
      "Train Epoch:5270 learning rate:1.0000e-05, Loss_tot:0.1158,\n",
      "Save model 5270\n",
      "Train Epoch:5271 learning rate:1.0000e-05, Loss_tot:0.1156,\n",
      "Save model 5271\n",
      "Train Epoch:5272 learning rate:1.0000e-05, Loss_tot:0.1156,\n",
      "Save model 5272\n",
      "Train Epoch:5273 learning rate:1.0000e-05, Loss_tot:0.1156,\n",
      "Save model 5273\n",
      "Train Epoch:5274 learning rate:1.0000e-05, Loss_tot:0.1155,\n",
      "Save model 5274\n",
      "Train Epoch:5275 learning rate:1.0000e-05, Loss_tot:0.1155,\n",
      "save model\n",
      "Save model 5275\n",
      "Train Epoch:5276 learning rate:1.0000e-05, Loss_tot:0.1154,\n",
      "save model\n",
      "Save model 5276\n",
      "Train Epoch:5277 learning rate:1.0000e-05, Loss_tot:0.1153,\n",
      "save model\n",
      "Save model 5277\n",
      "Train Epoch:5278 learning rate:1.0000e-05, Loss_tot:0.1153,\n",
      "save model\n",
      "Save model 5278\n",
      "Train Epoch:5279 learning rate:1.0000e-05, Loss_tot:0.1152,\n",
      "save model\n",
      "Save model 5279\n",
      "Train Epoch:5280 learning rate:1.0000e-05, Loss_tot:0.1152,\n",
      "save model\n",
      "Save model 5280\n",
      "Train Epoch:5281 learning rate:1.0000e-05, Loss_tot:0.1151,\n",
      "save model\n",
      "Save model 5281\n",
      "Train Epoch:5282 learning rate:1.0000e-05, Loss_tot:0.1150,\n",
      "save model\n",
      "Save model 5282\n",
      "Train Epoch:5283 learning rate:1.0000e-05, Loss_tot:0.1150,\n",
      "save model\n",
      "Save model 5283\n",
      "Train Epoch:5284 learning rate:1.0000e-05, Loss_tot:0.1149,\n",
      "save model\n",
      "Save model 5284\n",
      "Train Epoch:5285 learning rate:1.0000e-05, Loss_tot:0.1149,\n",
      "save model\n",
      "Save model 5285\n",
      "Train Epoch:5286 learning rate:1.0000e-05, Loss_tot:0.1149,\n",
      "save model\n",
      "Save model 5286\n",
      "Train Epoch:5287 learning rate:1.0000e-05, Loss_tot:0.1148,\n",
      "save model\n",
      "Save model 5287\n",
      "Train Epoch:5288 learning rate:1.0000e-05, Loss_tot:0.1147,\n",
      "save model\n",
      "Save model 5288\n",
      "Train Epoch:5289 learning rate:1.0000e-05, Loss_tot:0.1146,\n",
      "save model\n",
      "Save model 5289\n",
      "Train Epoch:5290 learning rate:1.0000e-05, Loss_tot:0.1146,\n",
      "save model\n",
      "Save model 5290\n",
      "Train Epoch:5291 learning rate:1.0000e-05, Loss_tot:0.1146,\n",
      "save model\n",
      "Save model 5291\n",
      "Train Epoch:5292 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "save model\n",
      "Save model 5292\n",
      "Train Epoch:5293 learning rate:1.0000e-05, Loss_tot:0.1144,\n",
      "save model\n",
      "Save model 5293\n",
      "Train Epoch:5294 learning rate:1.0000e-05, Loss_tot:0.1142,\n",
      "save model\n",
      "Save model 5294\n",
      "Train Epoch:5295 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "save model\n",
      "Save model 5295\n",
      "Train Epoch:5296 learning rate:1.0000e-05, Loss_tot:0.1140,\n",
      "save model\n",
      "Save model 5296\n",
      "Train Epoch:5297 learning rate:1.0000e-05, Loss_tot:0.1139,\n",
      "save model\n",
      "Save model 5297\n",
      "Train Epoch:5298 learning rate:1.0000e-05, Loss_tot:0.1137,\n",
      "save model\n",
      "Save model 5298\n",
      "Train Epoch:5299 learning rate:1.0000e-05, Loss_tot:0.1136,\n",
      "save model\n",
      "Save model 5299\n",
      "Train Epoch:5300 learning rate:1.0000e-05, Loss_tot:0.1134,\n",
      "save model\n",
      "Save model 5300\n",
      "Train Epoch:5301 learning rate:1.0000e-05, Loss_tot:0.1152,\n",
      "Save model 5301\n",
      "Train Epoch:5302 learning rate:1.0000e-05, Loss_tot:0.1151,\n",
      "Save model 5302\n",
      "Train Epoch:5303 learning rate:1.0000e-05, Loss_tot:0.1151,\n",
      "Save model 5303\n",
      "Train Epoch:5304 learning rate:1.0000e-05, Loss_tot:0.1151,\n",
      "Save model 5304\n",
      "Train Epoch:5305 learning rate:1.0000e-05, Loss_tot:0.1151,\n",
      "Save model 5305\n",
      "Train Epoch:5306 learning rate:1.0000e-05, Loss_tot:0.1150,\n",
      "Save model 5306\n",
      "Train Epoch:5307 learning rate:1.0000e-05, Loss_tot:0.1150,\n",
      "Save model 5307\n",
      "Train Epoch:5308 learning rate:1.0000e-05, Loss_tot:0.1149,\n",
      "Save model 5308\n",
      "Train Epoch:5309 learning rate:1.0000e-05, Loss_tot:0.1149,\n",
      "Save model 5309\n",
      "Train Epoch:5310 learning rate:1.0000e-05, Loss_tot:0.1148,\n",
      "Save model 5310\n",
      "Train Epoch:5311 learning rate:1.0000e-05, Loss_tot:0.1148,\n",
      "Save model 5311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5312 learning rate:1.0000e-05, Loss_tot:0.1148,\n",
      "Save model 5312\n",
      "Train Epoch:5313 learning rate:1.0000e-05, Loss_tot:0.1147,\n",
      "Save model 5313\n",
      "Train Epoch:5314 learning rate:1.0000e-05, Loss_tot:0.1147,\n",
      "Save model 5314\n",
      "Train Epoch:5315 learning rate:1.0000e-05, Loss_tot:0.1146,\n",
      "Save model 5315\n",
      "Train Epoch:5316 learning rate:1.0000e-05, Loss_tot:0.1146,\n",
      "Save model 5316\n",
      "Train Epoch:5317 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "Save model 5317\n",
      "Train Epoch:5318 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "Save model 5318\n",
      "Train Epoch:5319 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "Save model 5319\n",
      "Train Epoch:5320 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "Save model 5320\n",
      "Train Epoch:5321 learning rate:1.0000e-05, Loss_tot:0.1144,\n",
      "Save model 5321\n",
      "Train Epoch:5322 learning rate:1.0000e-05, Loss_tot:0.1143,\n",
      "Save model 5322\n",
      "Train Epoch:5323 learning rate:1.0000e-05, Loss_tot:0.1144,\n",
      "Save model 5323\n",
      "Train Epoch:5324 learning rate:1.0000e-05, Loss_tot:0.1143,\n",
      "Save model 5324\n",
      "Train Epoch:5325 learning rate:1.0000e-05, Loss_tot:0.1142,\n",
      "Save model 5325\n",
      "Train Epoch:5326 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "Save model 5326\n",
      "Train Epoch:5327 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "Save model 5327\n",
      "Train Epoch:5328 learning rate:1.0000e-05, Loss_tot:0.1140,\n",
      "Save model 5328\n",
      "Train Epoch:5329 learning rate:1.0000e-05, Loss_tot:0.1139,\n",
      "Save model 5329\n",
      "Train Epoch:5330 learning rate:1.0000e-05, Loss_tot:0.1138,\n",
      "Save model 5330\n",
      "Train Epoch:5331 learning rate:1.0000e-05, Loss_tot:0.1144,\n",
      "Save model 5331\n",
      "Train Epoch:5332 learning rate:1.0000e-05, Loss_tot:0.1143,\n",
      "Save model 5332\n",
      "Train Epoch:5333 learning rate:1.0000e-05, Loss_tot:0.1143,\n",
      "Save model 5333\n",
      "Train Epoch:5334 learning rate:1.0000e-05, Loss_tot:0.1143,\n",
      "Save model 5334\n",
      "Train Epoch:5335 learning rate:1.0000e-05, Loss_tot:0.1142,\n",
      "Save model 5335\n",
      "Train Epoch:5336 learning rate:1.0000e-05, Loss_tot:0.1142,\n",
      "Save model 5336\n",
      "Train Epoch:5337 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "Save model 5337\n",
      "Train Epoch:5338 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "Save model 5338\n",
      "Train Epoch:5339 learning rate:1.0000e-05, Loss_tot:0.1141,\n",
      "Save model 5339\n",
      "Train Epoch:5340 learning rate:1.0000e-05, Loss_tot:0.1140,\n",
      "Save model 5340\n",
      "Train Epoch:5341 learning rate:1.0000e-05, Loss_tot:0.1140,\n",
      "Save model 5341\n",
      "Train Epoch:5342 learning rate:1.0000e-05, Loss_tot:0.1140,\n",
      "Save model 5342\n",
      "Train Epoch:5343 learning rate:1.0000e-05, Loss_tot:0.1139,\n",
      "Save model 5343\n",
      "Train Epoch:5344 learning rate:1.0000e-05, Loss_tot:0.1139,\n",
      "Save model 5344\n",
      "Train Epoch:5345 learning rate:1.0000e-05, Loss_tot:0.1139,\n",
      "Save model 5345\n",
      "Train Epoch:5346 learning rate:1.0000e-05, Loss_tot:0.1138,\n",
      "Save model 5346\n",
      "Train Epoch:5347 learning rate:1.0000e-05, Loss_tot:0.1138,\n",
      "Save model 5347\n",
      "Train Epoch:5348 learning rate:1.0000e-05, Loss_tot:0.1137,\n",
      "Save model 5348\n",
      "Train Epoch:5349 learning rate:1.0000e-05, Loss_tot:0.1137,\n",
      "Save model 5349\n",
      "Train Epoch:5350 learning rate:1.0000e-05, Loss_tot:0.1137,\n",
      "Save model 5350\n",
      "Train Epoch:5351 learning rate:1.0000e-05, Loss_tot:0.1137,\n",
      "Save model 5351\n",
      "Train Epoch:5352 learning rate:1.0000e-05, Loss_tot:0.1136,\n",
      "Save model 5352\n",
      "Train Epoch:5353 learning rate:1.0000e-05, Loss_tot:0.1135,\n",
      "Save model 5353\n",
      "Train Epoch:5354 learning rate:1.0000e-05, Loss_tot:0.1135,\n",
      "Save model 5354\n",
      "Train Epoch:5355 learning rate:1.0000e-05, Loss_tot:0.1135,\n",
      "Save model 5355\n",
      "Train Epoch:5356 learning rate:1.0000e-05, Loss_tot:0.1134,\n",
      "save model\n",
      "Save model 5356\n",
      "Train Epoch:5357 learning rate:1.0000e-05, Loss_tot:0.1134,\n",
      "save model\n",
      "Save model 5357\n",
      "Train Epoch:5358 learning rate:1.0000e-05, Loss_tot:0.1133,\n",
      "save model\n",
      "Save model 5358\n",
      "Train Epoch:5359 learning rate:1.0000e-05, Loss_tot:0.1133,\n",
      "Save model 5359\n",
      "Train Epoch:5360 learning rate:1.0000e-05, Loss_tot:0.1133,\n",
      "save model\n",
      "Save model 5360\n",
      "Train Epoch:5361 learning rate:1.0000e-05, Loss_tot:0.1132,\n",
      "save model\n",
      "Save model 5361\n",
      "Train Epoch:5362 learning rate:1.0000e-05, Loss_tot:0.1132,\n",
      "save model\n",
      "Save model 5362\n",
      "Train Epoch:5363 learning rate:1.0000e-05, Loss_tot:0.1132,\n",
      "save model\n",
      "Save model 5363\n",
      "Train Epoch:5364 learning rate:1.0000e-05, Loss_tot:0.1131,\n",
      "save model\n",
      "Save model 5364\n",
      "Train Epoch:5365 learning rate:1.0000e-05, Loss_tot:0.1131,\n",
      "save model\n",
      "Save model 5365\n",
      "Train Epoch:5366 learning rate:1.0000e-05, Loss_tot:0.1130,\n",
      "save model\n",
      "Save model 5366\n",
      "Train Epoch:5367 learning rate:1.0000e-05, Loss_tot:0.1130,\n",
      "save model\n",
      "Save model 5367\n",
      "Train Epoch:5368 learning rate:1.0000e-05, Loss_tot:0.1129,\n",
      "save model\n",
      "Save model 5368\n",
      "Train Epoch:5369 learning rate:1.0000e-05, Loss_tot:0.1129,\n",
      "save model\n",
      "Save model 5369\n",
      "Train Epoch:5370 learning rate:1.0000e-05, Loss_tot:0.1128,\n",
      "save model\n",
      "Save model 5370\n",
      "Train Epoch:5371 learning rate:1.0000e-05, Loss_tot:0.1129,\n",
      "Save model 5371\n",
      "Train Epoch:5372 learning rate:1.0000e-05, Loss_tot:0.1128,\n",
      "Save model 5372\n",
      "Train Epoch:5373 learning rate:1.0000e-05, Loss_tot:0.1127,\n",
      "save model\n",
      "Save model 5373\n",
      "Train Epoch:5374 learning rate:1.0000e-05, Loss_tot:0.1127,\n",
      "Save model 5374\n",
      "Train Epoch:5375 learning rate:1.0000e-05, Loss_tot:0.1127,\n",
      "Save model 5375\n",
      "Train Epoch:5376 learning rate:1.0000e-05, Loss_tot:0.1127,\n",
      "Save model 5376\n",
      "Train Epoch:5377 learning rate:1.0000e-05, Loss_tot:0.1127,\n",
      "save model\n",
      "Save model 5377\n",
      "Train Epoch:5378 learning rate:1.0000e-05, Loss_tot:0.1126,\n",
      "save model\n",
      "Save model 5378\n",
      "Train Epoch:5379 learning rate:1.0000e-05, Loss_tot:0.1125,\n",
      "save model\n",
      "Save model 5379\n",
      "Train Epoch:5380 learning rate:1.0000e-05, Loss_tot:0.1124,\n",
      "save model\n",
      "Save model 5380\n",
      "Train Epoch:5381 learning rate:1.0000e-05, Loss_tot:0.1125,\n",
      "Save model 5381\n",
      "Train Epoch:5382 learning rate:1.0000e-05, Loss_tot:0.1125,\n",
      "Save model 5382\n",
      "Train Epoch:5383 learning rate:1.0000e-05, Loss_tot:0.1125,\n",
      "Save model 5383\n",
      "Train Epoch:5384 learning rate:1.0000e-05, Loss_tot:0.1123,\n",
      "save model\n",
      "Save model 5384\n",
      "Train Epoch:5385 learning rate:1.0000e-05, Loss_tot:0.1122,\n",
      "save model\n",
      "Save model 5385\n",
      "Train Epoch:5386 learning rate:1.0000e-05, Loss_tot:0.1123,\n",
      "Save model 5386\n",
      "Train Epoch:5387 learning rate:1.0000e-05, Loss_tot:0.1122,\n",
      "Save model 5387\n",
      "Train Epoch:5388 learning rate:1.0000e-05, Loss_tot:0.1122,\n",
      "save model\n",
      "Save model 5388\n",
      "Train Epoch:5389 learning rate:1.0000e-05, Loss_tot:0.1122,\n",
      "save model\n",
      "Save model 5389\n",
      "Train Epoch:5390 learning rate:1.0000e-05, Loss_tot:0.1121,\n",
      "save model\n",
      "Save model 5390\n",
      "Train Epoch:5391 learning rate:1.0000e-05, Loss_tot:0.1120,\n",
      "save model\n",
      "Save model 5391\n",
      "Train Epoch:5392 learning rate:1.0000e-05, Loss_tot:0.1119,\n",
      "save model\n",
      "Save model 5392\n",
      "Train Epoch:5393 learning rate:1.0000e-05, Loss_tot:0.1119,\n",
      "Save model 5393\n",
      "Train Epoch:5394 learning rate:1.0000e-05, Loss_tot:0.1119,\n",
      "save model\n",
      "Save model 5394\n",
      "Train Epoch:5395 learning rate:1.0000e-05, Loss_tot:0.1118,\n",
      "save model\n",
      "Save model 5395\n",
      "Train Epoch:5396 learning rate:1.0000e-05, Loss_tot:0.1118,\n",
      "save model\n",
      "Save model 5396\n",
      "Train Epoch:5397 learning rate:1.0000e-05, Loss_tot:0.1117,\n",
      "save model\n",
      "Save model 5397\n",
      "Train Epoch:5398 learning rate:1.0000e-05, Loss_tot:0.1117,\n",
      "save model\n",
      "Save model 5398\n",
      "Train Epoch:5399 learning rate:1.0000e-05, Loss_tot:0.1116,\n",
      "save model\n",
      "Save model 5399\n",
      "Train Epoch:5400 learning rate:1.0000e-05, Loss_tot:0.1116,\n",
      "Save model 5400\n",
      "Train Epoch:5401 learning rate:1.0000e-05, Loss_tot:0.1116,\n",
      "Save model 5401\n",
      "Train Epoch:5402 learning rate:1.0000e-05, Loss_tot:0.1115,\n",
      "save model\n",
      "Save model 5402\n",
      "Train Epoch:5403 learning rate:1.0000e-05, Loss_tot:0.1115,\n",
      "save model\n",
      "Save model 5403\n",
      "Train Epoch:5404 learning rate:1.0000e-05, Loss_tot:0.1115,\n",
      "Save model 5404\n",
      "Train Epoch:5405 learning rate:1.0000e-05, Loss_tot:0.1114,\n",
      "save model\n",
      "Save model 5405\n",
      "Train Epoch:5406 learning rate:1.0000e-05, Loss_tot:0.1114,\n",
      "save model\n",
      "Save model 5406\n",
      "Train Epoch:5407 learning rate:1.0000e-05, Loss_tot:0.1113,\n",
      "save model\n",
      "Save model 5407\n",
      "Train Epoch:5408 learning rate:1.0000e-05, Loss_tot:0.1112,\n",
      "save model\n",
      "Save model 5408\n",
      "Train Epoch:5409 learning rate:1.0000e-05, Loss_tot:0.1112,\n",
      "Save model 5409\n",
      "Train Epoch:5410 learning rate:1.0000e-05, Loss_tot:0.1112,\n",
      "Save model 5410\n",
      "Train Epoch:5411 learning rate:1.0000e-05, Loss_tot:0.1111,\n",
      "save model\n",
      "Save model 5411\n",
      "Train Epoch:5412 learning rate:1.0000e-05, Loss_tot:0.1110,\n",
      "save model\n",
      "Save model 5412\n",
      "Train Epoch:5413 learning rate:1.0000e-05, Loss_tot:0.1110,\n",
      "save model\n",
      "Save model 5413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5414 learning rate:1.0000e-05, Loss_tot:0.1110,\n",
      "save model\n",
      "Save model 5414\n",
      "Train Epoch:5415 learning rate:1.0000e-05, Loss_tot:0.1110,\n",
      "save model\n",
      "Save model 5415\n",
      "Train Epoch:5416 learning rate:1.0000e-05, Loss_tot:0.1109,\n",
      "save model\n",
      "Save model 5416\n",
      "Train Epoch:5417 learning rate:1.0000e-05, Loss_tot:0.1109,\n",
      "save model\n",
      "Save model 5417\n",
      "Train Epoch:5418 learning rate:1.0000e-05, Loss_tot:0.1108,\n",
      "save model\n",
      "Save model 5418\n",
      "Train Epoch:5419 learning rate:1.0000e-05, Loss_tot:0.1107,\n",
      "save model\n",
      "Save model 5419\n",
      "Train Epoch:5420 learning rate:1.0000e-05, Loss_tot:0.1107,\n",
      "save model\n",
      "Save model 5420\n",
      "Train Epoch:5421 learning rate:1.0000e-05, Loss_tot:0.1106,\n",
      "save model\n",
      "Save model 5421\n",
      "Train Epoch:5422 learning rate:1.0000e-05, Loss_tot:0.1107,\n",
      "Save model 5422\n",
      "Train Epoch:5423 learning rate:1.0000e-05, Loss_tot:0.1106,\n",
      "save model\n",
      "Save model 5423\n",
      "Train Epoch:5424 learning rate:1.0000e-05, Loss_tot:0.1105,\n",
      "save model\n",
      "Save model 5424\n",
      "Train Epoch:5425 learning rate:1.0000e-05, Loss_tot:0.1105,\n",
      "save model\n",
      "Save model 5425\n",
      "Train Epoch:5426 learning rate:1.0000e-05, Loss_tot:0.1105,\n",
      "save model\n",
      "Save model 5426\n",
      "Train Epoch:5427 learning rate:1.0000e-05, Loss_tot:0.1104,\n",
      "save model\n",
      "Save model 5427\n",
      "Train Epoch:5428 learning rate:1.0000e-05, Loss_tot:0.1103,\n",
      "save model\n",
      "Save model 5428\n",
      "Train Epoch:5429 learning rate:1.0000e-05, Loss_tot:0.1103,\n",
      "save model\n",
      "Save model 5429\n",
      "Train Epoch:5430 learning rate:1.0000e-05, Loss_tot:0.1103,\n",
      "Save model 5430\n",
      "Train Epoch:5431 learning rate:1.0000e-05, Loss_tot:0.1103,\n",
      "save model\n",
      "Save model 5431\n",
      "Train Epoch:5432 learning rate:1.0000e-05, Loss_tot:0.1102,\n",
      "save model\n",
      "Save model 5432\n",
      "Train Epoch:5433 learning rate:1.0000e-05, Loss_tot:0.1101,\n",
      "save model\n",
      "Save model 5433\n",
      "Train Epoch:5434 learning rate:1.0000e-05, Loss_tot:0.1101,\n",
      "save model\n",
      "Save model 5434\n",
      "Train Epoch:5435 learning rate:1.0000e-05, Loss_tot:0.1101,\n",
      "save model\n",
      "Save model 5435\n",
      "Train Epoch:5436 learning rate:1.0000e-05, Loss_tot:0.1100,\n",
      "save model\n",
      "Save model 5436\n",
      "Train Epoch:5437 learning rate:1.0000e-05, Loss_tot:0.1100,\n",
      "save model\n",
      "Save model 5437\n",
      "Train Epoch:5438 learning rate:1.0000e-05, Loss_tot:0.1099,\n",
      "save model\n",
      "Save model 5438\n",
      "Train Epoch:5439 learning rate:1.0000e-05, Loss_tot:0.1099,\n",
      "save model\n",
      "Save model 5439\n",
      "Train Epoch:5440 learning rate:1.0000e-05, Loss_tot:0.1098,\n",
      "save model\n",
      "Save model 5440\n",
      "Train Epoch:5441 learning rate:1.0000e-05, Loss_tot:0.1098,\n",
      "save model\n",
      "Save model 5441\n",
      "Train Epoch:5442 learning rate:1.0000e-05, Loss_tot:0.1098,\n",
      "Save model 5442\n",
      "Train Epoch:5443 learning rate:1.0000e-05, Loss_tot:0.1097,\n",
      "save model\n",
      "Save model 5443\n",
      "Train Epoch:5444 learning rate:1.0000e-05, Loss_tot:0.1097,\n",
      "save model\n",
      "Save model 5444\n",
      "Train Epoch:5445 learning rate:1.0000e-05, Loss_tot:0.1097,\n",
      "save model\n",
      "Save model 5445\n",
      "Train Epoch:5446 learning rate:1.0000e-05, Loss_tot:0.1096,\n",
      "save model\n",
      "Save model 5446\n",
      "Train Epoch:5447 learning rate:1.0000e-05, Loss_tot:0.1096,\n",
      "save model\n",
      "Save model 5447\n",
      "Train Epoch:5448 learning rate:1.0000e-05, Loss_tot:0.1095,\n",
      "save model\n",
      "Save model 5448\n",
      "Train Epoch:5449 learning rate:1.0000e-05, Loss_tot:0.1095,\n",
      "Save model 5449\n",
      "Train Epoch:5450 learning rate:1.0000e-05, Loss_tot:0.1095,\n",
      "Save model 5450\n",
      "Train Epoch:5451 learning rate:1.0000e-05, Loss_tot:0.1094,\n",
      "save model\n",
      "Save model 5451\n",
      "Train Epoch:5452 learning rate:1.0000e-05, Loss_tot:0.1093,\n",
      "save model\n",
      "Save model 5452\n",
      "Train Epoch:5453 learning rate:1.0000e-05, Loss_tot:0.1093,\n",
      "Save model 5453\n",
      "Train Epoch:5454 learning rate:1.0000e-05, Loss_tot:0.1093,\n",
      "save model\n",
      "Save model 5454\n",
      "Train Epoch:5455 learning rate:1.0000e-05, Loss_tot:0.1093,\n",
      "save model\n",
      "Save model 5455\n",
      "Train Epoch:5456 learning rate:1.0000e-05, Loss_tot:0.1092,\n",
      "save model\n",
      "Save model 5456\n",
      "Train Epoch:5457 learning rate:1.0000e-05, Loss_tot:0.1091,\n",
      "save model\n",
      "Save model 5457\n",
      "Train Epoch:5458 learning rate:1.0000e-05, Loss_tot:0.1092,\n",
      "Save model 5458\n",
      "Train Epoch:5459 learning rate:1.0000e-05, Loss_tot:0.1092,\n",
      "Save model 5459\n",
      "Train Epoch:5460 learning rate:1.0000e-05, Loss_tot:0.1091,\n",
      "save model\n",
      "Save model 5460\n",
      "Train Epoch:5461 learning rate:1.0000e-05, Loss_tot:0.1089,\n",
      "save model\n",
      "Save model 5461\n",
      "Train Epoch:5462 learning rate:1.0000e-05, Loss_tot:0.1089,\n",
      "save model\n",
      "Save model 5462\n",
      "Train Epoch:5463 learning rate:1.0000e-05, Loss_tot:0.1089,\n",
      "save model\n",
      "Save model 5463\n",
      "Train Epoch:5464 learning rate:1.0000e-05, Loss_tot:0.1088,\n",
      "save model\n",
      "Save model 5464\n",
      "Train Epoch:5465 learning rate:1.0000e-05, Loss_tot:0.1087,\n",
      "save model\n",
      "Save model 5465\n",
      "Train Epoch:5466 learning rate:1.0000e-05, Loss_tot:0.1088,\n",
      "Save model 5466\n",
      "Train Epoch:5467 learning rate:1.0000e-05, Loss_tot:0.1088,\n",
      "Save model 5467\n",
      "Train Epoch:5468 learning rate:1.0000e-05, Loss_tot:0.1086,\n",
      "save model\n",
      "Save model 5468\n",
      "Train Epoch:5469 learning rate:1.0000e-05, Loss_tot:0.1086,\n",
      "save model\n",
      "Save model 5469\n",
      "Train Epoch:5470 learning rate:1.0000e-05, Loss_tot:0.1086,\n",
      "Save model 5470\n",
      "Train Epoch:5471 learning rate:1.0000e-05, Loss_tot:0.1086,\n",
      "save model\n",
      "Save model 5471\n",
      "Train Epoch:5472 learning rate:1.0000e-05, Loss_tot:0.1085,\n",
      "save model\n",
      "Save model 5472\n",
      "Train Epoch:5473 learning rate:1.0000e-05, Loss_tot:0.1085,\n",
      "save model\n",
      "Save model 5473\n",
      "Train Epoch:5474 learning rate:1.0000e-05, Loss_tot:0.1084,\n",
      "save model\n",
      "Save model 5474\n",
      "Train Epoch:5475 learning rate:1.0000e-05, Loss_tot:0.1083,\n",
      "save model\n",
      "Save model 5475\n",
      "Train Epoch:5476 learning rate:1.0000e-05, Loss_tot:0.1083,\n",
      "save model\n",
      "Save model 5476\n",
      "Train Epoch:5477 learning rate:1.0000e-05, Loss_tot:0.1082,\n",
      "save model\n",
      "Save model 5477\n",
      "Train Epoch:5478 learning rate:1.0000e-05, Loss_tot:0.1082,\n",
      "save model\n",
      "Save model 5478\n",
      "Train Epoch:5479 learning rate:1.0000e-05, Loss_tot:0.1082,\n",
      "save model\n",
      "Save model 5479\n",
      "Train Epoch:5480 learning rate:1.0000e-05, Loss_tot:0.1082,\n",
      "save model\n",
      "Save model 5480\n",
      "Train Epoch:5481 learning rate:1.0000e-05, Loss_tot:0.1081,\n",
      "save model\n",
      "Save model 5481\n",
      "Train Epoch:5482 learning rate:1.0000e-05, Loss_tot:0.1080,\n",
      "save model\n",
      "Save model 5482\n",
      "Train Epoch:5483 learning rate:1.0000e-05, Loss_tot:0.1079,\n",
      "save model\n",
      "Save model 5483\n",
      "Train Epoch:5484 learning rate:1.0000e-05, Loss_tot:0.1080,\n",
      "Save model 5484\n",
      "Train Epoch:5485 learning rate:1.0000e-05, Loss_tot:0.1080,\n",
      "Save model 5485\n",
      "Train Epoch:5486 learning rate:1.0000e-05, Loss_tot:0.1079,\n",
      "Save model 5486\n",
      "Train Epoch:5487 learning rate:1.0000e-05, Loss_tot:0.1078,\n",
      "save model\n",
      "Save model 5487\n",
      "Train Epoch:5488 learning rate:1.0000e-05, Loss_tot:0.1078,\n",
      "Save model 5488\n",
      "Train Epoch:5489 learning rate:1.0000e-05, Loss_tot:0.1079,\n",
      "Save model 5489\n",
      "Train Epoch:5490 learning rate:1.0000e-05, Loss_tot:0.1079,\n",
      "Save model 5490\n",
      "Train Epoch:5491 learning rate:1.0000e-05, Loss_tot:0.1078,\n",
      "Save model 5491\n",
      "Train Epoch:5492 learning rate:1.0000e-05, Loss_tot:0.1078,\n",
      "Save model 5492\n",
      "Train Epoch:5493 learning rate:1.0000e-05, Loss_tot:0.1077,\n",
      "save model\n",
      "Save model 5493\n",
      "Train Epoch:5494 learning rate:1.0000e-05, Loss_tot:0.1076,\n",
      "save model\n",
      "Save model 5494\n",
      "Train Epoch:5495 learning rate:1.0000e-05, Loss_tot:0.1075,\n",
      "save model\n",
      "Save model 5495\n",
      "Train Epoch:5496 learning rate:1.0000e-05, Loss_tot:0.1075,\n",
      "Save model 5496\n",
      "Train Epoch:5497 learning rate:1.0000e-05, Loss_tot:0.1075,\n",
      "Save model 5497\n",
      "Train Epoch:5498 learning rate:1.0000e-05, Loss_tot:0.1074,\n",
      "save model\n",
      "Save model 5498\n",
      "Train Epoch:5499 learning rate:1.0000e-05, Loss_tot:0.1073,\n",
      "save model\n",
      "Save model 5499\n",
      "Train Epoch:5500 learning rate:1.0000e-05, Loss_tot:0.1073,\n",
      "Save model 5500\n",
      "Train Epoch:5501 learning rate:1.0000e-05, Loss_tot:0.1073,\n",
      "Save model 5501\n",
      "Train Epoch:5502 learning rate:1.0000e-05, Loss_tot:0.1073,\n",
      "Save model 5502\n",
      "Train Epoch:5503 learning rate:1.0000e-05, Loss_tot:0.1072,\n",
      "save model\n",
      "Save model 5503\n",
      "Train Epoch:5504 learning rate:1.0000e-05, Loss_tot:0.1072,\n",
      "save model\n",
      "Save model 5504\n",
      "Train Epoch:5505 learning rate:1.0000e-05, Loss_tot:0.1071,\n",
      "save model\n",
      "Save model 5505\n",
      "Train Epoch:5506 learning rate:1.0000e-05, Loss_tot:0.1070,\n",
      "save model\n",
      "Save model 5506\n",
      "Train Epoch:5507 learning rate:1.0000e-05, Loss_tot:0.1069,\n",
      "save model\n",
      "Save model 5507\n",
      "Train Epoch:5508 learning rate:1.0000e-05, Loss_tot:0.1070,\n",
      "Save model 5508\n",
      "Train Epoch:5509 learning rate:1.0000e-05, Loss_tot:0.1069,\n",
      "save model\n",
      "Save model 5509\n",
      "Train Epoch:5510 learning rate:1.0000e-05, Loss_tot:0.1068,\n",
      "save model\n",
      "Save model 5510\n",
      "Train Epoch:5511 learning rate:1.0000e-05, Loss_tot:0.1068,\n",
      "save model\n",
      "Save model 5511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5512 learning rate:1.0000e-05, Loss_tot:0.1067,\n",
      "save model\n",
      "Save model 5512\n",
      "Train Epoch:5513 learning rate:1.0000e-05, Loss_tot:0.1067,\n",
      "save model\n",
      "Save model 5513\n",
      "Train Epoch:5514 learning rate:1.0000e-05, Loss_tot:0.1066,\n",
      "save model\n",
      "Save model 5514\n",
      "Train Epoch:5515 learning rate:1.0000e-05, Loss_tot:0.1066,\n",
      "Save model 5515\n",
      "Train Epoch:5516 learning rate:1.0000e-05, Loss_tot:0.1066,\n",
      "save model\n",
      "Save model 5516\n",
      "Train Epoch:5517 learning rate:1.0000e-05, Loss_tot:0.1064,\n",
      "save model\n",
      "Save model 5517\n",
      "Train Epoch:5518 learning rate:1.0000e-05, Loss_tot:0.1064,\n",
      "Save model 5518\n",
      "Train Epoch:5519 learning rate:1.0000e-05, Loss_tot:0.1064,\n",
      "Save model 5519\n",
      "Train Epoch:5520 learning rate:1.0000e-05, Loss_tot:0.1064,\n",
      "save model\n",
      "Save model 5520\n",
      "Train Epoch:5521 learning rate:1.0000e-05, Loss_tot:0.1064,\n",
      "save model\n",
      "Save model 5521\n",
      "Train Epoch:5522 learning rate:1.0000e-05, Loss_tot:0.1063,\n",
      "save model\n",
      "Save model 5522\n",
      "Train Epoch:5523 learning rate:1.0000e-05, Loss_tot:0.1062,\n",
      "save model\n",
      "Save model 5523\n",
      "Train Epoch:5524 learning rate:1.0000e-05, Loss_tot:0.1062,\n",
      "save model\n",
      "Save model 5524\n",
      "Train Epoch:5525 learning rate:1.0000e-05, Loss_tot:0.1062,\n",
      "save model\n",
      "Save model 5525\n",
      "Train Epoch:5526 learning rate:1.0000e-05, Loss_tot:0.1061,\n",
      "save model\n",
      "Save model 5526\n",
      "Train Epoch:5527 learning rate:1.0000e-05, Loss_tot:0.1060,\n",
      "save model\n",
      "Save model 5527\n",
      "Train Epoch:5528 learning rate:1.0000e-05, Loss_tot:0.1060,\n",
      "save model\n",
      "Save model 5528\n",
      "Train Epoch:5529 learning rate:1.0000e-05, Loss_tot:0.1060,\n",
      "save model\n",
      "Save model 5529\n",
      "Train Epoch:5530 learning rate:1.0000e-05, Loss_tot:0.1059,\n",
      "save model\n",
      "Save model 5530\n",
      "Train Epoch:5531 learning rate:1.0000e-05, Loss_tot:0.1058,\n",
      "save model\n",
      "Save model 5531\n",
      "Train Epoch:5532 learning rate:1.0000e-05, Loss_tot:0.1058,\n",
      "save model\n",
      "Save model 5532\n",
      "Train Epoch:5533 learning rate:1.0000e-05, Loss_tot:0.1057,\n",
      "save model\n",
      "Save model 5533\n",
      "Train Epoch:5534 learning rate:1.0000e-05, Loss_tot:0.1057,\n",
      "save model\n",
      "Save model 5534\n",
      "Train Epoch:5535 learning rate:1.0000e-05, Loss_tot:0.1057,\n",
      "save model\n",
      "Save model 5535\n",
      "Train Epoch:5536 learning rate:1.0000e-05, Loss_tot:0.1056,\n",
      "save model\n",
      "Save model 5536\n",
      "Train Epoch:5537 learning rate:1.0000e-05, Loss_tot:0.1056,\n",
      "save model\n",
      "Save model 5537\n",
      "Train Epoch:5538 learning rate:1.0000e-05, Loss_tot:0.1055,\n",
      "save model\n",
      "Save model 5538\n",
      "Train Epoch:5539 learning rate:1.0000e-05, Loss_tot:0.1055,\n",
      "Save model 5539\n",
      "Train Epoch:5540 learning rate:1.0000e-05, Loss_tot:0.1055,\n",
      "save model\n",
      "Save model 5540\n",
      "Train Epoch:5541 learning rate:1.0000e-05, Loss_tot:0.1054,\n",
      "save model\n",
      "Save model 5541\n",
      "Train Epoch:5542 learning rate:1.0000e-05, Loss_tot:0.1054,\n",
      "save model\n",
      "Save model 5542\n",
      "Train Epoch:5543 learning rate:1.0000e-05, Loss_tot:0.1053,\n",
      "save model\n",
      "Save model 5543\n",
      "Train Epoch:5544 learning rate:1.0000e-05, Loss_tot:0.1053,\n",
      "Save model 5544\n",
      "Train Epoch:5545 learning rate:1.0000e-05, Loss_tot:0.1053,\n",
      "Save model 5545\n",
      "Train Epoch:5546 learning rate:1.0000e-05, Loss_tot:0.1052,\n",
      "save model\n",
      "Save model 5546\n",
      "Train Epoch:5547 learning rate:1.0000e-05, Loss_tot:0.1051,\n",
      "save model\n",
      "Save model 5547\n",
      "Train Epoch:5548 learning rate:1.0000e-05, Loss_tot:0.1051,\n",
      "Save model 5548\n",
      "Train Epoch:5549 learning rate:1.0000e-05, Loss_tot:0.1051,\n",
      "save model\n",
      "Save model 5549\n",
      "Train Epoch:5550 learning rate:1.0000e-05, Loss_tot:0.1050,\n",
      "save model\n",
      "Save model 5550\n",
      "Train Epoch:5551 learning rate:1.0000e-05, Loss_tot:0.1050,\n",
      "save model\n",
      "Save model 5551\n",
      "Train Epoch:5552 learning rate:1.0000e-05, Loss_tot:0.1049,\n",
      "save model\n",
      "Save model 5552\n",
      "Train Epoch:5553 learning rate:1.0000e-05, Loss_tot:0.1049,\n",
      "Save model 5553\n",
      "Train Epoch:5554 learning rate:1.0000e-05, Loss_tot:0.1049,\n",
      "Save model 5554\n",
      "Train Epoch:5555 learning rate:1.0000e-05, Loss_tot:0.1048,\n",
      "save model\n",
      "Save model 5555\n",
      "Train Epoch:5556 learning rate:1.0000e-05, Loss_tot:0.1047,\n",
      "save model\n",
      "Save model 5556\n",
      "Train Epoch:5557 learning rate:1.0000e-05, Loss_tot:0.1047,\n",
      "save model\n",
      "Save model 5557\n",
      "Train Epoch:5558 learning rate:1.0000e-05, Loss_tot:0.1046,\n",
      "save model\n",
      "Save model 5558\n",
      "Train Epoch:5559 learning rate:1.0000e-05, Loss_tot:0.1046,\n",
      "save model\n",
      "Save model 5559\n",
      "Train Epoch:5560 learning rate:1.0000e-05, Loss_tot:0.1045,\n",
      "save model\n",
      "Save model 5560\n",
      "Train Epoch:5561 learning rate:1.0000e-05, Loss_tot:0.1045,\n",
      "Save model 5561\n",
      "Train Epoch:5562 learning rate:1.0000e-05, Loss_tot:0.1045,\n",
      "Save model 5562\n",
      "Train Epoch:5563 learning rate:1.0000e-05, Loss_tot:0.1044,\n",
      "save model\n",
      "Save model 5563\n",
      "Train Epoch:5564 learning rate:1.0000e-05, Loss_tot:0.1044,\n",
      "Save model 5564\n",
      "Train Epoch:5565 learning rate:1.0000e-05, Loss_tot:0.1044,\n",
      "Save model 5565\n",
      "Train Epoch:5566 learning rate:1.0000e-05, Loss_tot:0.1044,\n",
      "save model\n",
      "Save model 5566\n",
      "Train Epoch:5567 learning rate:1.0000e-05, Loss_tot:0.1043,\n",
      "save model\n",
      "Save model 5567\n",
      "Train Epoch:5568 learning rate:1.0000e-05, Loss_tot:0.1042,\n",
      "save model\n",
      "Save model 5568\n",
      "Train Epoch:5569 learning rate:1.0000e-05, Loss_tot:0.1041,\n",
      "save model\n",
      "Save model 5569\n",
      "Train Epoch:5570 learning rate:1.0000e-05, Loss_tot:0.1041,\n",
      "save model\n",
      "Save model 5570\n",
      "Train Epoch:5571 learning rate:1.0000e-05, Loss_tot:0.1041,\n",
      "save model\n",
      "Save model 5571\n",
      "Train Epoch:5572 learning rate:1.0000e-05, Loss_tot:0.1039,\n",
      "save model\n",
      "Save model 5572\n",
      "Train Epoch:5573 learning rate:1.0000e-05, Loss_tot:0.1040,\n",
      "Save model 5573\n",
      "Train Epoch:5574 learning rate:1.0000e-05, Loss_tot:0.1040,\n",
      "Save model 5574\n",
      "Train Epoch:5575 learning rate:1.0000e-05, Loss_tot:0.1039,\n",
      "save model\n",
      "Save model 5575\n",
      "Train Epoch:5576 learning rate:1.0000e-05, Loss_tot:0.1039,\n",
      "save model\n",
      "Save model 5576\n",
      "Train Epoch:5577 learning rate:1.0000e-05, Loss_tot:0.1038,\n",
      "save model\n",
      "Save model 5577\n",
      "Train Epoch:5578 learning rate:1.0000e-05, Loss_tot:0.1037,\n",
      "save model\n",
      "Save model 5578\n",
      "Train Epoch:5579 learning rate:1.0000e-05, Loss_tot:0.1038,\n",
      "Save model 5579\n",
      "Train Epoch:5580 learning rate:1.0000e-05, Loss_tot:0.1038,\n",
      "Save model 5580\n",
      "Train Epoch:5581 learning rate:1.0000e-05, Loss_tot:0.1037,\n",
      "save model\n",
      "Save model 5581\n",
      "Train Epoch:5582 learning rate:1.0000e-05, Loss_tot:0.1035,\n",
      "save model\n",
      "Save model 5582\n",
      "Train Epoch:5583 learning rate:1.0000e-05, Loss_tot:0.1035,\n",
      "save model\n",
      "Save model 5583\n",
      "Train Epoch:5584 learning rate:1.0000e-05, Loss_tot:0.1035,\n",
      "save model\n",
      "Save model 5584\n",
      "Train Epoch:5585 learning rate:1.0000e-05, Loss_tot:0.1034,\n",
      "save model\n",
      "Save model 5585\n",
      "Train Epoch:5586 learning rate:1.0000e-05, Loss_tot:0.1033,\n",
      "save model\n",
      "Save model 5586\n",
      "Train Epoch:5587 learning rate:1.0000e-05, Loss_tot:0.1034,\n",
      "Save model 5587\n",
      "Train Epoch:5588 learning rate:1.0000e-05, Loss_tot:0.1033,\n",
      "Save model 5588\n",
      "Train Epoch:5589 learning rate:1.0000e-05, Loss_tot:0.1032,\n",
      "save model\n",
      "Save model 5589\n",
      "Train Epoch:5590 learning rate:1.0000e-05, Loss_tot:0.1032,\n",
      "save model\n",
      "Save model 5590\n",
      "Train Epoch:5591 learning rate:1.0000e-05, Loss_tot:0.1032,\n",
      "Save model 5591\n",
      "Train Epoch:5592 learning rate:1.0000e-05, Loss_tot:0.1031,\n",
      "save model\n",
      "Save model 5592\n",
      "Train Epoch:5593 learning rate:1.0000e-05, Loss_tot:0.1031,\n",
      "save model\n",
      "Save model 5593\n",
      "Train Epoch:5594 learning rate:1.0000e-05, Loss_tot:0.1030,\n",
      "save model\n",
      "Save model 5594\n",
      "Train Epoch:5595 learning rate:1.0000e-05, Loss_tot:0.1029,\n",
      "save model\n",
      "Save model 5595\n",
      "Train Epoch:5596 learning rate:1.0000e-05, Loss_tot:0.1029,\n",
      "Save model 5596\n",
      "Train Epoch:5597 learning rate:1.0000e-05, Loss_tot:0.1029,\n",
      "Save model 5597\n",
      "Train Epoch:5598 learning rate:1.0000e-05, Loss_tot:0.1028,\n",
      "save model\n",
      "Save model 5598\n",
      "Train Epoch:5599 learning rate:1.0000e-05, Loss_tot:0.1027,\n",
      "save model\n",
      "Save model 5599\n",
      "Train Epoch:5600 learning rate:1.0000e-05, Loss_tot:0.1027,\n",
      "save model\n",
      "Save model 5600\n",
      "Train Epoch:5601 learning rate:1.0000e-05, Loss_tot:0.1027,\n",
      "save model\n",
      "Save model 5601\n",
      "Train Epoch:5602 learning rate:1.0000e-05, Loss_tot:0.1026,\n",
      "save model\n",
      "Save model 5602\n",
      "Train Epoch:5603 learning rate:1.0000e-05, Loss_tot:0.1026,\n",
      "save model\n",
      "Save model 5603\n",
      "Train Epoch:5604 learning rate:1.0000e-05, Loss_tot:0.1026,\n",
      "Save model 5604\n",
      "Train Epoch:5605 learning rate:1.0000e-05, Loss_tot:0.1026,\n",
      "Save model 5605\n",
      "Train Epoch:5606 learning rate:1.0000e-05, Loss_tot:0.1025,\n",
      "save model\n",
      "Save model 5606\n",
      "Train Epoch:5607 learning rate:1.0000e-05, Loss_tot:0.1024,\n",
      "save model\n",
      "Save model 5607\n",
      "Train Epoch:5608 learning rate:1.0000e-05, Loss_tot:0.1025,\n",
      "Save model 5608\n",
      "Train Epoch:5609 learning rate:1.0000e-05, Loss_tot:0.1024,\n",
      "save model\n",
      "Save model 5609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5610 learning rate:1.0000e-05, Loss_tot:0.1024,\n",
      "save model\n",
      "Save model 5610\n",
      "Train Epoch:5611 learning rate:1.0000e-05, Loss_tot:0.1023,\n",
      "save model\n",
      "Save model 5611\n",
      "Train Epoch:5612 learning rate:1.0000e-05, Loss_tot:0.1022,\n",
      "save model\n",
      "Save model 5612\n",
      "Train Epoch:5613 learning rate:1.0000e-05, Loss_tot:0.1022,\n",
      "save model\n",
      "Save model 5613\n",
      "Train Epoch:5614 learning rate:1.0000e-05, Loss_tot:0.1022,\n",
      "save model\n",
      "Save model 5614\n",
      "Train Epoch:5615 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "save model\n",
      "Save model 5615\n",
      "Train Epoch:5616 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "save model\n",
      "Save model 5616\n",
      "Train Epoch:5617 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "Save model 5617\n",
      "Train Epoch:5618 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "save model\n",
      "Save model 5618\n",
      "Train Epoch:5619 learning rate:1.0000e-05, Loss_tot:0.1020,\n",
      "save model\n",
      "Save model 5619\n",
      "Train Epoch:5620 learning rate:1.0000e-05, Loss_tot:0.1020,\n",
      "save model\n",
      "Save model 5620\n",
      "Train Epoch:5621 learning rate:1.0000e-05, Loss_tot:0.1020,\n",
      "save model\n",
      "Save model 5621\n",
      "Train Epoch:5622 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "save model\n",
      "Save model 5622\n",
      "Train Epoch:5623 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "Save model 5623\n",
      "Train Epoch:5624 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "Save model 5624\n",
      "Train Epoch:5625 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "save model\n",
      "Save model 5625\n",
      "Train Epoch:5626 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "Save model 5626\n",
      "Train Epoch:5627 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "Save model 5627\n",
      "Train Epoch:5628 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "Save model 5628\n",
      "Train Epoch:5629 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "Save model 5629\n",
      "Train Epoch:5630 learning rate:1.0000e-05, Loss_tot:0.1017,\n",
      "save model\n",
      "Save model 5630\n",
      "Train Epoch:5631 learning rate:1.0000e-05, Loss_tot:0.1016,\n",
      "save model\n",
      "Save model 5631\n",
      "Train Epoch:5632 learning rate:1.0000e-05, Loss_tot:0.1017,\n",
      "Save model 5632\n",
      "Train Epoch:5633 learning rate:1.0000e-05, Loss_tot:0.1017,\n",
      "Save model 5633\n",
      "Train Epoch:5634 learning rate:1.0000e-05, Loss_tot:0.1016,\n",
      "save model\n",
      "Save model 5634\n",
      "Train Epoch:5635 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "save model\n",
      "Save model 5635\n",
      "Train Epoch:5636 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "Save model 5636\n",
      "Train Epoch:5637 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "save model\n",
      "Save model 5637\n",
      "Train Epoch:5638 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "save model\n",
      "Save model 5638\n",
      "Train Epoch:5639 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "save model\n",
      "Save model 5639\n",
      "Train Epoch:5640 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "save model\n",
      "Save model 5640\n",
      "Train Epoch:5641 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "save model\n",
      "Save model 5641\n",
      "Train Epoch:5642 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "save model\n",
      "Save model 5642\n",
      "Train Epoch:5643 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "save model\n",
      "Save model 5643\n",
      "Train Epoch:5644 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "save model\n",
      "Save model 5644\n",
      "Train Epoch:5645 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 5645\n",
      "Train Epoch:5646 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 5646\n",
      "Train Epoch:5647 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "save model\n",
      "Save model 5647\n",
      "Train Epoch:5648 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 5648\n",
      "Train Epoch:5649 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 5649\n",
      "Train Epoch:5650 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 5650\n",
      "Train Epoch:5651 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "save model\n",
      "Save model 5651\n",
      "Train Epoch:5652 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "save model\n",
      "Save model 5652\n",
      "Train Epoch:5653 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "save model\n",
      "Save model 5653\n",
      "Train Epoch:5654 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 5654\n",
      "Train Epoch:5655 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 5655\n",
      "Train Epoch:5656 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 5656\n",
      "Train Epoch:5657 learning rate:1.0000e-05, Loss_tot:0.1009,\n",
      "save model\n",
      "Save model 5657\n",
      "Train Epoch:5658 learning rate:1.0000e-05, Loss_tot:0.1009,\n",
      "Save model 5658\n",
      "Train Epoch:5659 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 5659\n",
      "Train Epoch:5660 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 5660\n",
      "Train Epoch:5661 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 5661\n",
      "Train Epoch:5662 learning rate:1.0000e-05, Loss_tot:0.1009,\n",
      "Save model 5662\n",
      "Train Epoch:5663 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "save model\n",
      "Save model 5663\n",
      "Train Epoch:5664 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "save model\n",
      "Save model 5664\n",
      "Train Epoch:5665 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 5665\n",
      "Train Epoch:5666 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 5666\n",
      "Train Epoch:5667 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 5667\n",
      "Train Epoch:5668 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "save model\n",
      "Save model 5668\n",
      "Train Epoch:5669 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 5669\n",
      "Train Epoch:5670 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 5670\n",
      "Train Epoch:5671 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 5671\n",
      "Train Epoch:5672 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 5672\n",
      "Train Epoch:5673 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "save model\n",
      "Save model 5673\n",
      "Train Epoch:5674 learning rate:1.0000e-05, Loss_tot:0.1005,\n",
      "save model\n",
      "Save model 5674\n",
      "Train Epoch:5675 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "save model\n",
      "Save model 5675\n",
      "Train Epoch:5676 learning rate:1.0000e-05, Loss_tot:0.1005,\n",
      "Save model 5676\n",
      "Train Epoch:5677 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "save model\n",
      "Save model 5677\n",
      "Train Epoch:5678 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "save model\n",
      "Save model 5678\n",
      "Train Epoch:5679 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 5679\n",
      "Train Epoch:5680 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 5680\n",
      "Train Epoch:5681 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "save model\n",
      "Save model 5681\n",
      "Train Epoch:5682 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "save model\n",
      "Save model 5682\n",
      "Train Epoch:5683 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "save model\n",
      "Save model 5683\n",
      "Train Epoch:5684 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 5684\n",
      "Train Epoch:5685 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "save model\n",
      "Save model 5685\n",
      "Train Epoch:5686 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "save model\n",
      "Save model 5686\n",
      "Train Epoch:5687 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 5687\n",
      "Train Epoch:5688 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "save model\n",
      "Save model 5688\n",
      "Train Epoch:5689 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 5689\n",
      "Train Epoch:5690 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 5690\n",
      "Train Epoch:5691 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 5691\n",
      "Train Epoch:5692 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "save model\n",
      "Save model 5692\n",
      "Train Epoch:5693 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "save model\n",
      "Save model 5693\n",
      "Train Epoch:5694 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 5694\n",
      "Train Epoch:5695 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 5695\n",
      "Train Epoch:5696 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 5696\n",
      "Train Epoch:5697 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "save model\n",
      "Save model 5697\n",
      "Train Epoch:5698 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 5698\n",
      "Train Epoch:5699 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "save model\n",
      "Save model 5699\n",
      "Train Epoch:5700 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "save model\n",
      "Save model 5700\n",
      "Train Epoch:5701 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 5701\n",
      "Train Epoch:5702 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 5702\n",
      "Train Epoch:5703 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "save model\n",
      "Save model 5703\n",
      "Train Epoch:5704 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "save model\n",
      "Save model 5704\n",
      "Train Epoch:5705 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "save model\n",
      "Save model 5705\n",
      "Train Epoch:5706 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 5706\n",
      "Train Epoch:5707 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "save model\n",
      "Save model 5707\n",
      "Train Epoch:5708 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "save model\n",
      "Save model 5708\n",
      "Train Epoch:5709 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 5709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5710 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "save model\n",
      "Save model 5710\n",
      "Train Epoch:5711 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "save model\n",
      "Save model 5711\n",
      "Train Epoch:5712 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 5712\n",
      "Train Epoch:5713 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 5713\n",
      "Train Epoch:5714 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "save model\n",
      "Save model 5714\n",
      "Train Epoch:5715 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "Save model 5715\n",
      "Train Epoch:5716 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "Save model 5716\n",
      "Train Epoch:5717 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "Save model 5717\n",
      "Train Epoch:5718 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "save model\n",
      "Save model 5718\n",
      "Train Epoch:5719 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "save model\n",
      "Save model 5719\n",
      "Train Epoch:5720 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "save model\n",
      "Save model 5720\n",
      "Train Epoch:5721 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 5721\n",
      "Train Epoch:5722 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "save model\n",
      "Save model 5722\n",
      "Train Epoch:5723 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "save model\n",
      "Save model 5723\n",
      "Train Epoch:5724 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "save model\n",
      "Save model 5724\n",
      "Train Epoch:5725 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "save model\n",
      "Save model 5725\n",
      "Train Epoch:5726 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "save model\n",
      "Save model 5726\n",
      "Train Epoch:5727 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 5727\n",
      "Train Epoch:5728 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "save model\n",
      "Save model 5728\n",
      "Train Epoch:5729 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "save model\n",
      "Save model 5729\n",
      "Train Epoch:5730 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 5730\n",
      "Train Epoch:5731 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "save model\n",
      "Save model 5731\n",
      "Train Epoch:5732 learning rate:1.0000e-05, Loss_tot:0.0992,\n",
      "save model\n",
      "Save model 5732\n",
      "Train Epoch:5733 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 5733\n",
      "Train Epoch:5734 learning rate:1.0000e-05, Loss_tot:0.0992,\n",
      "save model\n",
      "Save model 5734\n",
      "Train Epoch:5735 learning rate:1.0000e-05, Loss_tot:0.0992,\n",
      "save model\n",
      "Save model 5735\n",
      "Train Epoch:5736 learning rate:1.0000e-05, Loss_tot:0.0992,\n",
      "save model\n",
      "Save model 5736\n",
      "Train Epoch:5737 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "save model\n",
      "Save model 5737\n",
      "Train Epoch:5738 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "save model\n",
      "Save model 5738\n",
      "Train Epoch:5739 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "save model\n",
      "Save model 5739\n",
      "Train Epoch:5740 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "save model\n",
      "Save model 5740\n",
      "Train Epoch:5741 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "save model\n",
      "Save model 5741\n",
      "Train Epoch:5742 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "save model\n",
      "Save model 5742\n",
      "Train Epoch:5743 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 5743\n",
      "Train Epoch:5744 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "save model\n",
      "Save model 5744\n",
      "Train Epoch:5745 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "save model\n",
      "Save model 5745\n",
      "Train Epoch:5746 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 5746\n",
      "Train Epoch:5747 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 5747\n",
      "Train Epoch:5748 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "save model\n",
      "Save model 5748\n",
      "Train Epoch:5749 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "save model\n",
      "Save model 5749\n",
      "Train Epoch:5750 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "save model\n",
      "Save model 5750\n",
      "Train Epoch:5751 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 5751\n",
      "Train Epoch:5752 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 5752\n",
      "Train Epoch:5753 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "save model\n",
      "Save model 5753\n",
      "Train Epoch:5754 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 5754\n",
      "Train Epoch:5755 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "save model\n",
      "Save model 5755\n",
      "Train Epoch:5756 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "save model\n",
      "Save model 5756\n",
      "Train Epoch:5757 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "save model\n",
      "Save model 5757\n",
      "Train Epoch:5758 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 5758\n",
      "Train Epoch:5759 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 5759\n",
      "Train Epoch:5760 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 5760\n",
      "Train Epoch:5761 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "save model\n",
      "Save model 5761\n",
      "Train Epoch:5762 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5762\n",
      "Train Epoch:5763 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5763\n",
      "Train Epoch:5764 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "save model\n",
      "Save model 5764\n",
      "Train Epoch:5765 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5765\n",
      "Train Epoch:5766 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5766\n",
      "Train Epoch:5767 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5767\n",
      "Train Epoch:5768 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 5768\n",
      "Train Epoch:5769 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "save model\n",
      "Save model 5769\n",
      "Train Epoch:5770 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "save model\n",
      "Save model 5770\n",
      "Train Epoch:5771 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "Save model 5771\n",
      "Train Epoch:5772 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "save model\n",
      "Save model 5772\n",
      "Train Epoch:5773 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "save model\n",
      "Save model 5773\n",
      "Train Epoch:5774 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 5774\n",
      "Train Epoch:5775 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "save model\n",
      "Save model 5775\n",
      "Train Epoch:5776 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 5776\n",
      "Train Epoch:5777 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 5777\n",
      "Train Epoch:5778 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 5778\n",
      "Train Epoch:5779 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "save model\n",
      "Save model 5779\n",
      "Train Epoch:5780 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "save model\n",
      "Save model 5780\n",
      "Train Epoch:5781 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "save model\n",
      "Save model 5781\n",
      "Train Epoch:5782 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "save model\n",
      "Save model 5782\n",
      "Train Epoch:5783 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "save model\n",
      "Save model 5783\n",
      "Train Epoch:5784 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "save model\n",
      "Save model 5784\n",
      "Train Epoch:5785 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 5785\n",
      "Train Epoch:5786 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 5786\n",
      "Train Epoch:5787 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "save model\n",
      "Save model 5787\n",
      "Train Epoch:5788 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 5788\n",
      "Train Epoch:5789 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "save model\n",
      "Save model 5789\n",
      "Train Epoch:5790 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 5790\n",
      "Train Epoch:5791 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "save model\n",
      "Save model 5791\n",
      "Train Epoch:5792 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "save model\n",
      "Save model 5792\n",
      "Train Epoch:5793 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 5793\n",
      "Train Epoch:5794 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "save model\n",
      "Save model 5794\n",
      "Train Epoch:5795 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "save model\n",
      "Save model 5795\n",
      "Train Epoch:5796 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5796\n",
      "Train Epoch:5797 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5797\n",
      "Train Epoch:5798 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5798\n",
      "Train Epoch:5799 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5799\n",
      "Train Epoch:5800 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 5800\n",
      "Train Epoch:5801 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5801\n",
      "Train Epoch:5802 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 5802\n",
      "Train Epoch:5803 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 5803\n",
      "Train Epoch:5804 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 5804\n",
      "Train Epoch:5805 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "save model\n",
      "Save model 5805\n",
      "Train Epoch:5806 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "save model\n",
      "Save model 5806\n",
      "Train Epoch:5807 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "save model\n",
      "Save model 5807\n",
      "Train Epoch:5808 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "save model\n",
      "Save model 5808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5809 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "save model\n",
      "Save model 5809\n",
      "Train Epoch:5810 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "save model\n",
      "Save model 5810\n",
      "Train Epoch:5811 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 5811\n",
      "Train Epoch:5812 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 5812\n",
      "Train Epoch:5813 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "save model\n",
      "Save model 5813\n",
      "Train Epoch:5814 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 5814\n",
      "Train Epoch:5815 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "save model\n",
      "Save model 5815\n",
      "Train Epoch:5816 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 5816\n",
      "Train Epoch:5817 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "save model\n",
      "Save model 5817\n",
      "Train Epoch:5818 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "save model\n",
      "Save model 5818\n",
      "Train Epoch:5819 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "save model\n",
      "Save model 5819\n",
      "Train Epoch:5820 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "save model\n",
      "Save model 5820\n",
      "Train Epoch:5821 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 5821\n",
      "Train Epoch:5822 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 5822\n",
      "Train Epoch:5823 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "save model\n",
      "Save model 5823\n",
      "Train Epoch:5824 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 5824\n",
      "Train Epoch:5825 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "save model\n",
      "Save model 5825\n",
      "Train Epoch:5826 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "save model\n",
      "Save model 5826\n",
      "Train Epoch:5827 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "save model\n",
      "Save model 5827\n",
      "Train Epoch:5828 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "save model\n",
      "Save model 5828\n",
      "Train Epoch:5829 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "save model\n",
      "Save model 5829\n",
      "Train Epoch:5830 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "save model\n",
      "Save model 5830\n",
      "Train Epoch:5831 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 5831\n",
      "Train Epoch:5832 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "save model\n",
      "Save model 5832\n",
      "Train Epoch:5833 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 5833\n",
      "Train Epoch:5834 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 5834\n",
      "Train Epoch:5835 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 5835\n",
      "Train Epoch:5836 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "save model\n",
      "Save model 5836\n",
      "Train Epoch:5837 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "save model\n",
      "Save model 5837\n",
      "Train Epoch:5838 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 5838\n",
      "Train Epoch:5839 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "save model\n",
      "Save model 5839\n",
      "Train Epoch:5840 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "save model\n",
      "Save model 5840\n",
      "Train Epoch:5841 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 5841\n",
      "Train Epoch:5842 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "save model\n",
      "Save model 5842\n",
      "Train Epoch:5843 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "save model\n",
      "Save model 5843\n",
      "Train Epoch:5844 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 5844\n",
      "Train Epoch:5845 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 5845\n",
      "Train Epoch:5846 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "save model\n",
      "Save model 5846\n",
      "Train Epoch:5847 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 5847\n",
      "Train Epoch:5848 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 5848\n",
      "Train Epoch:5849 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "save model\n",
      "Save model 5849\n",
      "Train Epoch:5850 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "save model\n",
      "Save model 5850\n",
      "Train Epoch:5851 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "save model\n",
      "Save model 5851\n",
      "Train Epoch:5852 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 5852\n",
      "Train Epoch:5853 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 5853\n",
      "Train Epoch:5854 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "save model\n",
      "Save model 5854\n",
      "Train Epoch:5855 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 5855\n",
      "Train Epoch:5856 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "save model\n",
      "Save model 5856\n",
      "Train Epoch:5857 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 5857\n",
      "Train Epoch:5858 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "save model\n",
      "Save model 5858\n",
      "Train Epoch:5859 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "save model\n",
      "Save model 5859\n",
      "Train Epoch:5860 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 5860\n",
      "Train Epoch:5861 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "save model\n",
      "Save model 5861\n",
      "Train Epoch:5862 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "save model\n",
      "Save model 5862\n",
      "Train Epoch:5863 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "save model\n",
      "Save model 5863\n",
      "Train Epoch:5864 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 5864\n",
      "Train Epoch:5865 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "save model\n",
      "Save model 5865\n",
      "Train Epoch:5866 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "save model\n",
      "Save model 5866\n",
      "Train Epoch:5867 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 5867\n",
      "Train Epoch:5868 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 5868\n",
      "Train Epoch:5869 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 5869\n",
      "Train Epoch:5870 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 5870\n",
      "Train Epoch:5871 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "save model\n",
      "Save model 5871\n",
      "Train Epoch:5872 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "save model\n",
      "Save model 5872\n",
      "Train Epoch:5873 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5873\n",
      "Train Epoch:5874 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5874\n",
      "Train Epoch:5875 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "save model\n",
      "Save model 5875\n",
      "Train Epoch:5876 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5876\n",
      "Train Epoch:5877 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5877\n",
      "Train Epoch:5878 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5878\n",
      "Train Epoch:5879 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 5879\n",
      "Train Epoch:5880 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "save model\n",
      "Save model 5880\n",
      "Train Epoch:5881 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 5881\n",
      "Train Epoch:5882 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 5882\n",
      "Train Epoch:5883 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "save model\n",
      "Save model 5883\n",
      "Train Epoch:5884 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "save model\n",
      "Save model 5884\n",
      "Train Epoch:5885 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 5885\n",
      "Train Epoch:5886 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 5886\n",
      "Train Epoch:5887 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "save model\n",
      "Save model 5887\n",
      "Train Epoch:5888 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "save model\n",
      "Save model 5888\n",
      "Train Epoch:5889 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "save model\n",
      "Save model 5889\n",
      "Train Epoch:5890 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "save model\n",
      "Save model 5890\n",
      "Train Epoch:5891 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "save model\n",
      "Save model 5891\n",
      "Train Epoch:5892 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "save model\n",
      "Save model 5892\n",
      "Train Epoch:5893 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 5893\n",
      "Train Epoch:5894 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 5894\n",
      "Train Epoch:5895 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "save model\n",
      "Save model 5895\n",
      "Train Epoch:5896 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 5896\n",
      "Train Epoch:5897 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "save model\n",
      "Save model 5897\n",
      "Train Epoch:5898 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 5898\n",
      "Train Epoch:5899 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 5899\n",
      "Train Epoch:5900 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5900\n",
      "Train Epoch:5901 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5901\n",
      "Train Epoch:5902 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5902\n",
      "Train Epoch:5903 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 5903\n",
      "Train Epoch:5904 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5904\n",
      "Train Epoch:5905 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5905\n",
      "Train Epoch:5906 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5906\n",
      "Train Epoch:5907 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5907\n",
      "Train Epoch:5908 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5908\n",
      "Train Epoch:5909 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:5910 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "save model\n",
      "Save model 5910\n",
      "Train Epoch:5911 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5911\n",
      "Train Epoch:5912 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "save model\n",
      "Save model 5912\n",
      "Train Epoch:5913 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "save model\n",
      "Save model 5913\n",
      "Train Epoch:5914 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "save model\n",
      "Save model 5914\n",
      "Train Epoch:5915 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 5915\n",
      "Train Epoch:5916 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 5916\n",
      "Train Epoch:5917 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 5917\n",
      "Train Epoch:5918 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "save model\n",
      "Save model 5918\n",
      "Train Epoch:5919 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 5919\n",
      "Train Epoch:5920 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 5920\n",
      "Train Epoch:5921 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 5921\n",
      "Train Epoch:5922 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "save model\n",
      "Save model 5922\n",
      "Train Epoch:5923 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 5923\n",
      "Train Epoch:5924 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 5924\n",
      "Train Epoch:5925 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "save model\n",
      "Save model 5925\n",
      "Train Epoch:5926 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "save model\n",
      "Save model 5926\n",
      "Train Epoch:5927 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "save model\n",
      "Save model 5927\n",
      "Train Epoch:5928 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "save model\n",
      "Save model 5928\n",
      "Train Epoch:5929 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "save model\n",
      "Save model 5929\n",
      "Train Epoch:5930 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "save model\n",
      "Save model 5930\n",
      "Train Epoch:5931 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 5931\n",
      "Train Epoch:5932 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 5932\n",
      "Train Epoch:5933 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "save model\n",
      "Save model 5933\n",
      "Train Epoch:5934 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 5934\n",
      "Train Epoch:5935 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "save model\n",
      "Save model 5935\n",
      "Train Epoch:5936 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 5936\n",
      "Train Epoch:5937 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 5937\n",
      "Train Epoch:5938 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "save model\n",
      "Save model 5938\n",
      "Train Epoch:5939 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 5939\n",
      "Train Epoch:5940 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "save model\n",
      "Save model 5940\n",
      "Train Epoch:5941 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 5941\n",
      "Train Epoch:5942 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 5942\n",
      "Train Epoch:5943 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "save model\n",
      "Save model 5943\n",
      "Train Epoch:5944 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 5944\n",
      "Train Epoch:5945 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "save model\n",
      "Save model 5945\n",
      "Train Epoch:5946 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5946\n",
      "Train Epoch:5947 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "save model\n",
      "Save model 5947\n",
      "Train Epoch:5948 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5948\n",
      "Train Epoch:5949 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5949\n",
      "Train Epoch:5950 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "save model\n",
      "Save model 5950\n",
      "Train Epoch:5951 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "save model\n",
      "Save model 5951\n",
      "Train Epoch:5952 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "save model\n",
      "Save model 5952\n",
      "Train Epoch:5953 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5953\n",
      "Train Epoch:5954 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5954\n",
      "Train Epoch:5955 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5955\n",
      "Train Epoch:5956 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "save model\n",
      "Save model 5956\n",
      "Train Epoch:5957 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "save model\n",
      "Save model 5957\n",
      "Train Epoch:5958 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5958\n",
      "Train Epoch:5959 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "save model\n",
      "Save model 5959\n",
      "Train Epoch:5960 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5960\n",
      "Train Epoch:5961 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 5961\n",
      "Train Epoch:5962 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5962\n",
      "Train Epoch:5963 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5963\n",
      "Train Epoch:5964 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "save model\n",
      "Save model 5964\n",
      "Train Epoch:5965 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5965\n",
      "Train Epoch:5966 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5966\n",
      "Train Epoch:5967 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5967\n",
      "Train Epoch:5968 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "save model\n",
      "Save model 5968\n",
      "Train Epoch:5969 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5969\n",
      "Train Epoch:5970 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5970\n",
      "Train Epoch:5971 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "save model\n",
      "Save model 5971\n",
      "Train Epoch:5972 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5972\n",
      "Train Epoch:5973 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5973\n",
      "Train Epoch:5974 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 5974\n",
      "Train Epoch:5975 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5975\n",
      "Train Epoch:5976 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 5976\n",
      "Train Epoch:5977 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "save model\n",
      "Save model 5977\n",
      "Train Epoch:5978 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 5978\n",
      "Train Epoch:5979 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "save model\n",
      "Save model 5979\n",
      "Train Epoch:5980 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "save model\n",
      "Save model 5980\n",
      "Train Epoch:5981 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 5981\n",
      "Train Epoch:5982 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5982\n",
      "Train Epoch:5983 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 5983\n",
      "Train Epoch:5984 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 5984\n",
      "Train Epoch:5985 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5985\n",
      "Train Epoch:5986 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "save model\n",
      "Save model 5986\n",
      "Train Epoch:5987 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5987\n",
      "Train Epoch:5988 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 5988\n",
      "Train Epoch:5989 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "save model\n",
      "Save model 5989\n",
      "Train Epoch:5990 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5990\n",
      "Train Epoch:5991 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5991\n",
      "Train Epoch:5992 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5992\n",
      "Train Epoch:5993 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5993\n",
      "Train Epoch:5994 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "save model\n",
      "Save model 5994\n",
      "Train Epoch:5995 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5995\n",
      "Train Epoch:5996 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5996\n",
      "Train Epoch:5997 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 5997\n",
      "Train Epoch:5998 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "save model\n",
      "Save model 5998\n",
      "Train Epoch:5999 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 5999\n",
      "Train Epoch:6000 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6000\n",
      "Train Epoch:6001 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "save model\n",
      "Save model 6001\n",
      "Train Epoch:6002 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6002\n",
      "Train Epoch:6003 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6003\n",
      "Train Epoch:6004 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "save model\n",
      "Save model 6004\n",
      "Train Epoch:6005 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6005\n",
      "Train Epoch:6006 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6006\n",
      "Train Epoch:6007 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6007\n",
      "Train Epoch:6008 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6008\n",
      "Train Epoch:6009 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "save model\n",
      "Save model 6009\n",
      "Train Epoch:6010 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "save model\n",
      "Save model 6010\n",
      "Train Epoch:6011 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6011\n",
      "Train Epoch:6012 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "save model\n",
      "Save model 6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6013 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 6013\n",
      "Train Epoch:6014 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6014\n",
      "Train Epoch:6015 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 6015\n",
      "Train Epoch:6016 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "save model\n",
      "Save model 6016\n",
      "Train Epoch:6017 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "save model\n",
      "Save model 6017\n",
      "Train Epoch:6018 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 6018\n",
      "Train Epoch:6019 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "save model\n",
      "Save model 6019\n",
      "Train Epoch:6020 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "save model\n",
      "Save model 6020\n",
      "Train Epoch:6021 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6021\n",
      "Train Epoch:6022 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "save model\n",
      "Save model 6022\n",
      "Train Epoch:6023 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6023\n",
      "Train Epoch:6024 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6024\n",
      "Train Epoch:6025 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6025\n",
      "Train Epoch:6026 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6026\n",
      "Train Epoch:6027 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6027\n",
      "Train Epoch:6028 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6028\n",
      "Train Epoch:6029 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6029\n",
      "Train Epoch:6030 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6030\n",
      "Train Epoch:6031 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6031\n",
      "Train Epoch:6032 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6032\n",
      "Train Epoch:6033 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 6033\n",
      "Train Epoch:6034 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6034\n",
      "Train Epoch:6035 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6035\n",
      "Train Epoch:6036 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 6036\n",
      "Train Epoch:6037 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6037\n",
      "Train Epoch:6038 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "save model\n",
      "Save model 6038\n",
      "Train Epoch:6039 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 6039\n",
      "Train Epoch:6040 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 6040\n",
      "Train Epoch:6041 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "save model\n",
      "Save model 6041\n",
      "Train Epoch:6042 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "save model\n",
      "Save model 6042\n",
      "Train Epoch:6043 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 6043\n",
      "Train Epoch:6044 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "save model\n",
      "Save model 6044\n",
      "Train Epoch:6045 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "save model\n",
      "Save model 6045\n",
      "Train Epoch:6046 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "save model\n",
      "Save model 6046\n",
      "Train Epoch:6047 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6047\n",
      "Train Epoch:6048 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6048\n",
      "Train Epoch:6049 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 6049\n",
      "Train Epoch:6050 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6050\n",
      "Train Epoch:6051 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6051\n",
      "Train Epoch:6052 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6052\n",
      "Train Epoch:6053 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6053\n",
      "Train Epoch:6054 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6054\n",
      "Train Epoch:6055 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "save model\n",
      "Save model 6055\n",
      "Train Epoch:6056 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6056\n",
      "Train Epoch:6057 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6057\n",
      "Train Epoch:6058 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6058\n",
      "Train Epoch:6059 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "save model\n",
      "Save model 6059\n",
      "Train Epoch:6060 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6060\n",
      "Train Epoch:6061 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6061\n",
      "Train Epoch:6062 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 6062\n",
      "Train Epoch:6063 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "save model\n",
      "Save model 6063\n",
      "Train Epoch:6064 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 6064\n",
      "Train Epoch:6065 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 6065\n",
      "Train Epoch:6066 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "save model\n",
      "Save model 6066\n",
      "Train Epoch:6067 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 6067\n",
      "Train Epoch:6068 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 6068\n",
      "Train Epoch:6069 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "save model\n",
      "Save model 6069\n",
      "Train Epoch:6070 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "save model\n",
      "Save model 6070\n",
      "Train Epoch:6071 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6071\n",
      "Train Epoch:6072 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "save model\n",
      "Save model 6072\n",
      "Train Epoch:6073 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6073\n",
      "Train Epoch:6074 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6074\n",
      "Train Epoch:6075 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "save model\n",
      "Save model 6075\n",
      "Train Epoch:6076 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "save model\n",
      "Save model 6076\n",
      "Train Epoch:6077 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "save model\n",
      "Save model 6077\n",
      "Train Epoch:6078 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "save model\n",
      "Save model 6078\n",
      "Train Epoch:6079 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6079\n",
      "Train Epoch:6080 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6080\n",
      "Train Epoch:6081 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "save model\n",
      "Save model 6081\n",
      "Train Epoch:6082 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6082\n",
      "Train Epoch:6083 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6083\n",
      "Train Epoch:6084 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6084\n",
      "Train Epoch:6085 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6085\n",
      "Train Epoch:6086 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6086\n",
      "Train Epoch:6087 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 6087\n",
      "Train Epoch:6088 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6088\n",
      "Train Epoch:6089 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "save model\n",
      "Save model 6089\n",
      "Train Epoch:6090 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6090\n",
      "Train Epoch:6091 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6091\n",
      "Train Epoch:6092 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 6092\n",
      "Train Epoch:6093 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "save model\n",
      "Save model 6093\n",
      "Train Epoch:6094 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6094\n",
      "Train Epoch:6095 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6095\n",
      "Train Epoch:6096 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "save model\n",
      "Save model 6096\n",
      "Train Epoch:6097 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6097\n",
      "Train Epoch:6098 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6098\n",
      "Train Epoch:6099 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "save model\n",
      "Save model 6099\n",
      "Train Epoch:6100 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6100\n",
      "Train Epoch:6101 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6101\n",
      "Train Epoch:6102 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6102\n",
      "Train Epoch:6103 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6103\n",
      "Train Epoch:6104 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "save model\n",
      "Save model 6104\n",
      "Train Epoch:6105 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6105\n",
      "Train Epoch:6106 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 6106\n",
      "Train Epoch:6107 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "save model\n",
      "Save model 6107\n",
      "Train Epoch:6108 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "save model\n",
      "Save model 6108\n",
      "Train Epoch:6109 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6109\n",
      "Train Epoch:6110 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6110\n",
      "Train Epoch:6111 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "save model\n",
      "Save model 6111\n",
      "Train Epoch:6112 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6112\n",
      "Train Epoch:6113 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6113\n",
      "Train Epoch:6114 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "save model\n",
      "Save model 6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6115 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 6115\n",
      "Train Epoch:6116 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6116\n",
      "Train Epoch:6117 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 6117\n",
      "Train Epoch:6118 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 6118\n",
      "Train Epoch:6119 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "save model\n",
      "Save model 6119\n",
      "Train Epoch:6120 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 6120\n",
      "Train Epoch:6121 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 6121\n",
      "Train Epoch:6122 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 6122\n",
      "Train Epoch:6123 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "save model\n",
      "Save model 6123\n",
      "Train Epoch:6124 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6124\n",
      "Train Epoch:6125 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6125\n",
      "Train Epoch:6126 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "save model\n",
      "Save model 6126\n",
      "Train Epoch:6127 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "save model\n",
      "Save model 6127\n",
      "Train Epoch:6128 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6128\n",
      "Train Epoch:6129 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "save model\n",
      "Save model 6129\n",
      "Train Epoch:6130 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "save model\n",
      "Save model 6130\n",
      "Train Epoch:6131 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6131\n",
      "Train Epoch:6132 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "save model\n",
      "Save model 6132\n",
      "Train Epoch:6133 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6133\n",
      "Train Epoch:6134 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6134\n",
      "Train Epoch:6135 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 6135\n",
      "Train Epoch:6136 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "save model\n",
      "Save model 6136\n",
      "Train Epoch:6137 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6137\n",
      "Train Epoch:6138 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 6138\n",
      "Train Epoch:6139 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 6139\n",
      "Train Epoch:6140 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "save model\n",
      "Save model 6140\n",
      "Train Epoch:6141 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 6141\n",
      "Train Epoch:6142 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 6142\n",
      "Train Epoch:6143 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "save model\n",
      "Save model 6143\n",
      "Train Epoch:6144 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6144\n",
      "Train Epoch:6145 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 6145\n",
      "Train Epoch:6146 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6146\n",
      "Train Epoch:6147 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6147\n",
      "Train Epoch:6148 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6148\n",
      "Train Epoch:6149 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6149\n",
      "Train Epoch:6150 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "save model\n",
      "Save model 6150\n",
      "Train Epoch:6151 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 6151\n",
      "Train Epoch:6152 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 6152\n",
      "Train Epoch:6153 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6153\n",
      "Train Epoch:6154 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6154\n",
      "Train Epoch:6155 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6155\n",
      "Train Epoch:6156 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 6156\n",
      "Train Epoch:6157 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 6157\n",
      "Train Epoch:6158 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6158\n",
      "Train Epoch:6159 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6159\n",
      "Train Epoch:6160 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6160\n",
      "Train Epoch:6161 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6161\n",
      "Train Epoch:6162 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6162\n",
      "Train Epoch:6163 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6163\n",
      "Train Epoch:6164 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6164\n",
      "Train Epoch:6165 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "save model\n",
      "Save model 6165\n",
      "Train Epoch:6166 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6166\n",
      "Train Epoch:6167 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6167\n",
      "Train Epoch:6168 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "save model\n",
      "Save model 6168\n",
      "Train Epoch:6169 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6169\n",
      "Train Epoch:6170 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "save model\n",
      "Save model 6170\n",
      "Train Epoch:6171 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6171\n",
      "Train Epoch:6172 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 6172\n",
      "Train Epoch:6173 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "save model\n",
      "Save model 6173\n",
      "Train Epoch:6174 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6174\n",
      "Train Epoch:6175 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "save model\n",
      "Save model 6175\n",
      "Train Epoch:6176 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "save model\n",
      "Save model 6176\n",
      "Train Epoch:6177 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "save model\n",
      "Save model 6177\n",
      "Train Epoch:6178 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6178\n",
      "Train Epoch:6179 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "save model\n",
      "Save model 6179\n",
      "Train Epoch:6180 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6180\n",
      "Train Epoch:6181 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6181\n",
      "Train Epoch:6182 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6182\n",
      "Train Epoch:6183 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "save model\n",
      "Save model 6183\n",
      "Train Epoch:6184 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6184\n",
      "Train Epoch:6185 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 6185\n",
      "Train Epoch:6186 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6186\n",
      "Train Epoch:6187 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "save model\n",
      "Save model 6187\n",
      "Train Epoch:6188 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6188\n",
      "Train Epoch:6189 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6189\n",
      "Train Epoch:6190 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "save model\n",
      "Save model 6190\n",
      "Train Epoch:6191 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6191\n",
      "Train Epoch:6192 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6192\n",
      "Train Epoch:6193 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "save model\n",
      "Save model 6193\n",
      "Train Epoch:6194 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6194\n",
      "Train Epoch:6195 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6195\n",
      "Train Epoch:6196 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6196\n",
      "Train Epoch:6197 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6197\n",
      "Train Epoch:6198 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6198\n",
      "Train Epoch:6199 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6199\n",
      "Train Epoch:6200 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 6200\n",
      "Train Epoch:6201 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6201\n",
      "Train Epoch:6202 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "save model\n",
      "Save model 6202\n",
      "Train Epoch:6203 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6203\n",
      "Train Epoch:6204 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6204\n",
      "Train Epoch:6205 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "save model\n",
      "Save model 6205\n",
      "Train Epoch:6206 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6206\n",
      "Train Epoch:6207 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6207\n",
      "Train Epoch:6208 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6208\n",
      "Train Epoch:6209 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6209\n",
      "Train Epoch:6210 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6210\n",
      "Train Epoch:6211 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6211\n",
      "Train Epoch:6212 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6212\n",
      "Train Epoch:6213 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "save model\n",
      "Save model 6213\n",
      "Train Epoch:6214 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6214\n",
      "Train Epoch:6215 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6215\n",
      "Train Epoch:6216 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6216\n",
      "Train Epoch:6217 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "save model\n",
      "Save model 6217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6218 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6218\n",
      "Train Epoch:6219 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6219\n",
      "Train Epoch:6220 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6220\n",
      "Train Epoch:6221 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 6221\n",
      "Train Epoch:6222 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6222\n",
      "Train Epoch:6223 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "save model\n",
      "Save model 6223\n",
      "Train Epoch:6224 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6224\n",
      "Train Epoch:6225 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6225\n",
      "Train Epoch:6226 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6226\n",
      "Train Epoch:6227 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "save model\n",
      "Save model 6227\n",
      "Train Epoch:6228 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6228\n",
      "Train Epoch:6229 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6229\n",
      "Train Epoch:6230 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6230\n",
      "Train Epoch:6231 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 6231\n",
      "Train Epoch:6232 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6232\n",
      "Train Epoch:6233 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "save model\n",
      "Save model 6233\n",
      "Train Epoch:6234 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6234\n",
      "Train Epoch:6235 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6235\n",
      "Train Epoch:6236 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6236\n",
      "Train Epoch:6237 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "save model\n",
      "Save model 6237\n",
      "Train Epoch:6238 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6238\n",
      "Train Epoch:6239 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6239\n",
      "Train Epoch:6240 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6240\n",
      "Train Epoch:6241 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6241\n",
      "Train Epoch:6242 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6242\n",
      "Train Epoch:6243 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "save model\n",
      "Save model 6243\n",
      "Train Epoch:6244 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6244\n",
      "Train Epoch:6245 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6245\n",
      "Train Epoch:6246 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 6246\n",
      "Train Epoch:6247 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6247\n",
      "Train Epoch:6248 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "save model\n",
      "Save model 6248\n",
      "Train Epoch:6249 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6249\n",
      "Train Epoch:6250 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6250\n",
      "Train Epoch:6251 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6251\n",
      "Train Epoch:6252 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "save model\n",
      "Save model 6252\n",
      "Train Epoch:6253 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6253\n",
      "Train Epoch:6254 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 6254\n",
      "Train Epoch:6255 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6255\n",
      "Train Epoch:6256 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "save model\n",
      "Save model 6256\n",
      "Train Epoch:6257 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6257\n",
      "Train Epoch:6258 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6258\n",
      "Train Epoch:6259 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "save model\n",
      "Save model 6259\n",
      "Train Epoch:6260 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6260\n",
      "Train Epoch:6261 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6261\n",
      "Train Epoch:6262 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6262\n",
      "Train Epoch:6263 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6263\n",
      "Train Epoch:6264 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6264\n",
      "Train Epoch:6265 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6265\n",
      "Train Epoch:6266 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6266\n",
      "Train Epoch:6267 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "save model\n",
      "Save model 6267\n",
      "Train Epoch:6268 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6268\n",
      "Train Epoch:6269 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6269\n",
      "Train Epoch:6270 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6270\n",
      "Train Epoch:6271 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6271\n",
      "Train Epoch:6272 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6272\n",
      "Train Epoch:6273 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6273\n",
      "Train Epoch:6274 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6274\n",
      "Train Epoch:6275 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 6275\n",
      "Train Epoch:6276 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6276\n",
      "Train Epoch:6277 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "save model\n",
      "Save model 6277\n",
      "Train Epoch:6278 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6278\n",
      "Train Epoch:6279 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6279\n",
      "Train Epoch:6280 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6280\n",
      "Train Epoch:6281 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 6281\n",
      "Train Epoch:6282 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 6282\n",
      "Train Epoch:6283 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6283\n",
      "Train Epoch:6284 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6284\n",
      "Train Epoch:6285 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 6285\n",
      "Train Epoch:6286 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "save model\n",
      "Save model 6286\n",
      "Train Epoch:6287 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6287\n",
      "Train Epoch:6288 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6288\n",
      "Train Epoch:6289 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6289\n",
      "Train Epoch:6290 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6290\n",
      "Train Epoch:6291 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6291\n",
      "Train Epoch:6292 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6292\n",
      "Train Epoch:6293 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6293\n",
      "Train Epoch:6294 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6294\n",
      "Train Epoch:6295 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6295\n",
      "Train Epoch:6296 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6296\n",
      "Train Epoch:6297 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6297\n",
      "Train Epoch:6298 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 6298\n",
      "Train Epoch:6299 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 6299\n",
      "Train Epoch:6300 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6300\n",
      "Train Epoch:6301 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "save model\n",
      "Save model 6301\n",
      "Train Epoch:6302 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6302\n",
      "Train Epoch:6303 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6303\n",
      "Train Epoch:6304 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 6304\n",
      "Train Epoch:6305 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "save model\n",
      "Save model 6305\n",
      "Train Epoch:6306 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6306\n",
      "Train Epoch:6307 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6307\n",
      "Train Epoch:6308 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "save model\n",
      "Save model 6308\n",
      "Train Epoch:6309 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "save model\n",
      "Save model 6309\n",
      "Train Epoch:6310 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6310\n",
      "Train Epoch:6311 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "save model\n",
      "Save model 6311\n",
      "Train Epoch:6312 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "save model\n",
      "Save model 6312\n",
      "Train Epoch:6313 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6313\n",
      "Train Epoch:6314 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6314\n",
      "Train Epoch:6315 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "save model\n",
      "Save model 6315\n",
      "Train Epoch:6316 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6316\n",
      "Train Epoch:6317 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6317\n",
      "Train Epoch:6318 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "save model\n",
      "Save model 6318\n",
      "Train Epoch:6319 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6319\n",
      "Train Epoch:6320 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6320\n",
      "Train Epoch:6321 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6321\n",
      "Train Epoch:6322 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6323 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "save model\n",
      "Save model 6323\n",
      "Train Epoch:6324 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6324\n",
      "Train Epoch:6325 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 6325\n",
      "Train Epoch:6326 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6326\n",
      "Train Epoch:6327 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "save model\n",
      "Save model 6327\n",
      "Train Epoch:6328 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6328\n",
      "Train Epoch:6329 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6329\n",
      "Train Epoch:6330 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6330\n",
      "Train Epoch:6331 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "save model\n",
      "Save model 6331\n",
      "Train Epoch:6332 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6332\n",
      "Train Epoch:6333 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6333\n",
      "Train Epoch:6334 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6334\n",
      "Train Epoch:6335 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "save model\n",
      "Save model 6335\n",
      "Train Epoch:6336 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6336\n",
      "Train Epoch:6337 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6337\n",
      "Train Epoch:6338 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 6338\n",
      "Train Epoch:6339 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "save model\n",
      "Save model 6339\n",
      "Train Epoch:6340 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6340\n",
      "Train Epoch:6341 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6341\n",
      "Train Epoch:6342 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "save model\n",
      "Save model 6342\n",
      "Train Epoch:6343 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6343\n",
      "Train Epoch:6344 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6344\n",
      "Train Epoch:6345 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6345\n",
      "Train Epoch:6346 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6346\n",
      "Train Epoch:6347 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6347\n",
      "Train Epoch:6348 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6348\n",
      "Train Epoch:6349 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6349\n",
      "Train Epoch:6350 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "save model\n",
      "Save model 6350\n",
      "Train Epoch:6351 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6351\n",
      "Train Epoch:6352 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6352\n",
      "Train Epoch:6353 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6353\n",
      "Train Epoch:6354 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6354\n",
      "Train Epoch:6355 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6355\n",
      "Train Epoch:6356 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6356\n",
      "Train Epoch:6357 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6357\n",
      "Train Epoch:6358 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6358\n",
      "Train Epoch:6359 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6359\n",
      "Train Epoch:6360 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "save model\n",
      "Save model 6360\n",
      "Train Epoch:6361 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6361\n",
      "Train Epoch:6362 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 6362\n",
      "Train Epoch:6363 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6363\n",
      "Train Epoch:6364 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "save model\n",
      "Save model 6364\n",
      "Train Epoch:6365 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6365\n",
      "Train Epoch:6366 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6366\n",
      "Train Epoch:6367 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6367\n",
      "Train Epoch:6368 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6368\n",
      "Train Epoch:6369 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6369\n",
      "Train Epoch:6370 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "save model\n",
      "Save model 6370\n",
      "Train Epoch:6371 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6371\n",
      "Train Epoch:6372 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6372\n",
      "Train Epoch:6373 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 6373\n",
      "Train Epoch:6374 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6374\n",
      "Train Epoch:6375 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "save model\n",
      "Save model 6375\n",
      "Train Epoch:6376 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6376\n",
      "Train Epoch:6377 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6377\n",
      "Train Epoch:6378 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6378\n",
      "Train Epoch:6379 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "save model\n",
      "Save model 6379\n",
      "Train Epoch:6380 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6380\n",
      "Train Epoch:6381 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 6381\n",
      "Train Epoch:6382 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6382\n",
      "Train Epoch:6383 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "save model\n",
      "Save model 6383\n",
      "Train Epoch:6384 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6384\n",
      "Train Epoch:6385 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6385\n",
      "Train Epoch:6386 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "save model\n",
      "Save model 6386\n",
      "Train Epoch:6387 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6387\n",
      "Train Epoch:6388 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6388\n",
      "Train Epoch:6389 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6389\n",
      "Train Epoch:6390 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6390\n",
      "Train Epoch:6391 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6391\n",
      "Train Epoch:6392 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 6392\n",
      "Train Epoch:6393 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6393\n",
      "Train Epoch:6394 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "save model\n",
      "Save model 6394\n",
      "Train Epoch:6395 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6395\n",
      "Train Epoch:6396 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6396\n",
      "Train Epoch:6397 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6397\n",
      "Train Epoch:6398 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "save model\n",
      "Save model 6398\n",
      "Train Epoch:6399 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6399\n",
      "Train Epoch:6400 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6400\n",
      "Train Epoch:6401 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6401\n",
      "Train Epoch:6402 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6402\n",
      "Train Epoch:6403 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6403\n",
      "Train Epoch:6404 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6404\n",
      "Train Epoch:6405 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6405\n",
      "Train Epoch:6406 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6406\n",
      "Train Epoch:6407 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6407\n",
      "Train Epoch:6408 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6408\n",
      "Train Epoch:6409 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6409\n",
      "Train Epoch:6410 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6410\n",
      "Train Epoch:6411 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6411\n",
      "Train Epoch:6412 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6412\n",
      "Train Epoch:6413 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6413\n",
      "Train Epoch:6414 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6414\n",
      "Train Epoch:6415 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6415\n",
      "Train Epoch:6416 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6416\n",
      "Train Epoch:6417 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 6417\n",
      "Train Epoch:6418 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6418\n",
      "Train Epoch:6419 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6419\n",
      "Train Epoch:6420 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6420\n",
      "Train Epoch:6421 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6421\n",
      "Train Epoch:6422 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "save model\n",
      "Save model 6422\n",
      "Train Epoch:6423 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6423\n",
      "Train Epoch:6424 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6424\n",
      "Train Epoch:6425 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6425\n",
      "Train Epoch:6426 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6426\n",
      "Train Epoch:6427 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6428 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6428\n",
      "Train Epoch:6429 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6429\n",
      "Train Epoch:6430 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6430\n",
      "Train Epoch:6431 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6431\n",
      "Train Epoch:6432 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6432\n",
      "Train Epoch:6433 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6433\n",
      "Train Epoch:6434 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 6434\n",
      "Train Epoch:6435 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6435\n",
      "Train Epoch:6436 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "save model\n",
      "Save model 6436\n",
      "Train Epoch:6437 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6437\n",
      "Train Epoch:6438 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6438\n",
      "Train Epoch:6439 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "save model\n",
      "Save model 6439\n",
      "Train Epoch:6440 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6440\n",
      "Train Epoch:6441 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6441\n",
      "Train Epoch:6442 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "save model\n",
      "Save model 6442\n",
      "Train Epoch:6443 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6443\n",
      "Train Epoch:6444 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6444\n",
      "Train Epoch:6445 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6445\n",
      "Train Epoch:6446 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6446\n",
      "Train Epoch:6447 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "save model\n",
      "Save model 6447\n",
      "Train Epoch:6448 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6448\n",
      "Train Epoch:6449 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6449\n",
      "Train Epoch:6450 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6450\n",
      "Train Epoch:6451 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6451\n",
      "Train Epoch:6452 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6452\n",
      "Train Epoch:6453 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6453\n",
      "Train Epoch:6454 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6454\n",
      "Train Epoch:6455 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6455\n",
      "Train Epoch:6456 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6456\n",
      "Train Epoch:6457 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "save model\n",
      "Save model 6457\n",
      "Train Epoch:6458 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6458\n",
      "Train Epoch:6459 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6459\n",
      "Train Epoch:6460 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 6460\n",
      "Train Epoch:6461 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6461\n",
      "Train Epoch:6462 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "save model\n",
      "Save model 6462\n",
      "Train Epoch:6463 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 6463\n",
      "Train Epoch:6464 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 6464\n",
      "Train Epoch:6465 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 6465\n",
      "Train Epoch:6466 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "save model\n",
      "Save model 6466\n",
      "Train Epoch:6467 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 6467\n",
      "Train Epoch:6468 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 6468\n",
      "Train Epoch:6469 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 6469\n",
      "Train Epoch:6470 learning rate:1.0000e-05, Loss_tot:0.1234,\n",
      "Save model 6470\n",
      "Train Epoch:6471 learning rate:1.0000e-05, Loss_tot:0.1231,\n",
      "Save model 6471\n",
      "Train Epoch:6472 learning rate:1.0000e-05, Loss_tot:0.1224,\n",
      "Save model 6472\n",
      "Train Epoch:6473 learning rate:1.0000e-05, Loss_tot:0.1216,\n",
      "Save model 6473\n",
      "Train Epoch:6474 learning rate:1.0000e-05, Loss_tot:0.1206,\n",
      "Save model 6474\n",
      "Train Epoch:6475 learning rate:1.0000e-05, Loss_tot:0.1195,\n",
      "Save model 6475\n",
      "Train Epoch:6476 learning rate:1.0000e-05, Loss_tot:0.1183,\n",
      "Save model 6476\n",
      "Train Epoch:6477 learning rate:1.0000e-05, Loss_tot:0.1171,\n",
      "Save model 6477\n",
      "Train Epoch:6478 learning rate:1.0000e-05, Loss_tot:0.1158,\n",
      "Save model 6478\n",
      "Train Epoch:6479 learning rate:1.0000e-05, Loss_tot:0.1145,\n",
      "Save model 6479\n",
      "Train Epoch:6480 learning rate:1.0000e-05, Loss_tot:0.1133,\n",
      "Save model 6480\n",
      "Train Epoch:6481 learning rate:1.0000e-05, Loss_tot:0.1121,\n",
      "Save model 6481\n",
      "Train Epoch:6482 learning rate:1.0000e-05, Loss_tot:0.1110,\n",
      "Save model 6482\n",
      "Train Epoch:6483 learning rate:1.0000e-05, Loss_tot:0.1099,\n",
      "Save model 6483\n",
      "Train Epoch:6484 learning rate:1.0000e-05, Loss_tot:0.1088,\n",
      "Save model 6484\n",
      "Train Epoch:6485 learning rate:1.0000e-05, Loss_tot:0.1079,\n",
      "Save model 6485\n",
      "Train Epoch:6486 learning rate:1.0000e-05, Loss_tot:0.1071,\n",
      "Save model 6486\n",
      "Train Epoch:6487 learning rate:1.0000e-05, Loss_tot:0.1063,\n",
      "Save model 6487\n",
      "Train Epoch:6488 learning rate:1.0000e-05, Loss_tot:0.1055,\n",
      "Save model 6488\n",
      "Train Epoch:6489 learning rate:1.0000e-05, Loss_tot:0.1049,\n",
      "Save model 6489\n",
      "Train Epoch:6490 learning rate:1.0000e-05, Loss_tot:0.1043,\n",
      "Save model 6490\n",
      "Train Epoch:6491 learning rate:1.0000e-05, Loss_tot:0.1038,\n",
      "Save model 6491\n",
      "Train Epoch:6492 learning rate:1.0000e-05, Loss_tot:0.1033,\n",
      "Save model 6492\n",
      "Train Epoch:6493 learning rate:1.0000e-05, Loss_tot:0.1028,\n",
      "Save model 6493\n",
      "Train Epoch:6494 learning rate:1.0000e-05, Loss_tot:0.1025,\n",
      "Save model 6494\n",
      "Train Epoch:6495 learning rate:1.0000e-05, Loss_tot:0.1022,\n",
      "Save model 6495\n",
      "Train Epoch:6496 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "Save model 6496\n",
      "Train Epoch:6497 learning rate:1.0000e-05, Loss_tot:0.1016,\n",
      "Save model 6497\n",
      "Train Epoch:6498 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "Save model 6498\n",
      "Train Epoch:6499 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6499\n",
      "Train Epoch:6500 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6500\n",
      "Train Epoch:6501 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6501\n",
      "Train Epoch:6502 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 6502\n",
      "Train Epoch:6503 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 6503\n",
      "Train Epoch:6504 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 6504\n",
      "Train Epoch:6505 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 6505\n",
      "Train Epoch:6506 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6506\n",
      "Train Epoch:6507 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6507\n",
      "Train Epoch:6508 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6508\n",
      "Train Epoch:6509 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6509\n",
      "Train Epoch:6510 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6510\n",
      "Train Epoch:6511 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6511\n",
      "Train Epoch:6512 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6512\n",
      "Train Epoch:6513 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6513\n",
      "Train Epoch:6514 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "Save model 6514\n",
      "Train Epoch:6515 learning rate:1.0000e-05, Loss_tot:0.1015,\n",
      "Save model 6515\n",
      "Train Epoch:6516 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6516\n",
      "Train Epoch:6517 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6517\n",
      "Train Epoch:6518 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6518\n",
      "Train Epoch:6519 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 6519\n",
      "Train Epoch:6520 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6520\n",
      "Train Epoch:6521 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6521\n",
      "Train Epoch:6522 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6522\n",
      "Train Epoch:6523 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 6523\n",
      "Train Epoch:6524 learning rate:1.0000e-05, Loss_tot:0.1012,\n",
      "Save model 6524\n",
      "Train Epoch:6525 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 6525\n",
      "Train Epoch:6526 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 6526\n",
      "Train Epoch:6527 learning rate:1.0000e-05, Loss_tot:0.1011,\n",
      "Save model 6527\n",
      "Train Epoch:6528 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 6528\n",
      "Train Epoch:6529 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 6529\n",
      "Train Epoch:6530 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 6530\n",
      "Train Epoch:6531 learning rate:1.0000e-05, Loss_tot:0.1010,\n",
      "Save model 6531\n",
      "Train Epoch:6532 learning rate:1.0000e-05, Loss_tot:0.1009,\n",
      "Save model 6532\n",
      "Train Epoch:6533 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 6533\n",
      "Train Epoch:6534 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 6534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6535 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 6535\n",
      "Train Epoch:6536 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 6536\n",
      "Train Epoch:6537 learning rate:1.0000e-05, Loss_tot:0.1007,\n",
      "Save model 6537\n",
      "Train Epoch:6538 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "Save model 6538\n",
      "Train Epoch:6539 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "Save model 6539\n",
      "Train Epoch:6540 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "Save model 6540\n",
      "Train Epoch:6541 learning rate:1.0000e-05, Loss_tot:0.1005,\n",
      "Save model 6541\n",
      "Train Epoch:6542 learning rate:1.0000e-05, Loss_tot:0.1005,\n",
      "Save model 6542\n",
      "Train Epoch:6543 learning rate:1.0000e-05, Loss_tot:0.1005,\n",
      "Save model 6543\n",
      "Train Epoch:6544 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6544\n",
      "Train Epoch:6545 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6545\n",
      "Train Epoch:6546 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6546\n",
      "Train Epoch:6547 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6547\n",
      "Train Epoch:6548 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 6548\n",
      "Train Epoch:6549 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6549\n",
      "Train Epoch:6550 learning rate:1.0000e-05, Loss_tot:0.1004,\n",
      "Save model 6550\n",
      "Train Epoch:6551 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 6551\n",
      "Train Epoch:6552 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 6552\n",
      "Train Epoch:6553 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 6553\n",
      "Train Epoch:6554 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 6554\n",
      "Train Epoch:6555 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 6555\n",
      "Train Epoch:6556 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 6556\n",
      "Train Epoch:6557 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 6557\n",
      "Train Epoch:6558 learning rate:1.0000e-05, Loss_tot:0.1002,\n",
      "Save model 6558\n",
      "Train Epoch:6559 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6559\n",
      "Train Epoch:6560 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6560\n",
      "Train Epoch:6561 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6561\n",
      "Train Epoch:6562 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6562\n",
      "Train Epoch:6563 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6563\n",
      "Train Epoch:6564 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6564\n",
      "Train Epoch:6565 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6565\n",
      "Train Epoch:6566 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6566\n",
      "Train Epoch:6567 learning rate:1.0000e-05, Loss_tot:0.1001,\n",
      "Save model 6567\n",
      "Train Epoch:6568 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6568\n",
      "Train Epoch:6569 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6569\n",
      "Train Epoch:6570 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6570\n",
      "Train Epoch:6571 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6571\n",
      "Train Epoch:6572 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6572\n",
      "Train Epoch:6573 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6573\n",
      "Train Epoch:6574 learning rate:1.0000e-05, Loss_tot:0.1000,\n",
      "Save model 6574\n",
      "Train Epoch:6575 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6575\n",
      "Train Epoch:6576 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6576\n",
      "Train Epoch:6577 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6577\n",
      "Train Epoch:6578 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6578\n",
      "Train Epoch:6579 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6579\n",
      "Train Epoch:6580 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6580\n",
      "Train Epoch:6581 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6581\n",
      "Train Epoch:6582 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6582\n",
      "Train Epoch:6583 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6583\n",
      "Train Epoch:6584 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6584\n",
      "Train Epoch:6585 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 6585\n",
      "Train Epoch:6586 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6586\n",
      "Train Epoch:6587 learning rate:1.0000e-05, Loss_tot:0.0999,\n",
      "Save model 6587\n",
      "Train Epoch:6588 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6588\n",
      "Train Epoch:6589 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 6589\n",
      "Train Epoch:6590 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 6590\n",
      "Train Epoch:6591 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6591\n",
      "Train Epoch:6592 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6592\n",
      "Train Epoch:6593 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 6593\n",
      "Train Epoch:6594 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 6594\n",
      "Train Epoch:6595 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "Save model 6595\n",
      "Train Epoch:6596 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6596\n",
      "Train Epoch:6597 learning rate:1.0000e-05, Loss_tot:0.0996,\n",
      "Save model 6597\n",
      "Train Epoch:6598 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6598\n",
      "Train Epoch:6599 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6599\n",
      "Train Epoch:6600 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6600\n",
      "Train Epoch:6601 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6601\n",
      "Train Epoch:6602 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6602\n",
      "Train Epoch:6603 learning rate:1.0000e-05, Loss_tot:0.0995,\n",
      "Save model 6603\n",
      "Train Epoch:6604 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6604\n",
      "Train Epoch:6605 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6605\n",
      "Train Epoch:6606 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6606\n",
      "Train Epoch:6607 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6607\n",
      "Train Epoch:6608 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6608\n",
      "Train Epoch:6609 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6609\n",
      "Train Epoch:6610 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6610\n",
      "Train Epoch:6611 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6611\n",
      "Train Epoch:6612 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6612\n",
      "Train Epoch:6613 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6613\n",
      "Train Epoch:6614 learning rate:1.0000e-05, Loss_tot:0.0994,\n",
      "Save model 6614\n",
      "Train Epoch:6615 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6615\n",
      "Train Epoch:6616 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6616\n",
      "Train Epoch:6617 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6617\n",
      "Train Epoch:6618 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6618\n",
      "Train Epoch:6619 learning rate:1.0000e-05, Loss_tot:0.0993,\n",
      "Save model 6619\n",
      "Train Epoch:6620 learning rate:1.0000e-05, Loss_tot:0.0992,\n",
      "Save model 6620\n",
      "Train Epoch:6621 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "Save model 6621\n",
      "Train Epoch:6622 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "Save model 6622\n",
      "Train Epoch:6623 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "Save model 6623\n",
      "Train Epoch:6624 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "Save model 6624\n",
      "Train Epoch:6625 learning rate:1.0000e-05, Loss_tot:0.0991,\n",
      "Save model 6625\n",
      "Train Epoch:6626 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6626\n",
      "Train Epoch:6627 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6627\n",
      "Train Epoch:6628 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6628\n",
      "Train Epoch:6629 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6629\n",
      "Train Epoch:6630 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6630\n",
      "Train Epoch:6631 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6631\n",
      "Train Epoch:6632 learning rate:1.0000e-05, Loss_tot:0.0990,\n",
      "Save model 6632\n",
      "Train Epoch:6633 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6633\n",
      "Train Epoch:6634 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6634\n",
      "Train Epoch:6635 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6635\n",
      "Train Epoch:6636 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6636\n",
      "Train Epoch:6637 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6637\n",
      "Train Epoch:6638 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6638\n",
      "Train Epoch:6639 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 6639\n",
      "Train Epoch:6640 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6640\n",
      "Train Epoch:6641 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6641\n",
      "Train Epoch:6642 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6643 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6643\n",
      "Train Epoch:6644 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 6644\n",
      "Train Epoch:6645 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 6645\n",
      "Train Epoch:6646 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6646\n",
      "Train Epoch:6647 learning rate:1.0000e-05, Loss_tot:0.0988,\n",
      "Save model 6647\n",
      "Train Epoch:6648 learning rate:1.0000e-05, Loss_tot:0.0987,\n",
      "Save model 6648\n",
      "Train Epoch:6649 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "Save model 6649\n",
      "Train Epoch:6650 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "Save model 6650\n",
      "Train Epoch:6651 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "Save model 6651\n",
      "Train Epoch:6652 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 6652\n",
      "Train Epoch:6653 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 6653\n",
      "Train Epoch:6654 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 6654\n",
      "Train Epoch:6655 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6655\n",
      "Train Epoch:6656 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6656\n",
      "Train Epoch:6657 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6657\n",
      "Train Epoch:6658 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6658\n",
      "Train Epoch:6659 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6659\n",
      "Train Epoch:6660 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6660\n",
      "Train Epoch:6661 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6661\n",
      "Train Epoch:6662 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6662\n",
      "Train Epoch:6663 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6663\n",
      "Train Epoch:6664 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6664\n",
      "Train Epoch:6665 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6665\n",
      "Train Epoch:6666 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6666\n",
      "Train Epoch:6667 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 6667\n",
      "Train Epoch:6668 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6668\n",
      "Train Epoch:6669 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6669\n",
      "Train Epoch:6670 learning rate:1.0000e-05, Loss_tot:0.0985,\n",
      "Save model 6670\n",
      "Train Epoch:6671 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6671\n",
      "Train Epoch:6672 learning rate:1.0000e-05, Loss_tot:0.0984,\n",
      "Save model 6672\n",
      "Train Epoch:6673 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6673\n",
      "Train Epoch:6674 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6674\n",
      "Train Epoch:6675 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6675\n",
      "Train Epoch:6676 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6676\n",
      "Train Epoch:6677 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6677\n",
      "Train Epoch:6678 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6678\n",
      "Train Epoch:6679 learning rate:1.0000e-05, Loss_tot:0.0983,\n",
      "Save model 6679\n",
      "Train Epoch:6680 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6680\n",
      "Train Epoch:6681 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6681\n",
      "Train Epoch:6682 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6682\n",
      "Train Epoch:6683 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6683\n",
      "Train Epoch:6684 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6684\n",
      "Train Epoch:6685 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6685\n",
      "Train Epoch:6686 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6686\n",
      "Train Epoch:6687 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6687\n",
      "Train Epoch:6688 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6688\n",
      "Train Epoch:6689 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6689\n",
      "Train Epoch:6690 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6690\n",
      "Train Epoch:6691 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6691\n",
      "Train Epoch:6692 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6692\n",
      "Train Epoch:6693 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6693\n",
      "Train Epoch:6694 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6694\n",
      "Train Epoch:6695 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6695\n",
      "Train Epoch:6696 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6696\n",
      "Train Epoch:6697 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6697\n",
      "Train Epoch:6698 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6698\n",
      "Train Epoch:6699 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6699\n",
      "Train Epoch:6700 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6700\n",
      "Train Epoch:6701 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6701\n",
      "Train Epoch:6702 learning rate:1.0000e-05, Loss_tot:0.0982,\n",
      "Save model 6702\n",
      "Train Epoch:6703 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6703\n",
      "Train Epoch:6704 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6704\n",
      "Train Epoch:6705 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6705\n",
      "Train Epoch:6706 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6706\n",
      "Train Epoch:6707 learning rate:1.0000e-05, Loss_tot:0.0981,\n",
      "Save model 6707\n",
      "Train Epoch:6708 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6708\n",
      "Train Epoch:6709 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6709\n",
      "Train Epoch:6710 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6710\n",
      "Train Epoch:6711 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6711\n",
      "Train Epoch:6712 learning rate:1.0000e-05, Loss_tot:0.0980,\n",
      "Save model 6712\n",
      "Train Epoch:6713 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6713\n",
      "Train Epoch:6714 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6714\n",
      "Train Epoch:6715 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6715\n",
      "Train Epoch:6716 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6716\n",
      "Train Epoch:6717 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6717\n",
      "Train Epoch:6718 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6718\n",
      "Train Epoch:6719 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6719\n",
      "Train Epoch:6720 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6720\n",
      "Train Epoch:6721 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6721\n",
      "Train Epoch:6722 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6722\n",
      "Train Epoch:6723 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 6723\n",
      "Train Epoch:6724 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6724\n",
      "Train Epoch:6725 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6725\n",
      "Train Epoch:6726 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6726\n",
      "Train Epoch:6727 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6727\n",
      "Train Epoch:6728 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6728\n",
      "Train Epoch:6729 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6729\n",
      "Train Epoch:6730 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6730\n",
      "Train Epoch:6731 learning rate:1.0000e-05, Loss_tot:0.0978,\n",
      "Save model 6731\n",
      "Train Epoch:6732 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6732\n",
      "Train Epoch:6733 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6733\n",
      "Train Epoch:6734 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6734\n",
      "Train Epoch:6735 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6735\n",
      "Train Epoch:6736 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6736\n",
      "Train Epoch:6737 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6737\n",
      "Train Epoch:6738 learning rate:1.0000e-05, Loss_tot:0.0977,\n",
      "Save model 6738\n",
      "Train Epoch:6739 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6739\n",
      "Train Epoch:6740 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6740\n",
      "Train Epoch:6741 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6741\n",
      "Train Epoch:6742 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6742\n",
      "Train Epoch:6743 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6743\n",
      "Train Epoch:6744 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6744\n",
      "Train Epoch:6745 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6745\n",
      "Train Epoch:6746 learning rate:1.0000e-05, Loss_tot:0.0976,\n",
      "Save model 6746\n",
      "Train Epoch:6747 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6747\n",
      "Train Epoch:6748 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6748\n",
      "Train Epoch:6749 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6749\n",
      "Train Epoch:6750 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6751 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6751\n",
      "Train Epoch:6752 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6752\n",
      "Train Epoch:6753 learning rate:1.0000e-05, Loss_tot:0.0975,\n",
      "Save model 6753\n",
      "Train Epoch:6754 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6754\n",
      "Train Epoch:6755 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6755\n",
      "Train Epoch:6756 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6756\n",
      "Train Epoch:6757 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6757\n",
      "Train Epoch:6758 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6758\n",
      "Train Epoch:6759 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6759\n",
      "Train Epoch:6760 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6760\n",
      "Train Epoch:6761 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6761\n",
      "Train Epoch:6762 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6762\n",
      "Train Epoch:6763 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6763\n",
      "Train Epoch:6764 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6764\n",
      "Train Epoch:6765 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6765\n",
      "Train Epoch:6766 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6766\n",
      "Train Epoch:6767 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 6767\n",
      "Train Epoch:6768 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 6768\n",
      "Train Epoch:6769 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 6769\n",
      "Train Epoch:6770 learning rate:1.0000e-05, Loss_tot:0.0974,\n",
      "Save model 6770\n",
      "Train Epoch:6771 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 6771\n",
      "Train Epoch:6772 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6772\n",
      "Train Epoch:6773 learning rate:1.0000e-05, Loss_tot:0.0973,\n",
      "Save model 6773\n",
      "Train Epoch:6774 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6774\n",
      "Train Epoch:6775 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6775\n",
      "Train Epoch:6776 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6776\n",
      "Train Epoch:6777 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6777\n",
      "Train Epoch:6778 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6778\n",
      "Train Epoch:6779 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6779\n",
      "Train Epoch:6780 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6780\n",
      "Train Epoch:6781 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6781\n",
      "Train Epoch:6782 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6782\n",
      "Train Epoch:6783 learning rate:1.0000e-05, Loss_tot:0.0972,\n",
      "Save model 6783\n",
      "Train Epoch:6784 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6784\n",
      "Train Epoch:6785 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6785\n",
      "Train Epoch:6786 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6786\n",
      "Train Epoch:6787 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6787\n",
      "Train Epoch:6788 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6788\n",
      "Train Epoch:6789 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6789\n",
      "Train Epoch:6790 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6790\n",
      "Train Epoch:6791 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6791\n",
      "Train Epoch:6792 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6792\n",
      "Train Epoch:6793 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6793\n",
      "Train Epoch:6794 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6794\n",
      "Train Epoch:6795 learning rate:1.0000e-05, Loss_tot:0.0971,\n",
      "Save model 6795\n",
      "Train Epoch:6796 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6796\n",
      "Train Epoch:6797 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6797\n",
      "Train Epoch:6798 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6798\n",
      "Train Epoch:6799 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6799\n",
      "Train Epoch:6800 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6800\n",
      "Train Epoch:6801 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6801\n",
      "Train Epoch:6802 learning rate:1.0000e-05, Loss_tot:0.0970,\n",
      "Save model 6802\n",
      "Train Epoch:6803 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 6803\n",
      "Train Epoch:6804 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 6804\n",
      "Train Epoch:6805 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 6805\n",
      "Train Epoch:6806 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 6806\n",
      "Train Epoch:6807 learning rate:1.0000e-05, Loss_tot:0.0969,\n",
      "Save model 6807\n",
      "Train Epoch:6808 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6808\n",
      "Train Epoch:6809 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6809\n",
      "Train Epoch:6810 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6810\n",
      "Train Epoch:6811 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6811\n",
      "Train Epoch:6812 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6812\n",
      "Train Epoch:6813 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6813\n",
      "Train Epoch:6814 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6814\n",
      "Train Epoch:6815 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6815\n",
      "Train Epoch:6816 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6816\n",
      "Train Epoch:6817 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6817\n",
      "Train Epoch:6818 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6818\n",
      "Train Epoch:6819 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6819\n",
      "Train Epoch:6820 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6820\n",
      "Train Epoch:6821 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6821\n",
      "Train Epoch:6822 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6822\n",
      "Train Epoch:6823 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6823\n",
      "Train Epoch:6824 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6824\n",
      "Train Epoch:6825 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6825\n",
      "Train Epoch:6826 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6826\n",
      "Train Epoch:6827 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6827\n",
      "Train Epoch:6828 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6828\n",
      "Train Epoch:6829 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6829\n",
      "Train Epoch:6830 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6830\n",
      "Train Epoch:6831 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6831\n",
      "Train Epoch:6832 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6832\n",
      "Train Epoch:6833 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6833\n",
      "Train Epoch:6834 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6834\n",
      "Train Epoch:6835 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6835\n",
      "Train Epoch:6836 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6836\n",
      "Train Epoch:6837 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6837\n",
      "Train Epoch:6838 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6838\n",
      "Train Epoch:6839 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6839\n",
      "Train Epoch:6840 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6840\n",
      "Train Epoch:6841 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6841\n",
      "Train Epoch:6842 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6842\n",
      "Train Epoch:6843 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6843\n",
      "Train Epoch:6844 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6844\n",
      "Train Epoch:6845 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6845\n",
      "Train Epoch:6846 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6846\n",
      "Train Epoch:6847 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6847\n",
      "Train Epoch:6848 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6848\n",
      "Train Epoch:6849 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6849\n",
      "Train Epoch:6850 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6850\n",
      "Train Epoch:6851 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6851\n",
      "Train Epoch:6852 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6852\n",
      "Train Epoch:6853 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6853\n",
      "Train Epoch:6854 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6854\n",
      "Train Epoch:6855 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6855\n",
      "Train Epoch:6856 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6856\n",
      "Train Epoch:6857 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6857\n",
      "Train Epoch:6858 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6859 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 6859\n",
      "Train Epoch:6860 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6860\n",
      "Train Epoch:6861 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6861\n",
      "Train Epoch:6862 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6862\n",
      "Train Epoch:6863 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6863\n",
      "Train Epoch:6864 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6864\n",
      "Train Epoch:6865 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6865\n",
      "Train Epoch:6866 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 6866\n",
      "Train Epoch:6867 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6867\n",
      "Train Epoch:6868 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6868\n",
      "Train Epoch:6869 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6869\n",
      "Train Epoch:6870 learning rate:1.0000e-05, Loss_tot:0.0966,\n",
      "Save model 6870\n",
      "Train Epoch:6871 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6871\n",
      "Train Epoch:6872 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6872\n",
      "Train Epoch:6873 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6873\n",
      "Train Epoch:6874 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6874\n",
      "Train Epoch:6875 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6875\n",
      "Train Epoch:6876 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6876\n",
      "Train Epoch:6877 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6877\n",
      "Train Epoch:6878 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6878\n",
      "Train Epoch:6879 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6879\n",
      "Train Epoch:6880 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6880\n",
      "Train Epoch:6881 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6881\n",
      "Train Epoch:6882 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6882\n",
      "Train Epoch:6883 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6883\n",
      "Train Epoch:6884 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6884\n",
      "Train Epoch:6885 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6885\n",
      "Train Epoch:6886 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6886\n",
      "Train Epoch:6887 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6887\n",
      "Train Epoch:6888 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6888\n",
      "Train Epoch:6889 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6889\n",
      "Train Epoch:6890 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6890\n",
      "Train Epoch:6891 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6891\n",
      "Train Epoch:6892 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6892\n",
      "Train Epoch:6893 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6893\n",
      "Train Epoch:6894 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6894\n",
      "Train Epoch:6895 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6895\n",
      "Train Epoch:6896 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6896\n",
      "Train Epoch:6897 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6897\n",
      "Train Epoch:6898 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6898\n",
      "Train Epoch:6899 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6899\n",
      "Train Epoch:6900 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6900\n",
      "Train Epoch:6901 learning rate:1.0000e-05, Loss_tot:0.0965,\n",
      "Save model 6901\n",
      "Train Epoch:6902 learning rate:1.0000e-05, Loss_tot:0.0964,\n",
      "Save model 6902\n",
      "Train Epoch:6903 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6903\n",
      "Train Epoch:6904 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6904\n",
      "Train Epoch:6905 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6905\n",
      "Train Epoch:6906 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6906\n",
      "Train Epoch:6907 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6907\n",
      "Train Epoch:6908 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6908\n",
      "Train Epoch:6909 learning rate:1.0000e-05, Loss_tot:0.0963,\n",
      "Save model 6909\n",
      "Train Epoch:6910 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6910\n",
      "Train Epoch:6911 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6911\n",
      "Train Epoch:6912 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6912\n",
      "Train Epoch:6913 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6913\n",
      "Train Epoch:6914 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6914\n",
      "Train Epoch:6915 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6915\n",
      "Train Epoch:6916 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6916\n",
      "Train Epoch:6917 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6917\n",
      "Train Epoch:6918 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6918\n",
      "Train Epoch:6919 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6919\n",
      "Train Epoch:6920 learning rate:1.0000e-05, Loss_tot:0.0962,\n",
      "Save model 6920\n",
      "Train Epoch:6921 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6921\n",
      "Train Epoch:6922 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6922\n",
      "Train Epoch:6923 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6923\n",
      "Train Epoch:6924 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6924\n",
      "Train Epoch:6925 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6925\n",
      "Train Epoch:6926 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6926\n",
      "Train Epoch:6927 learning rate:1.0000e-05, Loss_tot:0.0961,\n",
      "Save model 6927\n",
      "Train Epoch:6928 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6928\n",
      "Train Epoch:6929 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6929\n",
      "Train Epoch:6930 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6930\n",
      "Train Epoch:6931 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6931\n",
      "Train Epoch:6932 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6932\n",
      "Train Epoch:6933 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6933\n",
      "Train Epoch:6934 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6934\n",
      "Train Epoch:6935 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6935\n",
      "Train Epoch:6936 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6936\n",
      "Train Epoch:6937 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6937\n",
      "Train Epoch:6938 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6938\n",
      "Train Epoch:6939 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6939\n",
      "Train Epoch:6940 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6940\n",
      "Train Epoch:6941 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6941\n",
      "Train Epoch:6942 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6942\n",
      "Train Epoch:6943 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6943\n",
      "Train Epoch:6944 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6944\n",
      "Train Epoch:6945 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6945\n",
      "Train Epoch:6946 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6946\n",
      "Train Epoch:6947 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6947\n",
      "Train Epoch:6948 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6948\n",
      "Train Epoch:6949 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6949\n",
      "Train Epoch:6950 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6950\n",
      "Train Epoch:6951 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6951\n",
      "Train Epoch:6952 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6952\n",
      "Train Epoch:6953 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6953\n",
      "Train Epoch:6954 learning rate:1.0000e-05, Loss_tot:0.0960,\n",
      "Save model 6954\n",
      "Train Epoch:6955 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6955\n",
      "Train Epoch:6956 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6956\n",
      "Train Epoch:6957 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6957\n",
      "Train Epoch:6958 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6958\n",
      "Train Epoch:6959 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6959\n",
      "Train Epoch:6960 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6960\n",
      "Train Epoch:6961 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6961\n",
      "Train Epoch:6962 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6962\n",
      "Train Epoch:6963 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6963\n",
      "Train Epoch:6964 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6964\n",
      "Train Epoch:6965 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6965\n",
      "Train Epoch:6966 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:6967 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6967\n",
      "Train Epoch:6968 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6968\n",
      "Train Epoch:6969 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6969\n",
      "Train Epoch:6970 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6970\n",
      "Train Epoch:6971 learning rate:1.0000e-05, Loss_tot:0.0959,\n",
      "Save model 6971\n",
      "Train Epoch:6972 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6972\n",
      "Train Epoch:6973 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6973\n",
      "Train Epoch:6974 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6974\n",
      "Train Epoch:6975 learning rate:1.0000e-05, Loss_tot:0.0958,\n",
      "Save model 6975\n",
      "Train Epoch:6976 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6976\n",
      "Train Epoch:6977 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6977\n",
      "Train Epoch:6978 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6978\n",
      "Train Epoch:6979 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6979\n",
      "Train Epoch:6980 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6980\n",
      "Train Epoch:6981 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6981\n",
      "Train Epoch:6982 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6982\n",
      "Train Epoch:6983 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6983\n",
      "Train Epoch:6984 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6984\n",
      "Train Epoch:6985 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6985\n",
      "Train Epoch:6986 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6986\n",
      "Train Epoch:6987 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6987\n",
      "Train Epoch:6988 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6988\n",
      "Train Epoch:6989 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6989\n",
      "Train Epoch:6990 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6990\n",
      "Train Epoch:6991 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6991\n",
      "Train Epoch:6992 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6992\n",
      "Train Epoch:6993 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6993\n",
      "Train Epoch:6994 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6994\n",
      "Train Epoch:6995 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6995\n",
      "Train Epoch:6996 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6996\n",
      "Train Epoch:6997 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 6997\n",
      "Train Epoch:6998 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 6998\n",
      "Train Epoch:6999 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 6999\n",
      "Train Epoch:7000 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 7000\n",
      "Train Epoch:7001 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 7001\n",
      "Train Epoch:7002 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7002\n",
      "Train Epoch:7003 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 7003\n",
      "Train Epoch:7004 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 7004\n",
      "Train Epoch:7005 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 7005\n",
      "Train Epoch:7006 learning rate:1.0000e-05, Loss_tot:0.0957,\n",
      "Save model 7006\n",
      "Train Epoch:7007 learning rate:1.0000e-05, Loss_tot:0.0956,\n",
      "Save model 7007\n",
      "Train Epoch:7008 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7008\n",
      "Train Epoch:7009 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7009\n",
      "Train Epoch:7010 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7010\n",
      "Train Epoch:7011 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7011\n",
      "Train Epoch:7012 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7012\n",
      "Train Epoch:7013 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7013\n",
      "Train Epoch:7014 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7014\n",
      "Train Epoch:7015 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7015\n",
      "Train Epoch:7016 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7016\n",
      "Train Epoch:7017 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7017\n",
      "Train Epoch:7018 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7018\n",
      "Train Epoch:7019 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7019\n",
      "Train Epoch:7020 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7020\n",
      "Train Epoch:7021 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7021\n",
      "Train Epoch:7022 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7022\n",
      "Train Epoch:7023 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7023\n",
      "Train Epoch:7024 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7024\n",
      "Train Epoch:7025 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 7025\n",
      "Train Epoch:7026 learning rate:1.0000e-05, Loss_tot:0.0954,\n",
      "Save model 7026\n",
      "Train Epoch:7027 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7027\n",
      "Train Epoch:7028 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7028\n",
      "Train Epoch:7029 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7029\n",
      "Train Epoch:7030 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7030\n",
      "Train Epoch:7031 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7031\n",
      "Train Epoch:7032 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7032\n",
      "Train Epoch:7033 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7033\n",
      "Train Epoch:7034 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7034\n",
      "Train Epoch:7035 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7035\n",
      "Train Epoch:7036 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7036\n",
      "Train Epoch:7037 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7037\n",
      "Train Epoch:7038 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7038\n",
      "Train Epoch:7039 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7039\n",
      "Train Epoch:7040 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7040\n",
      "Train Epoch:7041 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7041\n",
      "Train Epoch:7042 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7042\n",
      "Train Epoch:7043 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7043\n",
      "Train Epoch:7044 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7044\n",
      "Train Epoch:7045 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7045\n",
      "Train Epoch:7046 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7046\n",
      "Train Epoch:7047 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7047\n",
      "Train Epoch:7048 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7048\n",
      "Train Epoch:7049 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7049\n",
      "Train Epoch:7050 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7050\n",
      "Train Epoch:7051 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7051\n",
      "Train Epoch:7052 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7052\n",
      "Train Epoch:7053 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7053\n",
      "Train Epoch:7054 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7054\n",
      "Train Epoch:7055 learning rate:1.0000e-05, Loss_tot:0.0953,\n",
      "Save model 7055\n",
      "Train Epoch:7056 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7056\n",
      "Train Epoch:7057 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7057\n",
      "Train Epoch:7058 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7058\n",
      "Train Epoch:7059 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7059\n",
      "Train Epoch:7060 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7060\n",
      "Train Epoch:7061 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7061\n",
      "Train Epoch:7062 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7062\n",
      "Train Epoch:7063 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7063\n",
      "Train Epoch:7064 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7064\n",
      "Train Epoch:7065 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7065\n",
      "Train Epoch:7066 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7066\n",
      "Train Epoch:7067 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7067\n",
      "Train Epoch:7068 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7068\n",
      "Train Epoch:7069 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7069\n",
      "Train Epoch:7070 learning rate:1.0000e-05, Loss_tot:0.0952,\n",
      "Save model 7070\n",
      "Train Epoch:7071 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7071\n",
      "Train Epoch:7072 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7072\n",
      "Train Epoch:7073 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7073\n",
      "Train Epoch:7074 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7075 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7075\n",
      "Train Epoch:7076 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7076\n",
      "Train Epoch:7077 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7077\n",
      "Train Epoch:7078 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7078\n",
      "Train Epoch:7079 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7079\n",
      "Train Epoch:7080 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7080\n",
      "Train Epoch:7081 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7081\n",
      "Train Epoch:7082 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7082\n",
      "Train Epoch:7083 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7083\n",
      "Train Epoch:7084 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7084\n",
      "Train Epoch:7085 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7085\n",
      "Train Epoch:7086 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7086\n",
      "Train Epoch:7087 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7087\n",
      "Train Epoch:7088 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7088\n",
      "Train Epoch:7089 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7089\n",
      "Train Epoch:7090 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7090\n",
      "Train Epoch:7091 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7091\n",
      "Train Epoch:7092 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7092\n",
      "Train Epoch:7093 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7093\n",
      "Train Epoch:7094 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7094\n",
      "Train Epoch:7095 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7095\n",
      "Train Epoch:7096 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7096\n",
      "Train Epoch:7097 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7097\n",
      "Train Epoch:7098 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7098\n",
      "Train Epoch:7099 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7099\n",
      "Train Epoch:7100 learning rate:1.0000e-05, Loss_tot:0.0951,\n",
      "Save model 7100\n",
      "Train Epoch:7101 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7101\n",
      "Train Epoch:7102 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 7102\n",
      "Train Epoch:7103 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7103\n",
      "Train Epoch:7104 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7104\n",
      "Train Epoch:7105 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7105\n",
      "Train Epoch:7106 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7106\n",
      "Train Epoch:7107 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7107\n",
      "Train Epoch:7108 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7108\n",
      "Train Epoch:7109 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7109\n",
      "Train Epoch:7110 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7110\n",
      "Train Epoch:7111 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7111\n",
      "Train Epoch:7112 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7112\n",
      "Train Epoch:7113 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7113\n",
      "Train Epoch:7114 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7114\n",
      "Train Epoch:7115 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7115\n",
      "Train Epoch:7116 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7116\n",
      "Train Epoch:7117 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7117\n",
      "Train Epoch:7118 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7118\n",
      "Train Epoch:7119 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7119\n",
      "Train Epoch:7120 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7120\n",
      "Train Epoch:7121 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7121\n",
      "Train Epoch:7122 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7122\n",
      "Train Epoch:7123 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7123\n",
      "Train Epoch:7124 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7124\n",
      "Train Epoch:7125 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7125\n",
      "Train Epoch:7126 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 7126\n",
      "Train Epoch:7127 learning rate:1.0000e-05, Loss_tot:0.0948,\n",
      "Save model 7127\n",
      "Train Epoch:7128 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7128\n",
      "Train Epoch:7129 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7129\n",
      "Train Epoch:7130 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7130\n",
      "Train Epoch:7131 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7131\n",
      "Train Epoch:7132 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7132\n",
      "Train Epoch:7133 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7133\n",
      "Train Epoch:7134 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7134\n",
      "Train Epoch:7135 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7135\n",
      "Train Epoch:7136 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7136\n",
      "Train Epoch:7137 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7137\n",
      "Train Epoch:7138 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7138\n",
      "Train Epoch:7139 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7139\n",
      "Train Epoch:7140 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7140\n",
      "Train Epoch:7141 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7141\n",
      "Train Epoch:7142 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7142\n",
      "Train Epoch:7143 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7143\n",
      "Train Epoch:7144 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7144\n",
      "Train Epoch:7145 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7145\n",
      "Train Epoch:7146 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7146\n",
      "Train Epoch:7147 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7147\n",
      "Train Epoch:7148 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7148\n",
      "Train Epoch:7149 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7149\n",
      "Train Epoch:7150 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7150\n",
      "Train Epoch:7151 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7151\n",
      "Train Epoch:7152 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7152\n",
      "Train Epoch:7153 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7153\n",
      "Train Epoch:7154 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7154\n",
      "Train Epoch:7155 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7155\n",
      "Train Epoch:7156 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7156\n",
      "Train Epoch:7157 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7157\n",
      "Train Epoch:7158 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7158\n",
      "Train Epoch:7159 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7159\n",
      "Train Epoch:7160 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 7160\n",
      "Train Epoch:7161 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7161\n",
      "Train Epoch:7162 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 7162\n",
      "Train Epoch:7163 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7163\n",
      "Train Epoch:7164 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7164\n",
      "Train Epoch:7165 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7165\n",
      "Train Epoch:7166 learning rate:1.0000e-05, Loss_tot:0.0945,\n",
      "Save model 7166\n",
      "Train Epoch:7167 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7167\n",
      "Train Epoch:7168 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7168\n",
      "Train Epoch:7169 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7169\n",
      "Train Epoch:7170 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7170\n",
      "Train Epoch:7171 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7171\n",
      "Train Epoch:7172 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7172\n",
      "Train Epoch:7173 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7173\n",
      "Train Epoch:7174 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7174\n",
      "Train Epoch:7175 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7175\n",
      "Train Epoch:7176 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7176\n",
      "Train Epoch:7177 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7177\n",
      "Train Epoch:7178 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7178\n",
      "Train Epoch:7179 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7179\n",
      "Train Epoch:7180 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7180\n",
      "Train Epoch:7181 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7181\n",
      "Train Epoch:7182 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7183 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7183\n",
      "Train Epoch:7184 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7184\n",
      "Train Epoch:7185 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 7185\n",
      "Train Epoch:7186 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7186\n",
      "Train Epoch:7187 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7187\n",
      "Train Epoch:7188 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7188\n",
      "Train Epoch:7189 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7189\n",
      "Train Epoch:7190 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7190\n",
      "Train Epoch:7191 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7191\n",
      "Train Epoch:7192 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7192\n",
      "Train Epoch:7193 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7193\n",
      "Train Epoch:7194 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7194\n",
      "Train Epoch:7195 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7195\n",
      "Train Epoch:7196 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7196\n",
      "Train Epoch:7197 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7197\n",
      "Train Epoch:7198 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7198\n",
      "Train Epoch:7199 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7199\n",
      "Train Epoch:7200 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7200\n",
      "Train Epoch:7201 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7201\n",
      "Train Epoch:7202 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7202\n",
      "Train Epoch:7203 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7203\n",
      "Train Epoch:7204 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7204\n",
      "Train Epoch:7205 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7205\n",
      "Train Epoch:7206 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7206\n",
      "Train Epoch:7207 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7207\n",
      "Train Epoch:7208 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7208\n",
      "Train Epoch:7209 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7209\n",
      "Train Epoch:7210 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 7210\n",
      "Train Epoch:7211 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7211\n",
      "Train Epoch:7212 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7212\n",
      "Train Epoch:7213 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7213\n",
      "Train Epoch:7214 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7214\n",
      "Train Epoch:7215 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7215\n",
      "Train Epoch:7216 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7216\n",
      "Train Epoch:7217 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7217\n",
      "Train Epoch:7218 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7218\n",
      "Train Epoch:7219 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 7219\n",
      "Train Epoch:7220 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7220\n",
      "Train Epoch:7221 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7221\n",
      "Train Epoch:7222 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7222\n",
      "Train Epoch:7223 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7223\n",
      "Train Epoch:7224 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7224\n",
      "Train Epoch:7225 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7225\n",
      "Train Epoch:7226 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7226\n",
      "Train Epoch:7227 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7227\n",
      "Train Epoch:7228 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7228\n",
      "Train Epoch:7229 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7229\n",
      "Train Epoch:7230 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7230\n",
      "Train Epoch:7231 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7231\n",
      "Train Epoch:7232 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7232\n",
      "Train Epoch:7233 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7233\n",
      "Train Epoch:7234 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7234\n",
      "Train Epoch:7235 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7235\n",
      "Train Epoch:7236 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7236\n",
      "Train Epoch:7237 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7237\n",
      "Train Epoch:7238 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7238\n",
      "Train Epoch:7239 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7239\n",
      "Train Epoch:7240 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7240\n",
      "Train Epoch:7241 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7241\n",
      "Train Epoch:7242 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7242\n",
      "Train Epoch:7243 learning rate:1.0000e-05, Loss_tot:0.0941,\n",
      "Save model 7243\n",
      "Train Epoch:7244 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7244\n",
      "Train Epoch:7245 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7245\n",
      "Train Epoch:7246 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7246\n",
      "Train Epoch:7247 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7247\n",
      "Train Epoch:7248 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7248\n",
      "Train Epoch:7249 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7249\n",
      "Train Epoch:7250 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7250\n",
      "Train Epoch:7251 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7251\n",
      "Train Epoch:7252 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7252\n",
      "Train Epoch:7253 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7253\n",
      "Train Epoch:7254 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7254\n",
      "Train Epoch:7255 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7255\n",
      "Train Epoch:7256 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7256\n",
      "Train Epoch:7257 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7257\n",
      "Train Epoch:7258 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7258\n",
      "Train Epoch:7259 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7259\n",
      "Train Epoch:7260 learning rate:1.0000e-05, Loss_tot:0.0940,\n",
      "Save model 7260\n",
      "Train Epoch:7261 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7261\n",
      "Train Epoch:7262 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7262\n",
      "Train Epoch:7263 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7263\n",
      "Train Epoch:7264 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7264\n",
      "Train Epoch:7265 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7265\n",
      "Train Epoch:7266 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7266\n",
      "Train Epoch:7267 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7267\n",
      "Train Epoch:7268 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7268\n",
      "Train Epoch:7269 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7269\n",
      "Train Epoch:7270 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7270\n",
      "Train Epoch:7271 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7271\n",
      "Train Epoch:7272 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 7272\n",
      "Train Epoch:7273 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7273\n",
      "Train Epoch:7274 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7274\n",
      "Train Epoch:7275 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7275\n",
      "Train Epoch:7276 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7276\n",
      "Train Epoch:7277 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7277\n",
      "Train Epoch:7278 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7278\n",
      "Train Epoch:7279 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7279\n",
      "Train Epoch:7280 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7280\n",
      "Train Epoch:7281 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7281\n",
      "Train Epoch:7282 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7282\n",
      "Train Epoch:7283 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7283\n",
      "Train Epoch:7284 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7284\n",
      "Train Epoch:7285 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7285\n",
      "Train Epoch:7286 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7286\n",
      "Train Epoch:7287 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7287\n",
      "Train Epoch:7288 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7288\n",
      "Train Epoch:7289 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7289\n",
      "Train Epoch:7290 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7291 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7291\n",
      "Train Epoch:7292 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7292\n",
      "Train Epoch:7293 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7293\n",
      "Train Epoch:7294 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7294\n",
      "Train Epoch:7295 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7295\n",
      "Train Epoch:7296 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7296\n",
      "Train Epoch:7297 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7297\n",
      "Train Epoch:7298 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7298\n",
      "Train Epoch:7299 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7299\n",
      "Train Epoch:7300 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7300\n",
      "Train Epoch:7301 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7301\n",
      "Train Epoch:7302 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7302\n",
      "Train Epoch:7303 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7303\n",
      "Train Epoch:7304 learning rate:1.0000e-05, Loss_tot:0.0938,\n",
      "Save model 7304\n",
      "Train Epoch:7305 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7305\n",
      "Train Epoch:7306 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7306\n",
      "Train Epoch:7307 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7307\n",
      "Train Epoch:7308 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7308\n",
      "Train Epoch:7309 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7309\n",
      "Train Epoch:7310 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7310\n",
      "Train Epoch:7311 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7311\n",
      "Train Epoch:7312 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7312\n",
      "Train Epoch:7313 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7313\n",
      "Train Epoch:7314 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7314\n",
      "Train Epoch:7315 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7315\n",
      "Train Epoch:7316 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7316\n",
      "Train Epoch:7317 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7317\n",
      "Train Epoch:7318 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7318\n",
      "Train Epoch:7319 learning rate:1.0000e-05, Loss_tot:0.0937,\n",
      "Save model 7319\n",
      "Train Epoch:7320 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7320\n",
      "Train Epoch:7321 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7321\n",
      "Train Epoch:7322 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7322\n",
      "Train Epoch:7323 learning rate:1.0000e-05, Loss_tot:0.0936,\n",
      "Save model 7323\n",
      "Train Epoch:7324 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7324\n",
      "Train Epoch:7325 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7325\n",
      "Train Epoch:7326 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7326\n",
      "Train Epoch:7327 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7327\n",
      "Train Epoch:7328 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7328\n",
      "Train Epoch:7329 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7329\n",
      "Train Epoch:7330 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7330\n",
      "Train Epoch:7331 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7331\n",
      "Train Epoch:7332 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7332\n",
      "Train Epoch:7333 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7333\n",
      "Train Epoch:7334 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7334\n",
      "Train Epoch:7335 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7335\n",
      "Train Epoch:7336 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7336\n",
      "Train Epoch:7337 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7337\n",
      "Train Epoch:7338 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7338\n",
      "Train Epoch:7339 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 7339\n",
      "Train Epoch:7340 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7340\n",
      "Train Epoch:7341 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7341\n",
      "Train Epoch:7342 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7342\n",
      "Train Epoch:7343 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7343\n",
      "Train Epoch:7344 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7344\n",
      "Train Epoch:7345 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7345\n",
      "Train Epoch:7346 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7346\n",
      "Train Epoch:7347 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7347\n",
      "Train Epoch:7348 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7348\n",
      "Train Epoch:7349 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7349\n",
      "Train Epoch:7350 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7350\n",
      "Train Epoch:7351 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7351\n",
      "Train Epoch:7352 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7352\n",
      "Train Epoch:7353 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7353\n",
      "Train Epoch:7354 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7354\n",
      "Train Epoch:7355 learning rate:1.0000e-05, Loss_tot:0.0934,\n",
      "Save model 7355\n",
      "Train Epoch:7356 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7356\n",
      "Train Epoch:7357 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7357\n",
      "Train Epoch:7358 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7358\n",
      "Train Epoch:7359 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7359\n",
      "Train Epoch:7360 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7360\n",
      "Train Epoch:7361 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7361\n",
      "Train Epoch:7362 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 7362\n",
      "Train Epoch:7363 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7363\n",
      "Train Epoch:7364 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7364\n",
      "Train Epoch:7365 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7365\n",
      "Train Epoch:7366 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7366\n",
      "Train Epoch:7367 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7367\n",
      "Train Epoch:7368 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7368\n",
      "Train Epoch:7369 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7369\n",
      "Train Epoch:7370 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7370\n",
      "Train Epoch:7371 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7371\n",
      "Train Epoch:7372 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7372\n",
      "Train Epoch:7373 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7373\n",
      "Train Epoch:7374 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7374\n",
      "Train Epoch:7375 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7375\n",
      "Train Epoch:7376 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7376\n",
      "Train Epoch:7377 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7377\n",
      "Train Epoch:7378 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7378\n",
      "Train Epoch:7379 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7379\n",
      "Train Epoch:7380 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7380\n",
      "Train Epoch:7381 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7381\n",
      "Train Epoch:7382 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7382\n",
      "Train Epoch:7383 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7383\n",
      "Train Epoch:7384 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7384\n",
      "Train Epoch:7385 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7385\n",
      "Train Epoch:7386 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7386\n",
      "Train Epoch:7387 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7387\n",
      "Train Epoch:7388 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7388\n",
      "Train Epoch:7389 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7389\n",
      "Train Epoch:7390 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7390\n",
      "Train Epoch:7391 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7391\n",
      "Train Epoch:7392 learning rate:1.0000e-05, Loss_tot:0.0932,\n",
      "Save model 7392\n",
      "Train Epoch:7393 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7393\n",
      "Train Epoch:7394 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7394\n",
      "Train Epoch:7395 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7395\n",
      "Train Epoch:7396 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7396\n",
      "Train Epoch:7397 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7397\n",
      "Train Epoch:7398 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7399 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7399\n",
      "Train Epoch:7400 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7400\n",
      "Train Epoch:7401 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7401\n",
      "Train Epoch:7402 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7402\n",
      "Train Epoch:7403 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7403\n",
      "Train Epoch:7404 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7404\n",
      "Train Epoch:7405 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7405\n",
      "Train Epoch:7406 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7406\n",
      "Train Epoch:7407 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7407\n",
      "Train Epoch:7408 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7408\n",
      "Train Epoch:7409 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7409\n",
      "Train Epoch:7410 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7410\n",
      "Train Epoch:7411 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7411\n",
      "Train Epoch:7412 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7412\n",
      "Train Epoch:7413 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7413\n",
      "Train Epoch:7414 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7414\n",
      "Train Epoch:7415 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7415\n",
      "Train Epoch:7416 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7416\n",
      "Train Epoch:7417 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7417\n",
      "Train Epoch:7418 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7418\n",
      "Train Epoch:7419 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7419\n",
      "Train Epoch:7420 learning rate:1.0000e-05, Loss_tot:0.0931,\n",
      "Save model 7420\n",
      "Train Epoch:7421 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7421\n",
      "Train Epoch:7422 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7422\n",
      "Train Epoch:7423 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7423\n",
      "Train Epoch:7424 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7424\n",
      "Train Epoch:7425 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7425\n",
      "Train Epoch:7426 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7426\n",
      "Train Epoch:7427 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7427\n",
      "Train Epoch:7428 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7428\n",
      "Train Epoch:7429 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7429\n",
      "Train Epoch:7430 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7430\n",
      "Train Epoch:7431 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7431\n",
      "Train Epoch:7432 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7432\n",
      "Train Epoch:7433 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7433\n",
      "Train Epoch:7434 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7434\n",
      "Train Epoch:7435 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7435\n",
      "Train Epoch:7436 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 7436\n",
      "Train Epoch:7437 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7437\n",
      "Train Epoch:7438 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7438\n",
      "Train Epoch:7439 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7439\n",
      "Train Epoch:7440 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7440\n",
      "Train Epoch:7441 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7441\n",
      "Train Epoch:7442 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7442\n",
      "Train Epoch:7443 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7443\n",
      "Train Epoch:7444 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7444\n",
      "Train Epoch:7445 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7445\n",
      "Train Epoch:7446 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7446\n",
      "Train Epoch:7447 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7447\n",
      "Train Epoch:7448 learning rate:1.0000e-05, Loss_tot:0.0929,\n",
      "Save model 7448\n",
      "Train Epoch:7449 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7449\n",
      "Train Epoch:7450 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7450\n",
      "Train Epoch:7451 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7451\n",
      "Train Epoch:7452 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7452\n",
      "Train Epoch:7453 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7453\n",
      "Train Epoch:7454 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7454\n",
      "Train Epoch:7455 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7455\n",
      "Train Epoch:7456 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7456\n",
      "Train Epoch:7457 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7457\n",
      "Train Epoch:7458 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7458\n",
      "Train Epoch:7459 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7459\n",
      "Train Epoch:7460 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7460\n",
      "Train Epoch:7461 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7461\n",
      "Train Epoch:7462 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7462\n",
      "Train Epoch:7463 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7463\n",
      "Train Epoch:7464 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7464\n",
      "Train Epoch:7465 learning rate:1.0000e-05, Loss_tot:0.0928,\n",
      "Save model 7465\n",
      "Train Epoch:7466 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7466\n",
      "Train Epoch:7467 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7467\n",
      "Train Epoch:7468 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7468\n",
      "Train Epoch:7469 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7469\n",
      "Train Epoch:7470 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7470\n",
      "Train Epoch:7471 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7471\n",
      "Train Epoch:7472 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7472\n",
      "Train Epoch:7473 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7473\n",
      "Train Epoch:7474 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7474\n",
      "Train Epoch:7475 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7475\n",
      "Train Epoch:7476 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7476\n",
      "Train Epoch:7477 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7477\n",
      "Train Epoch:7478 learning rate:1.0000e-05, Loss_tot:0.0927,\n",
      "Save model 7478\n",
      "Train Epoch:7479 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7479\n",
      "Train Epoch:7480 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7480\n",
      "Train Epoch:7481 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7481\n",
      "Train Epoch:7482 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7482\n",
      "Train Epoch:7483 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7483\n",
      "Train Epoch:7484 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7484\n",
      "Train Epoch:7485 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7485\n",
      "Train Epoch:7486 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7486\n",
      "Train Epoch:7487 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7487\n",
      "Train Epoch:7488 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7488\n",
      "Train Epoch:7489 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7489\n",
      "Train Epoch:7490 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7490\n",
      "Train Epoch:7491 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7491\n",
      "Train Epoch:7492 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7492\n",
      "Train Epoch:7493 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7493\n",
      "Train Epoch:7494 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7494\n",
      "Train Epoch:7495 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7495\n",
      "Train Epoch:7496 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7496\n",
      "Train Epoch:7497 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7497\n",
      "Train Epoch:7498 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7498\n",
      "Train Epoch:7499 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7499\n",
      "Train Epoch:7500 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7500\n",
      "Train Epoch:7501 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7501\n",
      "Train Epoch:7502 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7502\n",
      "Train Epoch:7503 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7503\n",
      "Train Epoch:7504 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7504\n",
      "Train Epoch:7505 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7505\n",
      "Train Epoch:7506 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7507 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7507\n",
      "Train Epoch:7508 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7508\n",
      "Train Epoch:7509 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7509\n",
      "Train Epoch:7510 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 7510\n",
      "Train Epoch:7511 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7511\n",
      "Train Epoch:7512 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7512\n",
      "Train Epoch:7513 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7513\n",
      "Train Epoch:7514 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7514\n",
      "Train Epoch:7515 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7515\n",
      "Train Epoch:7516 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 7516\n",
      "Train Epoch:7517 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7517\n",
      "Train Epoch:7518 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7518\n",
      "Train Epoch:7519 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7519\n",
      "Train Epoch:7520 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7520\n",
      "Train Epoch:7521 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7521\n",
      "Train Epoch:7522 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7522\n",
      "Train Epoch:7523 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7523\n",
      "Train Epoch:7524 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7524\n",
      "Train Epoch:7525 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7525\n",
      "Train Epoch:7526 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7526\n",
      "Train Epoch:7527 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7527\n",
      "Train Epoch:7528 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7528\n",
      "Train Epoch:7529 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7529\n",
      "Train Epoch:7530 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7530\n",
      "Train Epoch:7531 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7531\n",
      "Train Epoch:7532 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7532\n",
      "Train Epoch:7533 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7533\n",
      "Train Epoch:7534 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7534\n",
      "Train Epoch:7535 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7535\n",
      "Train Epoch:7536 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7536\n",
      "Train Epoch:7537 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7537\n",
      "Train Epoch:7538 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7538\n",
      "Train Epoch:7539 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7539\n",
      "Train Epoch:7540 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7540\n",
      "Train Epoch:7541 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7541\n",
      "Train Epoch:7542 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7542\n",
      "Train Epoch:7543 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7543\n",
      "Train Epoch:7544 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7544\n",
      "Train Epoch:7545 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7545\n",
      "Train Epoch:7546 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7546\n",
      "Train Epoch:7547 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7547\n",
      "Train Epoch:7548 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7548\n",
      "Train Epoch:7549 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7549\n",
      "Train Epoch:7550 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7550\n",
      "Train Epoch:7551 learning rate:1.0000e-05, Loss_tot:0.0924,\n",
      "Save model 7551\n",
      "Train Epoch:7552 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7552\n",
      "Train Epoch:7553 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7553\n",
      "Train Epoch:7554 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7554\n",
      "Train Epoch:7555 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7555\n",
      "Train Epoch:7556 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7556\n",
      "Train Epoch:7557 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7557\n",
      "Train Epoch:7558 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7558\n",
      "Train Epoch:7559 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7559\n",
      "Train Epoch:7560 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7560\n",
      "Train Epoch:7561 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7561\n",
      "Train Epoch:7562 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7562\n",
      "Train Epoch:7563 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7563\n",
      "Train Epoch:7564 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7564\n",
      "Train Epoch:7565 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7565\n",
      "Train Epoch:7566 learning rate:1.0000e-05, Loss_tot:0.0923,\n",
      "Save model 7566\n",
      "Train Epoch:7567 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7567\n",
      "Train Epoch:7568 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7568\n",
      "Train Epoch:7569 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7569\n",
      "Train Epoch:7570 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7570\n",
      "Train Epoch:7571 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7571\n",
      "Train Epoch:7572 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7572\n",
      "Train Epoch:7573 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7573\n",
      "Train Epoch:7574 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7574\n",
      "Train Epoch:7575 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7575\n",
      "Train Epoch:7576 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7576\n",
      "Train Epoch:7577 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7577\n",
      "Train Epoch:7578 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7578\n",
      "Train Epoch:7579 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7579\n",
      "Train Epoch:7580 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7580\n",
      "Train Epoch:7581 learning rate:1.0000e-05, Loss_tot:0.0922,\n",
      "Save model 7581\n",
      "Train Epoch:7582 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7582\n",
      "Train Epoch:7583 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7583\n",
      "Train Epoch:7584 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7584\n",
      "Train Epoch:7585 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7585\n",
      "Train Epoch:7586 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7586\n",
      "Train Epoch:7587 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7587\n",
      "Train Epoch:7588 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7588\n",
      "Train Epoch:7589 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7589\n",
      "Train Epoch:7590 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7590\n",
      "Train Epoch:7591 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7591\n",
      "Train Epoch:7592 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7592\n",
      "Train Epoch:7593 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7593\n",
      "Train Epoch:7594 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7594\n",
      "Train Epoch:7595 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7595\n",
      "Train Epoch:7596 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7596\n",
      "Train Epoch:7597 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7597\n",
      "Train Epoch:7598 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7598\n",
      "Train Epoch:7599 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7599\n",
      "Train Epoch:7600 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7600\n",
      "Train Epoch:7601 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7601\n",
      "Train Epoch:7602 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7602\n",
      "Train Epoch:7603 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7603\n",
      "Train Epoch:7604 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "save model\n",
      "Save model 7604\n",
      "Train Epoch:7605 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7605\n",
      "Train Epoch:7606 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7606\n",
      "Train Epoch:7607 learning rate:1.0000e-05, Loss_tot:0.0921,\n",
      "Save model 7607\n",
      "Train Epoch:7608 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7608\n",
      "Train Epoch:7609 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "save model\n",
      "Save model 7609\n",
      "Train Epoch:7610 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7610\n",
      "Train Epoch:7611 learning rate:1.0000e-05, Loss_tot:0.0920,\n",
      "Save model 7611\n",
      "Train Epoch:7612 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7612\n",
      "Train Epoch:7613 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "save model\n",
      "Save model 7613\n",
      "Train Epoch:7614 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7615 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7615\n",
      "Train Epoch:7616 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7616\n",
      "Train Epoch:7617 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "save model\n",
      "Save model 7617\n",
      "Train Epoch:7618 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7618\n",
      "Train Epoch:7619 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7619\n",
      "Train Epoch:7620 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "save model\n",
      "Save model 7620\n",
      "Train Epoch:7621 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7621\n",
      "Train Epoch:7622 learning rate:1.0000e-05, Loss_tot:0.0919,\n",
      "Save model 7622\n",
      "Train Epoch:7623 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7623\n",
      "Train Epoch:7624 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7624\n",
      "Train Epoch:7625 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7625\n",
      "Train Epoch:7626 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7626\n",
      "Train Epoch:7627 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "save model\n",
      "Save model 7627\n",
      "Train Epoch:7628 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "save model\n",
      "Save model 7628\n",
      "Train Epoch:7629 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7629\n",
      "Train Epoch:7630 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7630\n",
      "Train Epoch:7631 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7631\n",
      "Train Epoch:7632 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7632\n",
      "Train Epoch:7633 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7633\n",
      "Train Epoch:7634 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7634\n",
      "Train Epoch:7635 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7635\n",
      "Train Epoch:7636 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7636\n",
      "Train Epoch:7637 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7637\n",
      "Train Epoch:7638 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7638\n",
      "Train Epoch:7639 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7639\n",
      "Train Epoch:7640 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7640\n",
      "Train Epoch:7641 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7641\n",
      "Train Epoch:7642 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7642\n",
      "Train Epoch:7643 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7643\n",
      "Train Epoch:7644 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7644\n",
      "Train Epoch:7645 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7645\n",
      "Train Epoch:7646 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7646\n",
      "Train Epoch:7647 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "save model\n",
      "Save model 7647\n",
      "Train Epoch:7648 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7648\n",
      "Train Epoch:7649 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7649\n",
      "Train Epoch:7650 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7650\n",
      "Train Epoch:7651 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7651\n",
      "Train Epoch:7652 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "save model\n",
      "Save model 7652\n",
      "Train Epoch:7653 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7653\n",
      "Train Epoch:7654 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7654\n",
      "Train Epoch:7655 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7655\n",
      "Train Epoch:7656 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7656\n",
      "Train Epoch:7657 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7657\n",
      "Train Epoch:7658 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7658\n",
      "Train Epoch:7659 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7659\n",
      "Train Epoch:7660 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7660\n",
      "Train Epoch:7661 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7661\n",
      "Train Epoch:7662 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "save model\n",
      "Save model 7662\n",
      "Train Epoch:7663 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7663\n",
      "Train Epoch:7664 learning rate:1.0000e-05, Loss_tot:0.0918,\n",
      "Save model 7664\n",
      "Train Epoch:7665 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 7665\n",
      "Train Epoch:7666 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7666\n",
      "Train Epoch:7667 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "save model\n",
      "Save model 7667\n",
      "Train Epoch:7668 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7668\n",
      "Train Epoch:7669 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7669\n",
      "Train Epoch:7670 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7670\n",
      "Train Epoch:7671 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "save model\n",
      "Save model 7671\n",
      "Train Epoch:7672 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7672\n",
      "Train Epoch:7673 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7673\n",
      "Train Epoch:7674 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7674\n",
      "Train Epoch:7675 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "save model\n",
      "Save model 7675\n",
      "Train Epoch:7676 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7676\n",
      "Train Epoch:7677 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7677\n",
      "Train Epoch:7678 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "save model\n",
      "Save model 7678\n",
      "Train Epoch:7679 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7679\n",
      "Train Epoch:7680 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 7680\n",
      "Train Epoch:7681 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7681\n",
      "Train Epoch:7682 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7682\n",
      "Train Epoch:7683 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7683\n",
      "Train Epoch:7684 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7684\n",
      "Train Epoch:7685 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7685\n",
      "Train Epoch:7686 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "save model\n",
      "Save model 7686\n",
      "Train Epoch:7687 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7687\n",
      "Train Epoch:7688 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7688\n",
      "Train Epoch:7689 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7689\n",
      "Train Epoch:7690 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "save model\n",
      "Save model 7690\n",
      "Train Epoch:7691 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7691\n",
      "Train Epoch:7692 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "save model\n",
      "Save model 7692\n",
      "Train Epoch:7693 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7693\n",
      "Train Epoch:7694 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7694\n",
      "Train Epoch:7695 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "save model\n",
      "Save model 7695\n",
      "Train Epoch:7696 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7696\n",
      "Train Epoch:7697 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7697\n",
      "Train Epoch:7698 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7698\n",
      "Train Epoch:7699 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7699\n",
      "Train Epoch:7700 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7700\n",
      "Train Epoch:7701 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7701\n",
      "Train Epoch:7702 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7702\n",
      "Train Epoch:7703 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 7703\n",
      "Train Epoch:7704 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7704\n",
      "Train Epoch:7705 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7705\n",
      "Train Epoch:7706 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7706\n",
      "Train Epoch:7707 learning rate:1.0000e-05, Loss_tot:0.0914,\n",
      "Save model 7707\n",
      "Train Epoch:7708 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7708\n",
      "Train Epoch:7709 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7709\n",
      "Train Epoch:7710 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7710\n",
      "Train Epoch:7711 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7711\n",
      "Train Epoch:7712 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7712\n",
      "Train Epoch:7713 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7713\n",
      "Train Epoch:7714 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7714\n",
      "Train Epoch:7715 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7715\n",
      "Train Epoch:7716 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7716\n",
      "Train Epoch:7717 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7717\n",
      "Train Epoch:7718 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "Save model 7718\n",
      "Train Epoch:7719 learning rate:1.0000e-05, Loss_tot:0.0913,\n",
      "save model\n",
      "Save model 7719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7720 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7720\n",
      "Train Epoch:7721 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7721\n",
      "Train Epoch:7722 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7722\n",
      "Train Epoch:7723 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7723\n",
      "Train Epoch:7724 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7724\n",
      "Train Epoch:7725 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7725\n",
      "Train Epoch:7726 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7726\n",
      "Train Epoch:7727 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7727\n",
      "Train Epoch:7728 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7728\n",
      "Train Epoch:7729 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "save model\n",
      "Save model 7729\n",
      "Train Epoch:7730 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7730\n",
      "Train Epoch:7731 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7731\n",
      "Train Epoch:7732 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7732\n",
      "Train Epoch:7733 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7733\n",
      "Train Epoch:7734 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7734\n",
      "Train Epoch:7735 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7735\n",
      "Train Epoch:7736 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7736\n",
      "Train Epoch:7737 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7737\n",
      "Train Epoch:7738 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7738\n",
      "Train Epoch:7739 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7739\n",
      "Train Epoch:7740 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7740\n",
      "Train Epoch:7741 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7741\n",
      "Train Epoch:7742 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7742\n",
      "Train Epoch:7743 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7743\n",
      "Train Epoch:7744 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7744\n",
      "Train Epoch:7745 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7745\n",
      "Train Epoch:7746 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "save model\n",
      "Save model 7746\n",
      "Train Epoch:7747 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7747\n",
      "Train Epoch:7748 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7748\n",
      "Train Epoch:7749 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7749\n",
      "Train Epoch:7750 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7750\n",
      "Train Epoch:7751 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7751\n",
      "Train Epoch:7752 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7752\n",
      "Train Epoch:7753 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7753\n",
      "Train Epoch:7754 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7754\n",
      "Train Epoch:7755 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "save model\n",
      "Save model 7755\n",
      "Train Epoch:7756 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7756\n",
      "Train Epoch:7757 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7757\n",
      "Train Epoch:7758 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7758\n",
      "Train Epoch:7759 learning rate:1.0000e-05, Loss_tot:0.0912,\n",
      "Save model 7759\n",
      "Train Epoch:7760 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7760\n",
      "Train Epoch:7761 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7761\n",
      "Train Epoch:7762 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7762\n",
      "Train Epoch:7763 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7763\n",
      "Train Epoch:7764 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7764\n",
      "Train Epoch:7765 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7765\n",
      "Train Epoch:7766 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "save model\n",
      "Save model 7766\n",
      "Train Epoch:7767 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7767\n",
      "Train Epoch:7768 learning rate:1.0000e-05, Loss_tot:0.0911,\n",
      "Save model 7768\n",
      "Train Epoch:7769 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7769\n",
      "Train Epoch:7770 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "save model\n",
      "Save model 7770\n",
      "Train Epoch:7771 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7771\n",
      "Train Epoch:7772 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7772\n",
      "Train Epoch:7773 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "save model\n",
      "Save model 7773\n",
      "Train Epoch:7774 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "save model\n",
      "Save model 7774\n",
      "Train Epoch:7775 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7775\n",
      "Train Epoch:7776 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7776\n",
      "Train Epoch:7777 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "save model\n",
      "Save model 7777\n",
      "Train Epoch:7778 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7778\n",
      "Train Epoch:7779 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 7779\n",
      "Train Epoch:7780 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7780\n",
      "Train Epoch:7781 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "save model\n",
      "Save model 7781\n",
      "Train Epoch:7782 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7782\n",
      "Train Epoch:7783 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7783\n",
      "Train Epoch:7784 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "save model\n",
      "Save model 7784\n",
      "Train Epoch:7785 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7785\n",
      "Train Epoch:7786 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7786\n",
      "Train Epoch:7787 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "save model\n",
      "Save model 7787\n",
      "Train Epoch:7788 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7788\n",
      "Train Epoch:7789 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7789\n",
      "Train Epoch:7790 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7790\n",
      "Train Epoch:7791 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 7791\n",
      "Train Epoch:7792 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "save model\n",
      "Save model 7792\n",
      "Train Epoch:7793 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7793\n",
      "Train Epoch:7794 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7794\n",
      "Train Epoch:7795 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7795\n",
      "Train Epoch:7796 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "save model\n",
      "Save model 7796\n",
      "Train Epoch:7797 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "save model\n",
      "Save model 7797\n",
      "Train Epoch:7798 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "save model\n",
      "Save model 7798\n",
      "Train Epoch:7799 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7799\n",
      "Train Epoch:7800 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7800\n",
      "Train Epoch:7801 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "save model\n",
      "Save model 7801\n",
      "Train Epoch:7802 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "save model\n",
      "Save model 7802\n",
      "Train Epoch:7803 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7803\n",
      "Train Epoch:7804 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7804\n",
      "Train Epoch:7805 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7805\n",
      "Train Epoch:7806 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7806\n",
      "Train Epoch:7807 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7807\n",
      "Train Epoch:7808 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7808\n",
      "Train Epoch:7809 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7809\n",
      "Train Epoch:7810 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7810\n",
      "Train Epoch:7811 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7811\n",
      "Train Epoch:7812 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7812\n",
      "Train Epoch:7813 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7813\n",
      "Train Epoch:7814 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7814\n",
      "Train Epoch:7815 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7815\n",
      "Train Epoch:7816 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7816\n",
      "Train Epoch:7817 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7817\n",
      "Train Epoch:7818 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7818\n",
      "Train Epoch:7819 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7819\n",
      "Train Epoch:7820 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7820\n",
      "Train Epoch:7821 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7821\n",
      "Train Epoch:7822 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "save model\n",
      "Save model 7822\n",
      "Train Epoch:7823 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7824 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7824\n",
      "Train Epoch:7825 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "save model\n",
      "Save model 7825\n",
      "Train Epoch:7826 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7826\n",
      "Train Epoch:7827 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7827\n",
      "Train Epoch:7828 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7828\n",
      "Train Epoch:7829 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 7829\n",
      "Train Epoch:7830 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7830\n",
      "Train Epoch:7831 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7831\n",
      "Train Epoch:7832 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7832\n",
      "Train Epoch:7833 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 7833\n",
      "Train Epoch:7834 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "save model\n",
      "Save model 7834\n",
      "Train Epoch:7835 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7835\n",
      "Train Epoch:7836 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7836\n",
      "Train Epoch:7837 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "save model\n",
      "Save model 7837\n",
      "Train Epoch:7838 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "save model\n",
      "Save model 7838\n",
      "Train Epoch:7839 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7839\n",
      "Train Epoch:7840 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "save model\n",
      "Save model 7840\n",
      "Train Epoch:7841 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7841\n",
      "Train Epoch:7842 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7842\n",
      "Train Epoch:7843 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7843\n",
      "Train Epoch:7844 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7844\n",
      "Train Epoch:7845 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7845\n",
      "Train Epoch:7846 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7846\n",
      "Train Epoch:7847 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7847\n",
      "Train Epoch:7848 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7848\n",
      "Train Epoch:7849 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7849\n",
      "Train Epoch:7850 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7850\n",
      "Train Epoch:7851 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7851\n",
      "Train Epoch:7852 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7852\n",
      "Train Epoch:7853 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7853\n",
      "Train Epoch:7854 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7854\n",
      "Train Epoch:7855 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7855\n",
      "Train Epoch:7856 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7856\n",
      "Train Epoch:7857 learning rate:1.0000e-05, Loss_tot:0.0906,\n",
      "Save model 7857\n",
      "Train Epoch:7858 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7858\n",
      "Train Epoch:7859 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7859\n",
      "Train Epoch:7860 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7860\n",
      "Train Epoch:7861 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "save model\n",
      "Save model 7861\n",
      "Train Epoch:7862 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "save model\n",
      "Save model 7862\n",
      "Train Epoch:7863 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7863\n",
      "Train Epoch:7864 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "save model\n",
      "Save model 7864\n",
      "Train Epoch:7865 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "save model\n",
      "Save model 7865\n",
      "Train Epoch:7866 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7866\n",
      "Train Epoch:7867 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7867\n",
      "Train Epoch:7868 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "save model\n",
      "Save model 7868\n",
      "Train Epoch:7869 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7869\n",
      "Train Epoch:7870 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7870\n",
      "Train Epoch:7871 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7871\n",
      "Train Epoch:7872 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7872\n",
      "Train Epoch:7873 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7873\n",
      "Train Epoch:7874 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7874\n",
      "Train Epoch:7875 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7875\n",
      "Train Epoch:7876 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "save model\n",
      "Save model 7876\n",
      "Train Epoch:7877 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7877\n",
      "Train Epoch:7878 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7878\n",
      "Train Epoch:7879 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7879\n",
      "Train Epoch:7880 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7880\n",
      "Train Epoch:7881 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7881\n",
      "Train Epoch:7882 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7882\n",
      "Train Epoch:7883 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7883\n",
      "Train Epoch:7884 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7884\n",
      "Train Epoch:7885 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7885\n",
      "Train Epoch:7886 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "save model\n",
      "Save model 7886\n",
      "Train Epoch:7887 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7887\n",
      "Train Epoch:7888 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7888\n",
      "Train Epoch:7889 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 7889\n",
      "Train Epoch:7890 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7890\n",
      "Train Epoch:7891 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "save model\n",
      "Save model 7891\n",
      "Train Epoch:7892 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7892\n",
      "Train Epoch:7893 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7893\n",
      "Train Epoch:7894 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7894\n",
      "Train Epoch:7895 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "save model\n",
      "Save model 7895\n",
      "Train Epoch:7896 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7896\n",
      "Train Epoch:7897 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7897\n",
      "Train Epoch:7898 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7898\n",
      "Train Epoch:7899 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "save model\n",
      "Save model 7899\n",
      "Train Epoch:7900 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7900\n",
      "Train Epoch:7901 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7901\n",
      "Train Epoch:7902 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7902\n",
      "Train Epoch:7903 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7903\n",
      "Train Epoch:7904 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7904\n",
      "Train Epoch:7905 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7905\n",
      "Train Epoch:7906 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7906\n",
      "Train Epoch:7907 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "save model\n",
      "Save model 7907\n",
      "Train Epoch:7908 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7908\n",
      "Train Epoch:7909 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7909\n",
      "Train Epoch:7910 learning rate:1.0000e-05, Loss_tot:0.0904,\n",
      "Save model 7910\n",
      "Train Epoch:7911 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7911\n",
      "Train Epoch:7912 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7912\n",
      "Train Epoch:7913 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7913\n",
      "Train Epoch:7914 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7914\n",
      "Train Epoch:7915 learning rate:1.0000e-05, Loss_tot:0.0903,\n",
      "Save model 7915\n",
      "Train Epoch:7916 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7916\n",
      "Train Epoch:7917 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "save model\n",
      "Save model 7917\n",
      "Train Epoch:7918 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7918\n",
      "Train Epoch:7919 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "save model\n",
      "Save model 7919\n",
      "Train Epoch:7920 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7920\n",
      "Train Epoch:7921 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7921\n",
      "Train Epoch:7922 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7922\n",
      "Train Epoch:7923 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "save model\n",
      "Save model 7923\n",
      "Train Epoch:7924 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7924\n",
      "Train Epoch:7925 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7925\n",
      "Train Epoch:7926 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7926\n",
      "Train Epoch:7927 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "save model\n",
      "Save model 7927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:7928 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7928\n",
      "Train Epoch:7929 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7929\n",
      "Train Epoch:7930 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "save model\n",
      "Save model 7930\n",
      "Train Epoch:7931 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7931\n",
      "Train Epoch:7932 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7932\n",
      "Train Epoch:7933 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "save model\n",
      "Save model 7933\n",
      "Train Epoch:7934 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7934\n",
      "Train Epoch:7935 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7935\n",
      "Train Epoch:7936 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7936\n",
      "Train Epoch:7937 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7937\n",
      "Train Epoch:7938 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "save model\n",
      "Save model 7938\n",
      "Train Epoch:7939 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7939\n",
      "Train Epoch:7940 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "save model\n",
      "Save model 7940\n",
      "Train Epoch:7941 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "save model\n",
      "Save model 7941\n",
      "Train Epoch:7942 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7942\n",
      "Train Epoch:7943 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "save model\n",
      "Save model 7943\n",
      "Train Epoch:7944 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7944\n",
      "Train Epoch:7945 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7945\n",
      "Train Epoch:7946 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7946\n",
      "Train Epoch:7947 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "save model\n",
      "Save model 7947\n",
      "Train Epoch:7948 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7948\n",
      "Train Epoch:7949 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7949\n",
      "Train Epoch:7950 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7950\n",
      "Train Epoch:7951 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "save model\n",
      "Save model 7951\n",
      "Train Epoch:7952 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7952\n",
      "Train Epoch:7953 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7953\n",
      "Train Epoch:7954 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7954\n",
      "Train Epoch:7955 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 7955\n",
      "Train Epoch:7956 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7956\n",
      "Train Epoch:7957 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7957\n",
      "Train Epoch:7958 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7958\n",
      "Train Epoch:7959 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7959\n",
      "Train Epoch:7960 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7960\n",
      "Train Epoch:7961 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "save model\n",
      "Save model 7961\n",
      "Train Epoch:7962 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7962\n",
      "Train Epoch:7963 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7963\n",
      "Train Epoch:7964 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 7964\n",
      "Train Epoch:7965 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7965\n",
      "Train Epoch:7966 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "save model\n",
      "Save model 7966\n",
      "Train Epoch:7967 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "save model\n",
      "Save model 7967\n",
      "Train Epoch:7968 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7968\n",
      "Train Epoch:7969 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "save model\n",
      "Save model 7969\n",
      "Train Epoch:7970 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "save model\n",
      "Save model 7970\n",
      "Train Epoch:7971 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7971\n",
      "Train Epoch:7972 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7972\n",
      "Train Epoch:7973 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "save model\n",
      "Save model 7973\n",
      "Train Epoch:7974 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7974\n",
      "Train Epoch:7975 learning rate:1.0000e-05, Loss_tot:0.0900,\n",
      "Save model 7975\n",
      "Train Epoch:7976 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7976\n",
      "Train Epoch:7977 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "save model\n",
      "Save model 7977\n",
      "Train Epoch:7978 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7978\n",
      "Train Epoch:7979 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7979\n",
      "Train Epoch:7980 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "save model\n",
      "Save model 7980\n",
      "Train Epoch:7981 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7981\n",
      "Train Epoch:7982 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7982\n",
      "Train Epoch:7983 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7983\n",
      "Train Epoch:7984 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 7984\n",
      "Train Epoch:7985 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7985\n",
      "Train Epoch:7986 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7986\n",
      "Train Epoch:7987 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "save model\n",
      "Save model 7987\n",
      "Train Epoch:7988 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 7988\n",
      "Train Epoch:7989 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 7989\n",
      "Train Epoch:7990 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "save model\n",
      "Save model 7990\n",
      "Train Epoch:7991 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 7991\n",
      "Train Epoch:7992 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7992\n",
      "Train Epoch:7993 learning rate:1.0000e-05, Loss_tot:0.0899,\n",
      "Save model 7993\n",
      "Train Epoch:7994 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 7994\n",
      "Train Epoch:7995 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 7995\n",
      "Train Epoch:7996 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 7996\n",
      "Train Epoch:7997 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 7997\n",
      "Train Epoch:7998 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 7998\n",
      "Train Epoch:7999 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 7999\n",
      "Train Epoch:8000 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 8000\n",
      "Train Epoch:8001 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8001\n",
      "Train Epoch:8002 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8002\n",
      "Train Epoch:8003 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 8003\n",
      "Train Epoch:8004 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8004\n",
      "Train Epoch:8005 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8005\n",
      "Train Epoch:8006 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8006\n",
      "Train Epoch:8007 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8007\n",
      "Train Epoch:8008 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8008\n",
      "Train Epoch:8009 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 8009\n",
      "Train Epoch:8010 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "save model\n",
      "Save model 8010\n",
      "Train Epoch:8011 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8011\n",
      "Train Epoch:8012 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8012\n",
      "Train Epoch:8013 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "save model\n",
      "Save model 8013\n",
      "Train Epoch:8014 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8014\n",
      "Train Epoch:8015 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8015\n",
      "Train Epoch:8016 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "save model\n",
      "Save model 8016\n",
      "Train Epoch:8017 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8017\n",
      "Train Epoch:8018 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8018\n",
      "Train Epoch:8019 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "save model\n",
      "Save model 8019\n",
      "Train Epoch:8020 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8020\n",
      "Train Epoch:8021 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8021\n",
      "Train Epoch:8022 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8022\n",
      "Train Epoch:8023 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8023\n",
      "Train Epoch:8024 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8024\n",
      "Train Epoch:8025 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 8025\n",
      "Train Epoch:8026 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8026\n",
      "Train Epoch:8027 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "save model\n",
      "Save model 8027\n",
      "Train Epoch:8028 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8028\n",
      "Train Epoch:8029 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8029\n",
      "Train Epoch:8030 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "save model\n",
      "Save model 8030\n",
      "Train Epoch:8031 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8032 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8032\n",
      "Train Epoch:8033 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8033\n",
      "Train Epoch:8034 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8034\n",
      "Train Epoch:8035 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8035\n",
      "Train Epoch:8036 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8036\n",
      "Train Epoch:8037 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8037\n",
      "Train Epoch:8038 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8038\n",
      "Train Epoch:8039 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8039\n",
      "Train Epoch:8040 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8040\n",
      "Train Epoch:8041 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8041\n",
      "Train Epoch:8042 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8042\n",
      "Train Epoch:8043 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8043\n",
      "Train Epoch:8044 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8044\n",
      "Train Epoch:8045 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8045\n",
      "Train Epoch:8046 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8046\n",
      "Train Epoch:8047 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "save model\n",
      "Save model 8047\n",
      "Train Epoch:8048 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8048\n",
      "Train Epoch:8049 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 8049\n",
      "Train Epoch:8050 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8050\n",
      "Train Epoch:8051 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8051\n",
      "Train Epoch:8052 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8052\n",
      "Train Epoch:8053 learning rate:1.0000e-05, Loss_tot:0.0895,\n",
      "Save model 8053\n",
      "Train Epoch:8054 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8054\n",
      "Train Epoch:8055 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8055\n",
      "Train Epoch:8056 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8056\n",
      "Train Epoch:8057 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8057\n",
      "Train Epoch:8058 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8058\n",
      "Train Epoch:8059 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8059\n",
      "Train Epoch:8060 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8060\n",
      "Train Epoch:8061 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8061\n",
      "Train Epoch:8062 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8062\n",
      "Train Epoch:8063 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8063\n",
      "Train Epoch:8064 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8064\n",
      "Train Epoch:8065 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8065\n",
      "Train Epoch:8066 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8066\n",
      "Train Epoch:8067 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8067\n",
      "Train Epoch:8068 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8068\n",
      "Train Epoch:8069 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8069\n",
      "Train Epoch:8070 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "save model\n",
      "Save model 8070\n",
      "Train Epoch:8071 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8071\n",
      "Train Epoch:8072 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8072\n",
      "Train Epoch:8073 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8073\n",
      "Train Epoch:8074 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8074\n",
      "Train Epoch:8075 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8075\n",
      "Train Epoch:8076 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8076\n",
      "Train Epoch:8077 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8077\n",
      "Train Epoch:8078 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8078\n",
      "Train Epoch:8079 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8079\n",
      "Train Epoch:8080 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8080\n",
      "Train Epoch:8081 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8081\n",
      "Train Epoch:8082 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8082\n",
      "Train Epoch:8083 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8083\n",
      "Train Epoch:8084 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8084\n",
      "Train Epoch:8085 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8085\n",
      "Train Epoch:8086 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "save model\n",
      "Save model 8086\n",
      "Train Epoch:8087 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8087\n",
      "Train Epoch:8088 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8088\n",
      "Train Epoch:8089 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8089\n",
      "Train Epoch:8090 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8090\n",
      "Train Epoch:8091 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8091\n",
      "Train Epoch:8092 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "save model\n",
      "Save model 8092\n",
      "Train Epoch:8093 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8093\n",
      "Train Epoch:8094 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8094\n",
      "Train Epoch:8095 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8095\n",
      "Train Epoch:8096 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8096\n",
      "Train Epoch:8097 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8097\n",
      "Train Epoch:8098 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8098\n",
      "Train Epoch:8099 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8099\n",
      "Train Epoch:8100 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8100\n",
      "Train Epoch:8101 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "save model\n",
      "Save model 8101\n",
      "Train Epoch:8102 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8102\n",
      "Train Epoch:8103 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8103\n",
      "Train Epoch:8104 learning rate:1.0000e-05, Loss_tot:0.0894,\n",
      "Save model 8104\n",
      "Train Epoch:8105 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8105\n",
      "Train Epoch:8106 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 8106\n",
      "Train Epoch:8107 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "save model\n",
      "Save model 8107\n",
      "Train Epoch:8108 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8108\n",
      "Train Epoch:8109 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "save model\n",
      "Save model 8109\n",
      "Train Epoch:8110 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8110\n",
      "Train Epoch:8111 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8111\n",
      "Train Epoch:8112 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8112\n",
      "Train Epoch:8113 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8113\n",
      "Train Epoch:8114 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8114\n",
      "Train Epoch:8115 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8115\n",
      "Train Epoch:8116 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8116\n",
      "Train Epoch:8117 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "save model\n",
      "Save model 8117\n",
      "Train Epoch:8118 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8118\n",
      "Train Epoch:8119 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8119\n",
      "Train Epoch:8120 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "save model\n",
      "Save model 8120\n",
      "Train Epoch:8121 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8121\n",
      "Train Epoch:8122 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8122\n",
      "Train Epoch:8123 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "save model\n",
      "Save model 8123\n",
      "Train Epoch:8124 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8124\n",
      "Train Epoch:8125 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8125\n",
      "Train Epoch:8126 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8126\n",
      "Train Epoch:8127 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "save model\n",
      "Save model 8127\n",
      "Train Epoch:8128 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "save model\n",
      "Save model 8128\n",
      "Train Epoch:8129 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8129\n",
      "Train Epoch:8130 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8130\n",
      "Train Epoch:8131 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8131\n",
      "Train Epoch:8132 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8132\n",
      "Train Epoch:8133 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8133\n",
      "Train Epoch:8134 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8134\n",
      "Train Epoch:8135 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8136 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8136\n",
      "Train Epoch:8137 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8137\n",
      "Train Epoch:8138 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 8138\n",
      "Train Epoch:8139 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8139\n",
      "Train Epoch:8140 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8140\n",
      "Train Epoch:8141 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8141\n",
      "Train Epoch:8142 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8142\n",
      "Train Epoch:8143 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8143\n",
      "Train Epoch:8144 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8144\n",
      "Train Epoch:8145 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8145\n",
      "Train Epoch:8146 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8146\n",
      "Train Epoch:8147 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 8147\n",
      "Train Epoch:8148 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8148\n",
      "Train Epoch:8149 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8149\n",
      "Train Epoch:8150 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8150\n",
      "Train Epoch:8151 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8151\n",
      "Train Epoch:8152 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "save model\n",
      "Save model 8152\n",
      "Train Epoch:8153 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8153\n",
      "Train Epoch:8154 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8154\n",
      "Train Epoch:8155 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8155\n",
      "Train Epoch:8156 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8156\n",
      "Train Epoch:8157 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8157\n",
      "Train Epoch:8158 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8158\n",
      "Train Epoch:8159 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8159\n",
      "Train Epoch:8160 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8160\n",
      "Train Epoch:8161 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8161\n",
      "Train Epoch:8162 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8162\n",
      "Train Epoch:8163 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8163\n",
      "Train Epoch:8164 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 8164\n",
      "Train Epoch:8165 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8165\n",
      "Train Epoch:8166 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8166\n",
      "Train Epoch:8167 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8167\n",
      "Train Epoch:8168 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8168\n",
      "Train Epoch:8169 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8169\n",
      "Train Epoch:8170 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8170\n",
      "Train Epoch:8171 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8171\n",
      "Train Epoch:8172 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8172\n",
      "Train Epoch:8173 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8173\n",
      "Train Epoch:8174 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "save model\n",
      "Save model 8174\n",
      "Train Epoch:8175 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8175\n",
      "Train Epoch:8176 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8176\n",
      "Train Epoch:8177 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8177\n",
      "Train Epoch:8178 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8178\n",
      "Train Epoch:8179 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8179\n",
      "Train Epoch:8180 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8180\n",
      "Train Epoch:8181 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8181\n",
      "Train Epoch:8182 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8182\n",
      "Train Epoch:8183 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8183\n",
      "Train Epoch:8184 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8184\n",
      "Train Epoch:8185 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8185\n",
      "Train Epoch:8186 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8186\n",
      "Train Epoch:8187 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8187\n",
      "Train Epoch:8188 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8188\n",
      "Train Epoch:8189 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8189\n",
      "Train Epoch:8190 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "save model\n",
      "Save model 8190\n",
      "Train Epoch:8191 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8191\n",
      "Train Epoch:8192 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8192\n",
      "Train Epoch:8193 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8193\n",
      "Train Epoch:8194 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8194\n",
      "Train Epoch:8195 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8195\n",
      "Train Epoch:8196 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "save model\n",
      "Save model 8196\n",
      "Train Epoch:8197 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8197\n",
      "Train Epoch:8198 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8198\n",
      "Train Epoch:8199 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "save model\n",
      "Save model 8199\n",
      "Train Epoch:8200 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8200\n",
      "Train Epoch:8201 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8201\n",
      "Train Epoch:8202 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8202\n",
      "Train Epoch:8203 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8203\n",
      "Train Epoch:8204 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8204\n",
      "Train Epoch:8205 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8205\n",
      "Train Epoch:8206 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8206\n",
      "Train Epoch:8207 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "save model\n",
      "Save model 8207\n",
      "Train Epoch:8208 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8208\n",
      "Train Epoch:8209 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8209\n",
      "Train Epoch:8210 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "save model\n",
      "Save model 8210\n",
      "Train Epoch:8211 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8211\n",
      "Train Epoch:8212 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8212\n",
      "Train Epoch:8213 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8213\n",
      "Train Epoch:8214 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8214\n",
      "Train Epoch:8215 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8215\n",
      "Train Epoch:8216 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8216\n",
      "Train Epoch:8217 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "save model\n",
      "Save model 8217\n",
      "Train Epoch:8218 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8218\n",
      "Train Epoch:8219 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8219\n",
      "Train Epoch:8220 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8220\n",
      "Train Epoch:8221 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8221\n",
      "Train Epoch:8222 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8222\n",
      "Train Epoch:8223 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8223\n",
      "Train Epoch:8224 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "save model\n",
      "Save model 8224\n",
      "Train Epoch:8225 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "save model\n",
      "Save model 8225\n",
      "Train Epoch:8226 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8226\n",
      "Train Epoch:8227 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "save model\n",
      "Save model 8227\n",
      "Train Epoch:8228 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8228\n",
      "Train Epoch:8229 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8229\n",
      "Train Epoch:8230 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8230\n",
      "Train Epoch:8231 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8231\n",
      "Train Epoch:8232 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "save model\n",
      "Save model 8232\n",
      "Train Epoch:8233 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8233\n",
      "Train Epoch:8234 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 8234\n",
      "Train Epoch:8235 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8235\n",
      "Train Epoch:8236 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8236\n",
      "Train Epoch:8237 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8237\n",
      "Train Epoch:8238 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8238\n",
      "Train Epoch:8239 learning rate:1.0000e-05, Loss_tot:0.0887,\n",
      "Save model 8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8240 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8240\n",
      "Train Epoch:8241 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "save model\n",
      "Save model 8241\n",
      "Train Epoch:8242 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8242\n",
      "Train Epoch:8243 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8243\n",
      "Train Epoch:8244 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8244\n",
      "Train Epoch:8245 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "save model\n",
      "Save model 8245\n",
      "Train Epoch:8246 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8246\n",
      "Train Epoch:8247 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8247\n",
      "Train Epoch:8248 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "save model\n",
      "Save model 8248\n",
      "Train Epoch:8249 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "save model\n",
      "Save model 8249\n",
      "Train Epoch:8250 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8250\n",
      "Train Epoch:8251 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8251\n",
      "Train Epoch:8252 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8252\n",
      "Train Epoch:8253 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "save model\n",
      "Save model 8253\n",
      "Train Epoch:8254 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "save model\n",
      "Save model 8254\n",
      "Train Epoch:8255 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8255\n",
      "Train Epoch:8256 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8256\n",
      "Train Epoch:8257 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "save model\n",
      "Save model 8257\n",
      "Train Epoch:8258 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8258\n",
      "Train Epoch:8259 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8259\n",
      "Train Epoch:8260 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8260\n",
      "Train Epoch:8261 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8261\n",
      "Train Epoch:8262 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8262\n",
      "Train Epoch:8263 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8263\n",
      "Train Epoch:8264 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8264\n",
      "Train Epoch:8265 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "save model\n",
      "Save model 8265\n",
      "Train Epoch:8266 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8266\n",
      "Train Epoch:8267 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8267\n",
      "Train Epoch:8268 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8268\n",
      "Train Epoch:8269 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8269\n",
      "Train Epoch:8270 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "save model\n",
      "Save model 8270\n",
      "Train Epoch:8271 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8271\n",
      "Train Epoch:8272 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8272\n",
      "Train Epoch:8273 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8273\n",
      "Train Epoch:8274 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "save model\n",
      "Save model 8274\n",
      "Train Epoch:8275 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8275\n",
      "Train Epoch:8276 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8276\n",
      "Train Epoch:8277 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8277\n",
      "Train Epoch:8278 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8278\n",
      "Train Epoch:8279 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8279\n",
      "Train Epoch:8280 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8280\n",
      "Train Epoch:8281 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8281\n",
      "Train Epoch:8282 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8282\n",
      "Train Epoch:8283 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8283\n",
      "Train Epoch:8284 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "save model\n",
      "Save model 8284\n",
      "Train Epoch:8285 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8285\n",
      "Train Epoch:8286 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8286\n",
      "Train Epoch:8287 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8287\n",
      "Train Epoch:8288 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8288\n",
      "Train Epoch:8289 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "save model\n",
      "Save model 8289\n",
      "Train Epoch:8290 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8290\n",
      "Train Epoch:8291 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8291\n",
      "Train Epoch:8292 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8292\n",
      "Train Epoch:8293 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 8293\n",
      "Train Epoch:8294 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8294\n",
      "Train Epoch:8295 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8295\n",
      "Train Epoch:8296 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8296\n",
      "Train Epoch:8297 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8297\n",
      "Train Epoch:8298 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8298\n",
      "Train Epoch:8299 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8299\n",
      "Train Epoch:8300 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8300\n",
      "Train Epoch:8301 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "save model\n",
      "Save model 8301\n",
      "Train Epoch:8302 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8302\n",
      "Train Epoch:8303 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8303\n",
      "Train Epoch:8304 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8304\n",
      "Train Epoch:8305 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 8305\n",
      "Train Epoch:8306 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 8306\n",
      "Train Epoch:8307 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8307\n",
      "Train Epoch:8308 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "save model\n",
      "Save model 8308\n",
      "Train Epoch:8309 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8309\n",
      "Train Epoch:8310 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8310\n",
      "Train Epoch:8311 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "save model\n",
      "Save model 8311\n",
      "Train Epoch:8312 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8312\n",
      "Train Epoch:8313 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "save model\n",
      "Save model 8313\n",
      "Train Epoch:8314 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8314\n",
      "Train Epoch:8315 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8315\n",
      "Train Epoch:8316 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8316\n",
      "Train Epoch:8317 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8317\n",
      "Train Epoch:8318 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8318\n",
      "Train Epoch:8319 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8319\n",
      "Train Epoch:8320 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8320\n",
      "Train Epoch:8321 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8321\n",
      "Train Epoch:8322 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8322\n",
      "Train Epoch:8323 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8323\n",
      "Train Epoch:8324 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8324\n",
      "Train Epoch:8325 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "save model\n",
      "Save model 8325\n",
      "Train Epoch:8326 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8326\n",
      "Train Epoch:8327 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8327\n",
      "Train Epoch:8328 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8328\n",
      "Train Epoch:8329 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8329\n",
      "Train Epoch:8330 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8330\n",
      "Train Epoch:8331 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "save model\n",
      "Save model 8331\n",
      "Train Epoch:8332 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8332\n",
      "Train Epoch:8333 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8333\n",
      "Train Epoch:8334 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8334\n",
      "Train Epoch:8335 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8335\n",
      "Train Epoch:8336 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "save model\n",
      "Save model 8336\n",
      "Train Epoch:8337 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8337\n",
      "Train Epoch:8338 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8338\n",
      "Train Epoch:8339 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8339\n",
      "Train Epoch:8340 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8340\n",
      "Train Epoch:8341 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8341\n",
      "Train Epoch:8342 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8342\n",
      "Train Epoch:8343 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "save model\n",
      "Save model 8343\n",
      "Train Epoch:8344 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8345 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8345\n",
      "Train Epoch:8346 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 8346\n",
      "Train Epoch:8347 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8347\n",
      "Train Epoch:8348 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8348\n",
      "Train Epoch:8349 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8349\n",
      "Train Epoch:8350 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8350\n",
      "Train Epoch:8351 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8351\n",
      "Train Epoch:8352 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8352\n",
      "Train Epoch:8353 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8353\n",
      "Train Epoch:8354 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "save model\n",
      "Save model 8354\n",
      "Train Epoch:8355 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "save model\n",
      "Save model 8355\n",
      "Train Epoch:8356 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8356\n",
      "Train Epoch:8357 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8357\n",
      "Train Epoch:8358 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "save model\n",
      "Save model 8358\n",
      "Train Epoch:8359 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8359\n",
      "Train Epoch:8360 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8360\n",
      "Train Epoch:8361 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8361\n",
      "Train Epoch:8362 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8362\n",
      "Train Epoch:8363 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8363\n",
      "Train Epoch:8364 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "save model\n",
      "Save model 8364\n",
      "Train Epoch:8365 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8365\n",
      "Train Epoch:8366 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8366\n",
      "Train Epoch:8367 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8367\n",
      "Train Epoch:8368 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8368\n",
      "Train Epoch:8369 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8369\n",
      "Train Epoch:8370 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8370\n",
      "Train Epoch:8371 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8371\n",
      "Train Epoch:8372 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 8372\n",
      "Train Epoch:8373 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 8373\n",
      "Train Epoch:8374 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8374\n",
      "Train Epoch:8375 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8375\n",
      "Train Epoch:8376 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8376\n",
      "Train Epoch:8377 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8377\n",
      "Train Epoch:8378 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8378\n",
      "Train Epoch:8379 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8379\n",
      "Train Epoch:8380 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8380\n",
      "Train Epoch:8381 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8381\n",
      "Train Epoch:8382 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8382\n",
      "Train Epoch:8383 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8383\n",
      "Train Epoch:8384 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "save model\n",
      "Save model 8384\n",
      "Train Epoch:8385 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 8385\n",
      "Train Epoch:8386 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8386\n",
      "Train Epoch:8387 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8387\n",
      "Train Epoch:8388 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8388\n",
      "Train Epoch:8389 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8389\n",
      "Train Epoch:8390 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8390\n",
      "Train Epoch:8391 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8391\n",
      "Train Epoch:8392 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8392\n",
      "Train Epoch:8393 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8393\n",
      "Train Epoch:8394 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8394\n",
      "Train Epoch:8395 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8395\n",
      "Train Epoch:8396 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8396\n",
      "Train Epoch:8397 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8397\n",
      "Train Epoch:8398 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8398\n",
      "Train Epoch:8399 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8399\n",
      "Train Epoch:8400 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8400\n",
      "Train Epoch:8401 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8401\n",
      "Train Epoch:8402 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8402\n",
      "Train Epoch:8403 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8403\n",
      "Train Epoch:8404 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8404\n",
      "Train Epoch:8405 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8405\n",
      "Train Epoch:8406 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "save model\n",
      "Save model 8406\n",
      "Train Epoch:8407 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8407\n",
      "Train Epoch:8408 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8408\n",
      "Train Epoch:8409 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8409\n",
      "Train Epoch:8410 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8410\n",
      "Train Epoch:8411 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8411\n",
      "Train Epoch:8412 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8412\n",
      "Train Epoch:8413 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8413\n",
      "Train Epoch:8414 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8414\n",
      "Train Epoch:8415 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8415\n",
      "Train Epoch:8416 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8416\n",
      "Train Epoch:8417 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8417\n",
      "Train Epoch:8418 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8418\n",
      "Train Epoch:8419 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8419\n",
      "Train Epoch:8420 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8420\n",
      "Train Epoch:8421 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8421\n",
      "Train Epoch:8422 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8422\n",
      "Train Epoch:8423 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "save model\n",
      "Save model 8423\n",
      "Train Epoch:8424 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8424\n",
      "Train Epoch:8425 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8425\n",
      "Train Epoch:8426 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8426\n",
      "Train Epoch:8427 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8427\n",
      "Train Epoch:8428 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "save model\n",
      "Save model 8428\n",
      "Train Epoch:8429 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8429\n",
      "Train Epoch:8430 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8430\n",
      "Train Epoch:8431 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8431\n",
      "Train Epoch:8432 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8432\n",
      "Train Epoch:8433 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8433\n",
      "Train Epoch:8434 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8434\n",
      "Train Epoch:8435 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8435\n",
      "Train Epoch:8436 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8436\n",
      "Train Epoch:8437 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8437\n",
      "Train Epoch:8438 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "save model\n",
      "Save model 8438\n",
      "Train Epoch:8439 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8439\n",
      "Train Epoch:8440 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "save model\n",
      "Save model 8440\n",
      "Train Epoch:8441 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "save model\n",
      "Save model 8441\n",
      "Train Epoch:8442 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8442\n",
      "Train Epoch:8443 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8443\n",
      "Train Epoch:8444 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8444\n",
      "Train Epoch:8445 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8445\n",
      "Train Epoch:8446 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 8446\n",
      "Train Epoch:8447 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8447\n",
      "Train Epoch:8448 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8449 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8449\n",
      "Train Epoch:8450 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8450\n",
      "Train Epoch:8451 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8451\n",
      "Train Epoch:8452 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8452\n",
      "Train Epoch:8453 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8453\n",
      "Train Epoch:8454 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8454\n",
      "Train Epoch:8455 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8455\n",
      "Train Epoch:8456 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8456\n",
      "Train Epoch:8457 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8457\n",
      "Train Epoch:8458 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8458\n",
      "Train Epoch:8459 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8459\n",
      "Train Epoch:8460 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8460\n",
      "Train Epoch:8461 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8461\n",
      "Train Epoch:8462 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8462\n",
      "Train Epoch:8463 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "save model\n",
      "Save model 8463\n",
      "Train Epoch:8464 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8464\n",
      "Train Epoch:8465 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8465\n",
      "Train Epoch:8466 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8466\n",
      "Train Epoch:8467 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8467\n",
      "Train Epoch:8468 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8468\n",
      "Train Epoch:8469 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8469\n",
      "Train Epoch:8470 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8470\n",
      "Train Epoch:8471 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 8471\n",
      "Train Epoch:8472 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8472\n",
      "Train Epoch:8473 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8473\n",
      "Train Epoch:8474 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8474\n",
      "Train Epoch:8475 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8475\n",
      "Train Epoch:8476 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8476\n",
      "Train Epoch:8477 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8477\n",
      "Train Epoch:8478 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8478\n",
      "Train Epoch:8479 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8479\n",
      "Train Epoch:8480 learning rate:1.0000e-05, Loss_tot:0.0875,\n",
      "Save model 8480\n",
      "Train Epoch:8481 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8481\n",
      "Train Epoch:8482 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8482\n",
      "Train Epoch:8483 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8483\n",
      "Train Epoch:8484 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8484\n",
      "Train Epoch:8485 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8485\n",
      "Train Epoch:8486 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8486\n",
      "Train Epoch:8487 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8487\n",
      "Train Epoch:8488 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8488\n",
      "Train Epoch:8489 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "save model\n",
      "Save model 8489\n",
      "Train Epoch:8490 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8490\n",
      "Train Epoch:8491 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8491\n",
      "Train Epoch:8492 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8492\n",
      "Train Epoch:8493 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8493\n",
      "Train Epoch:8494 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8494\n",
      "Train Epoch:8495 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8495\n",
      "Train Epoch:8496 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8496\n",
      "Train Epoch:8497 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8497\n",
      "Train Epoch:8498 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8498\n",
      "Train Epoch:8499 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8499\n",
      "Train Epoch:8500 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8500\n",
      "Train Epoch:8501 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8501\n",
      "Train Epoch:8502 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8502\n",
      "Train Epoch:8503 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8503\n",
      "Train Epoch:8504 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8504\n",
      "Train Epoch:8505 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8505\n",
      "Train Epoch:8506 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8506\n",
      "Train Epoch:8507 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8507\n",
      "Train Epoch:8508 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8508\n",
      "Train Epoch:8509 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8509\n",
      "Train Epoch:8510 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8510\n",
      "Train Epoch:8511 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8511\n",
      "Train Epoch:8512 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "save model\n",
      "Save model 8512\n",
      "Train Epoch:8513 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8513\n",
      "Train Epoch:8514 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8514\n",
      "Train Epoch:8515 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8515\n",
      "Train Epoch:8516 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8516\n",
      "Train Epoch:8517 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8517\n",
      "Train Epoch:8518 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8518\n",
      "Train Epoch:8519 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8519\n",
      "Train Epoch:8520 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8520\n",
      "Train Epoch:8521 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8521\n",
      "Train Epoch:8522 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8522\n",
      "Train Epoch:8523 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8523\n",
      "Train Epoch:8524 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8524\n",
      "Train Epoch:8525 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8525\n",
      "Train Epoch:8526 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8526\n",
      "Train Epoch:8527 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8527\n",
      "Train Epoch:8528 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8528\n",
      "Train Epoch:8529 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8529\n",
      "Train Epoch:8530 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8530\n",
      "Train Epoch:8531 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8531\n",
      "Train Epoch:8532 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8532\n",
      "Train Epoch:8533 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8533\n",
      "Train Epoch:8534 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8534\n",
      "Train Epoch:8535 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "save model\n",
      "Save model 8535\n",
      "Train Epoch:8536 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8536\n",
      "Train Epoch:8537 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8537\n",
      "Train Epoch:8538 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8538\n",
      "Train Epoch:8539 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8539\n",
      "Train Epoch:8540 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8540\n",
      "Train Epoch:8541 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8541\n",
      "Train Epoch:8542 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8542\n",
      "Train Epoch:8543 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8543\n",
      "Train Epoch:8544 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8544\n",
      "Train Epoch:8545 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8545\n",
      "Train Epoch:8546 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8546\n",
      "Train Epoch:8547 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8547\n",
      "Train Epoch:8548 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8548\n",
      "Train Epoch:8549 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8549\n",
      "Train Epoch:8550 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8550\n",
      "Train Epoch:8551 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8551\n",
      "Train Epoch:8552 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8553 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8553\n",
      "Train Epoch:8554 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8554\n",
      "Train Epoch:8555 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8555\n",
      "Train Epoch:8556 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8556\n",
      "Train Epoch:8557 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8557\n",
      "Train Epoch:8558 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8558\n",
      "Train Epoch:8559 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8559\n",
      "Train Epoch:8560 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "save model\n",
      "Save model 8560\n",
      "Train Epoch:8561 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8561\n",
      "Train Epoch:8562 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8562\n",
      "Train Epoch:8563 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8563\n",
      "Train Epoch:8564 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8564\n",
      "Train Epoch:8565 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8565\n",
      "Train Epoch:8566 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8566\n",
      "Train Epoch:8567 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8567\n",
      "Train Epoch:8568 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8568\n",
      "Train Epoch:8569 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8569\n",
      "Train Epoch:8570 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8570\n",
      "Train Epoch:8571 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8571\n",
      "Train Epoch:8572 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8572\n",
      "Train Epoch:8573 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8573\n",
      "Train Epoch:8574 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8574\n",
      "Train Epoch:8575 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8575\n",
      "Train Epoch:8576 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8576\n",
      "Train Epoch:8577 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8577\n",
      "Train Epoch:8578 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8578\n",
      "Train Epoch:8579 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "save model\n",
      "Save model 8579\n",
      "Train Epoch:8580 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8580\n",
      "Train Epoch:8581 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8581\n",
      "Train Epoch:8582 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8582\n",
      "Train Epoch:8583 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8583\n",
      "Train Epoch:8584 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8584\n",
      "Train Epoch:8585 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8585\n",
      "Train Epoch:8586 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8586\n",
      "Train Epoch:8587 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8587\n",
      "Train Epoch:8588 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8588\n",
      "Train Epoch:8589 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8589\n",
      "Train Epoch:8590 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8590\n",
      "Train Epoch:8591 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8591\n",
      "Train Epoch:8592 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8592\n",
      "Train Epoch:8593 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8593\n",
      "Train Epoch:8594 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8594\n",
      "Train Epoch:8595 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8595\n",
      "Train Epoch:8596 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8596\n",
      "Train Epoch:8597 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8597\n",
      "Train Epoch:8598 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8598\n",
      "Train Epoch:8599 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8599\n",
      "Train Epoch:8600 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8600\n",
      "Train Epoch:8601 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8601\n",
      "Train Epoch:8602 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8602\n",
      "Train Epoch:8603 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8603\n",
      "Train Epoch:8604 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8604\n",
      "Train Epoch:8605 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "save model\n",
      "Save model 8605\n",
      "Train Epoch:8606 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8606\n",
      "Train Epoch:8607 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8607\n",
      "Train Epoch:8608 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8608\n",
      "Train Epoch:8609 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8609\n",
      "Train Epoch:8610 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8610\n",
      "Train Epoch:8611 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8611\n",
      "Train Epoch:8612 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8612\n",
      "Train Epoch:8613 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8613\n",
      "Train Epoch:8614 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8614\n",
      "Train Epoch:8615 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8615\n",
      "Train Epoch:8616 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8616\n",
      "Train Epoch:8617 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8617\n",
      "Train Epoch:8618 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8618\n",
      "Train Epoch:8619 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8619\n",
      "Train Epoch:8620 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8620\n",
      "Train Epoch:8621 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8621\n",
      "Train Epoch:8622 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8622\n",
      "Train Epoch:8623 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8623\n",
      "Train Epoch:8624 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8624\n",
      "Train Epoch:8625 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8625\n",
      "Train Epoch:8626 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "save model\n",
      "Save model 8626\n",
      "Train Epoch:8627 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8627\n",
      "Train Epoch:8628 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "save model\n",
      "Save model 8628\n",
      "Train Epoch:8629 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8629\n",
      "Train Epoch:8630 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8630\n",
      "Train Epoch:8631 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8631\n",
      "Train Epoch:8632 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8632\n",
      "Train Epoch:8633 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8633\n",
      "Train Epoch:8634 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8634\n",
      "Train Epoch:8635 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8635\n",
      "Train Epoch:8636 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8636\n",
      "Train Epoch:8637 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8637\n",
      "Train Epoch:8638 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "save model\n",
      "Save model 8638\n",
      "Train Epoch:8639 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "save model\n",
      "Save model 8639\n",
      "Train Epoch:8640 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8640\n",
      "Train Epoch:8641 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8641\n",
      "Train Epoch:8642 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8642\n",
      "Train Epoch:8643 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8643\n",
      "Train Epoch:8644 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8644\n",
      "Train Epoch:8645 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8645\n",
      "Train Epoch:8646 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8646\n",
      "Train Epoch:8647 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8647\n",
      "Train Epoch:8648 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8648\n",
      "Train Epoch:8649 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8649\n",
      "Train Epoch:8650 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8650\n",
      "Train Epoch:8651 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8651\n",
      "Train Epoch:8652 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8652\n",
      "Train Epoch:8653 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8653\n",
      "Train Epoch:8654 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8654\n",
      "Train Epoch:8655 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8655\n",
      "Train Epoch:8656 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8657 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8657\n",
      "Train Epoch:8658 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8658\n",
      "Train Epoch:8659 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8659\n",
      "Train Epoch:8660 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8660\n",
      "Train Epoch:8661 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8661\n",
      "Train Epoch:8662 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8662\n",
      "Train Epoch:8663 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8663\n",
      "Train Epoch:8664 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8664\n",
      "Train Epoch:8665 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8665\n",
      "Train Epoch:8666 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8666\n",
      "Train Epoch:8667 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8667\n",
      "Train Epoch:8668 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8668\n",
      "Train Epoch:8669 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8669\n",
      "Train Epoch:8670 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8670\n",
      "Train Epoch:8671 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "save model\n",
      "Save model 8671\n",
      "Train Epoch:8672 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8672\n",
      "Train Epoch:8673 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8673\n",
      "Train Epoch:8674 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8674\n",
      "Train Epoch:8675 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8675\n",
      "Train Epoch:8676 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8676\n",
      "Train Epoch:8677 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8677\n",
      "Train Epoch:8678 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8678\n",
      "Train Epoch:8679 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8679\n",
      "Train Epoch:8680 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8680\n",
      "Train Epoch:8681 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8681\n",
      "Train Epoch:8682 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8682\n",
      "Train Epoch:8683 learning rate:1.0000e-05, Loss_tot:0.1622,\n",
      "Save model 8683\n",
      "Train Epoch:8684 learning rate:1.0000e-05, Loss_tot:0.1616,\n",
      "Save model 8684\n",
      "Train Epoch:8685 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "Save model 8685\n",
      "Train Epoch:8686 learning rate:1.0000e-05, Loss_tot:0.1597,\n",
      "Save model 8686\n",
      "Train Epoch:8687 learning rate:1.0000e-05, Loss_tot:0.1583,\n",
      "Save model 8687\n",
      "Train Epoch:8688 learning rate:1.0000e-05, Loss_tot:0.1568,\n",
      "Save model 8688\n",
      "Train Epoch:8689 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "Save model 8689\n",
      "Train Epoch:8690 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "Save model 8690\n",
      "Train Epoch:8691 learning rate:1.0000e-05, Loss_tot:0.1516,\n",
      "Save model 8691\n",
      "Train Epoch:8692 learning rate:1.0000e-05, Loss_tot:0.1498,\n",
      "Save model 8692\n",
      "Train Epoch:8693 learning rate:1.0000e-05, Loss_tot:0.1478,\n",
      "Save model 8693\n",
      "Train Epoch:8694 learning rate:1.0000e-05, Loss_tot:0.1458,\n",
      "Save model 8694\n",
      "Train Epoch:8695 learning rate:1.0000e-05, Loss_tot:0.1437,\n",
      "Save model 8695\n",
      "Train Epoch:8696 learning rate:1.0000e-05, Loss_tot:0.1419,\n",
      "Save model 8696\n",
      "Train Epoch:8697 learning rate:1.0000e-05, Loss_tot:0.1399,\n",
      "Save model 8697\n",
      "Train Epoch:8698 learning rate:1.0000e-05, Loss_tot:0.1379,\n",
      "Save model 8698\n",
      "Train Epoch:8699 learning rate:1.0000e-05, Loss_tot:0.1358,\n",
      "Save model 8699\n",
      "Train Epoch:8700 learning rate:1.0000e-05, Loss_tot:0.1086,\n",
      "Save model 8700\n",
      "Train Epoch:8701 learning rate:1.0000e-05, Loss_tot:0.1101,\n",
      "Save model 8701\n",
      "Train Epoch:8702 learning rate:1.0000e-05, Loss_tot:0.1112,\n",
      "Save model 8702\n",
      "Train Epoch:8703 learning rate:1.0000e-05, Loss_tot:0.1119,\n",
      "Save model 8703\n",
      "Train Epoch:8704 learning rate:1.0000e-05, Loss_tot:0.1121,\n",
      "Save model 8704\n",
      "Train Epoch:8705 learning rate:1.0000e-05, Loss_tot:0.1121,\n",
      "Save model 8705\n",
      "Train Epoch:8706 learning rate:1.0000e-05, Loss_tot:0.1118,\n",
      "Save model 8706\n",
      "Train Epoch:8707 learning rate:1.0000e-05, Loss_tot:0.1112,\n",
      "Save model 8707\n",
      "Train Epoch:8708 learning rate:1.0000e-05, Loss_tot:0.1107,\n",
      "Save model 8708\n",
      "Train Epoch:8709 learning rate:1.0000e-05, Loss_tot:0.1098,\n",
      "Save model 8709\n",
      "Train Epoch:8710 learning rate:1.0000e-05, Loss_tot:0.1087,\n",
      "Save model 8710\n",
      "Train Epoch:8711 learning rate:1.0000e-05, Loss_tot:0.1075,\n",
      "Save model 8711\n",
      "Train Epoch:8712 learning rate:1.0000e-05, Loss_tot:0.1063,\n",
      "Save model 8712\n",
      "Train Epoch:8713 learning rate:1.0000e-05, Loss_tot:0.1049,\n",
      "Save model 8713\n",
      "Train Epoch:8714 learning rate:1.0000e-05, Loss_tot:0.1035,\n",
      "Save model 8714\n",
      "Train Epoch:8715 learning rate:1.0000e-05, Loss_tot:0.1019,\n",
      "Save model 8715\n",
      "Train Epoch:8716 learning rate:1.0000e-05, Loss_tot:0.1003,\n",
      "Save model 8716\n",
      "Train Epoch:8717 learning rate:1.0000e-05, Loss_tot:0.0986,\n",
      "Save model 8717\n",
      "Train Epoch:8718 learning rate:1.0000e-05, Loss_tot:0.0968,\n",
      "Save model 8718\n",
      "Train Epoch:8719 learning rate:1.0000e-05, Loss_tot:0.0950,\n",
      "Save model 8719\n",
      "Train Epoch:8720 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 8720\n",
      "Train Epoch:8721 learning rate:1.0000e-05, Loss_tot:0.0915,\n",
      "Save model 8721\n",
      "Train Epoch:8722 learning rate:1.0000e-05, Loss_tot:0.0898,\n",
      "Save model 8722\n",
      "Train Epoch:8723 learning rate:1.0000e-05, Loss_tot:0.0889,\n",
      "Save model 8723\n",
      "Train Epoch:8724 learning rate:1.0000e-05, Loss_tot:0.0882,\n",
      "Save model 8724\n",
      "Train Epoch:8725 learning rate:1.0000e-05, Loss_tot:0.0878,\n",
      "Save model 8725\n",
      "Train Epoch:8726 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8726\n",
      "Train Epoch:8727 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8727\n",
      "Train Epoch:8728 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8728\n",
      "Train Epoch:8729 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8729\n",
      "Train Epoch:8730 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8730\n",
      "Train Epoch:8731 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8731\n",
      "Train Epoch:8732 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8732\n",
      "Train Epoch:8733 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8733\n",
      "Train Epoch:8734 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8734\n",
      "Train Epoch:8735 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8735\n",
      "Train Epoch:8736 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8736\n",
      "Train Epoch:8737 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8737\n",
      "Train Epoch:8738 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8738\n",
      "Train Epoch:8739 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8739\n",
      "Train Epoch:8740 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8740\n",
      "Train Epoch:8741 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8741\n",
      "Train Epoch:8742 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8742\n",
      "Train Epoch:8743 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8743\n",
      "Train Epoch:8744 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 8744\n",
      "Train Epoch:8745 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8745\n",
      "Train Epoch:8746 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8746\n",
      "Train Epoch:8747 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 8747\n",
      "Train Epoch:8748 learning rate:1.0000e-05, Loss_tot:0.0872,\n",
      "Save model 8748\n",
      "Train Epoch:8749 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8749\n",
      "Train Epoch:8750 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8750\n",
      "Train Epoch:8751 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 8751\n",
      "Train Epoch:8752 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8752\n",
      "Train Epoch:8753 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8753\n",
      "Train Epoch:8754 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8754\n",
      "Train Epoch:8755 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8755\n",
      "Train Epoch:8756 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 8756\n",
      "Train Epoch:8757 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8757\n",
      "Train Epoch:8758 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 8758\n",
      "Train Epoch:8759 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8759\n",
      "Train Epoch:8760 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8760\n",
      "Train Epoch:8761 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8761\n",
      "Train Epoch:8762 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8762\n",
      "Train Epoch:8763 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8764 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 8764\n",
      "Train Epoch:8765 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8765\n",
      "Train Epoch:8766 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8766\n",
      "Train Epoch:8767 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8767\n",
      "Train Epoch:8768 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8768\n",
      "Train Epoch:8769 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8769\n",
      "Train Epoch:8770 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8770\n",
      "Train Epoch:8771 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8771\n",
      "Train Epoch:8772 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8772\n",
      "Train Epoch:8773 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8773\n",
      "Train Epoch:8774 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8774\n",
      "Train Epoch:8775 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8775\n",
      "Train Epoch:8776 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8776\n",
      "Train Epoch:8777 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8777\n",
      "Train Epoch:8778 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8778\n",
      "Train Epoch:8779 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8779\n",
      "Train Epoch:8780 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8780\n",
      "Train Epoch:8781 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8781\n",
      "Train Epoch:8782 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8782\n",
      "Train Epoch:8783 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8783\n",
      "Train Epoch:8784 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8784\n",
      "Train Epoch:8785 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8785\n",
      "Train Epoch:8786 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8786\n",
      "Train Epoch:8787 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8787\n",
      "Train Epoch:8788 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 8788\n",
      "Train Epoch:8789 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8789\n",
      "Train Epoch:8790 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8790\n",
      "Train Epoch:8791 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8791\n",
      "Train Epoch:8792 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8792\n",
      "Train Epoch:8793 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8793\n",
      "Train Epoch:8794 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8794\n",
      "Train Epoch:8795 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8795\n",
      "Train Epoch:8796 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8796\n",
      "Train Epoch:8797 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8797\n",
      "Train Epoch:8798 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8798\n",
      "Train Epoch:8799 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8799\n",
      "Train Epoch:8800 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8800\n",
      "Train Epoch:8801 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8801\n",
      "Train Epoch:8802 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8802\n",
      "Train Epoch:8803 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8803\n",
      "Train Epoch:8804 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8804\n",
      "Train Epoch:8805 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8805\n",
      "Train Epoch:8806 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8806\n",
      "Train Epoch:8807 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8807\n",
      "Train Epoch:8808 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8808\n",
      "Train Epoch:8809 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8809\n",
      "Train Epoch:8810 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8810\n",
      "Train Epoch:8811 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8811\n",
      "Train Epoch:8812 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8812\n",
      "Train Epoch:8813 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8813\n",
      "Train Epoch:8814 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "save model\n",
      "Save model 8814\n",
      "Train Epoch:8815 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8815\n",
      "Train Epoch:8816 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8816\n",
      "Train Epoch:8817 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8817\n",
      "Train Epoch:8818 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8818\n",
      "Train Epoch:8819 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8819\n",
      "Train Epoch:8820 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8820\n",
      "Train Epoch:8821 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8821\n",
      "Train Epoch:8822 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "save model\n",
      "Save model 8822\n",
      "Train Epoch:8823 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8823\n",
      "Train Epoch:8824 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8824\n",
      "Train Epoch:8825 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8825\n",
      "Train Epoch:8826 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8826\n",
      "Train Epoch:8827 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8827\n",
      "Train Epoch:8828 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8828\n",
      "Train Epoch:8829 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 8829\n",
      "Train Epoch:8830 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8830\n",
      "Train Epoch:8831 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8831\n",
      "Train Epoch:8832 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8832\n",
      "Train Epoch:8833 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8833\n",
      "Train Epoch:8834 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8834\n",
      "Train Epoch:8835 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "save model\n",
      "Save model 8835\n",
      "Train Epoch:8836 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8836\n",
      "Train Epoch:8837 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8837\n",
      "Train Epoch:8838 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8838\n",
      "Train Epoch:8839 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8839\n",
      "Train Epoch:8840 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8840\n",
      "Train Epoch:8841 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8841\n",
      "Train Epoch:8842 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "save model\n",
      "Save model 8842\n",
      "Train Epoch:8843 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8843\n",
      "Train Epoch:8844 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8844\n",
      "Train Epoch:8845 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "save model\n",
      "Save model 8845\n",
      "Train Epoch:8846 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8846\n",
      "Train Epoch:8847 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8847\n",
      "Train Epoch:8848 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8848\n",
      "Train Epoch:8849 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8849\n",
      "Train Epoch:8850 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8850\n",
      "Train Epoch:8851 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8851\n",
      "Train Epoch:8852 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8852\n",
      "Train Epoch:8853 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8853\n",
      "Train Epoch:8854 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8854\n",
      "Train Epoch:8855 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8855\n",
      "Train Epoch:8856 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8856\n",
      "Train Epoch:8857 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8857\n",
      "Train Epoch:8858 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8858\n",
      "Train Epoch:8859 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8859\n",
      "Train Epoch:8860 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "save model\n",
      "Save model 8860\n",
      "Train Epoch:8861 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8861\n",
      "Train Epoch:8862 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8862\n",
      "Train Epoch:8863 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8863\n",
      "Train Epoch:8864 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8864\n",
      "Train Epoch:8865 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8865\n",
      "Train Epoch:8866 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8866\n",
      "Train Epoch:8867 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8867\n",
      "Train Epoch:8868 learning rate:1.0000e-05, Loss_tot:0.0865,\n",
      "Save model 8868\n",
      "Train Epoch:8869 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8869\n",
      "Train Epoch:8870 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8871 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8871\n",
      "Train Epoch:8872 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8872\n",
      "Train Epoch:8873 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8873\n",
      "Train Epoch:8874 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8874\n",
      "Train Epoch:8875 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8875\n",
      "Train Epoch:8876 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8876\n",
      "Train Epoch:8877 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8877\n",
      "Train Epoch:8878 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8878\n",
      "Train Epoch:8879 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8879\n",
      "Train Epoch:8880 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8880\n",
      "Train Epoch:8881 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8881\n",
      "Train Epoch:8882 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8882\n",
      "Train Epoch:8883 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8883\n",
      "Train Epoch:8884 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8884\n",
      "Train Epoch:8885 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8885\n",
      "Train Epoch:8886 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8886\n",
      "Train Epoch:8887 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8887\n",
      "Train Epoch:8888 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8888\n",
      "Train Epoch:8889 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8889\n",
      "Train Epoch:8890 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8890\n",
      "Train Epoch:8891 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8891\n",
      "Train Epoch:8892 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "save model\n",
      "Save model 8892\n",
      "Train Epoch:8893 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8893\n",
      "Train Epoch:8894 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8894\n",
      "Train Epoch:8895 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8895\n",
      "Train Epoch:8896 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8896\n",
      "Train Epoch:8897 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8897\n",
      "Train Epoch:8898 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8898\n",
      "Train Epoch:8899 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8899\n",
      "Train Epoch:8900 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8900\n",
      "Train Epoch:8901 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8901\n",
      "Train Epoch:8902 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8902\n",
      "Train Epoch:8903 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8903\n",
      "Train Epoch:8904 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8904\n",
      "Train Epoch:8905 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8905\n",
      "Train Epoch:8906 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8906\n",
      "Train Epoch:8907 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8907\n",
      "Train Epoch:8908 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8908\n",
      "Train Epoch:8909 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8909\n",
      "Train Epoch:8910 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8910\n",
      "Train Epoch:8911 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8911\n",
      "Train Epoch:8912 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8912\n",
      "Train Epoch:8913 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 8913\n",
      "Train Epoch:8914 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8914\n",
      "Train Epoch:8915 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8915\n",
      "Train Epoch:8916 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8916\n",
      "Train Epoch:8917 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8917\n",
      "Train Epoch:8918 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8918\n",
      "Train Epoch:8919 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8919\n",
      "Train Epoch:8920 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8920\n",
      "Train Epoch:8921 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8921\n",
      "Train Epoch:8922 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 8922\n",
      "Train Epoch:8923 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8923\n",
      "Train Epoch:8924 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "save model\n",
      "Save model 8924\n",
      "Train Epoch:8925 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8925\n",
      "Train Epoch:8926 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8926\n",
      "Train Epoch:8927 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8927\n",
      "Train Epoch:8928 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8928\n",
      "Train Epoch:8929 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8929\n",
      "Train Epoch:8930 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8930\n",
      "Train Epoch:8931 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8931\n",
      "Train Epoch:8932 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8932\n",
      "Train Epoch:8933 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 8933\n",
      "Train Epoch:8934 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8934\n",
      "Train Epoch:8935 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8935\n",
      "Train Epoch:8936 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8936\n",
      "Train Epoch:8937 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8937\n",
      "Train Epoch:8938 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8938\n",
      "Train Epoch:8939 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8939\n",
      "Train Epoch:8940 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8940\n",
      "Train Epoch:8941 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8941\n",
      "Train Epoch:8942 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8942\n",
      "Train Epoch:8943 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8943\n",
      "Train Epoch:8944 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8944\n",
      "Train Epoch:8945 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8945\n",
      "Train Epoch:8946 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8946\n",
      "Train Epoch:8947 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8947\n",
      "Train Epoch:8948 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8948\n",
      "Train Epoch:8949 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8949\n",
      "Train Epoch:8950 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8950\n",
      "Train Epoch:8951 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8951\n",
      "Train Epoch:8952 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8952\n",
      "Train Epoch:8953 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8953\n",
      "Train Epoch:8954 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8954\n",
      "Train Epoch:8955 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8955\n",
      "Train Epoch:8956 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8956\n",
      "Train Epoch:8957 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8957\n",
      "Train Epoch:8958 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8958\n",
      "Train Epoch:8959 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8959\n",
      "Train Epoch:8960 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8960\n",
      "Train Epoch:8961 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "save model\n",
      "Save model 8961\n",
      "Train Epoch:8962 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8962\n",
      "Train Epoch:8963 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8963\n",
      "Train Epoch:8964 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8964\n",
      "Train Epoch:8965 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8965\n",
      "Train Epoch:8966 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8966\n",
      "Train Epoch:8967 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8967\n",
      "Train Epoch:8968 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8968\n",
      "Train Epoch:8969 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8969\n",
      "Train Epoch:8970 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8970\n",
      "Train Epoch:8971 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8971\n",
      "Train Epoch:8972 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8972\n",
      "Train Epoch:8973 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8973\n",
      "Train Epoch:8974 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8974\n",
      "Train Epoch:8975 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:8976 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8976\n",
      "Train Epoch:8977 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8977\n",
      "Train Epoch:8978 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8978\n",
      "Train Epoch:8979 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8979\n",
      "Train Epoch:8980 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8980\n",
      "Train Epoch:8981 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8981\n",
      "Train Epoch:8982 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8982\n",
      "Train Epoch:8983 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8983\n",
      "Train Epoch:8984 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8984\n",
      "Train Epoch:8985 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8985\n",
      "Train Epoch:8986 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8986\n",
      "Train Epoch:8987 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8987\n",
      "Train Epoch:8988 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 8988\n",
      "Train Epoch:8989 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8989\n",
      "Train Epoch:8990 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8990\n",
      "Train Epoch:8991 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8991\n",
      "Train Epoch:8992 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8992\n",
      "Train Epoch:8993 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8993\n",
      "Train Epoch:8994 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8994\n",
      "Train Epoch:8995 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8995\n",
      "Train Epoch:8996 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8996\n",
      "Train Epoch:8997 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8997\n",
      "Train Epoch:8998 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 8998\n",
      "Train Epoch:8999 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 8999\n",
      "Train Epoch:9000 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9000\n",
      "Train Epoch:9001 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9001\n",
      "Train Epoch:9002 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9002\n",
      "Train Epoch:9003 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "save model\n",
      "Save model 9003\n",
      "Train Epoch:9004 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9004\n",
      "Train Epoch:9005 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9005\n",
      "Train Epoch:9006 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9006\n",
      "Train Epoch:9007 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9007\n",
      "Train Epoch:9008 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9008\n",
      "Train Epoch:9009 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9009\n",
      "Train Epoch:9010 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9010\n",
      "Train Epoch:9011 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9011\n",
      "Train Epoch:9012 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9012\n",
      "Train Epoch:9013 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9013\n",
      "Train Epoch:9014 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9014\n",
      "Train Epoch:9015 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9015\n",
      "Train Epoch:9016 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9016\n",
      "Train Epoch:9017 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9017\n",
      "Train Epoch:9018 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9018\n",
      "Train Epoch:9019 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9019\n",
      "Train Epoch:9020 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9020\n",
      "Train Epoch:9021 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9021\n",
      "Train Epoch:9022 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "save model\n",
      "Save model 9022\n",
      "Train Epoch:9023 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9023\n",
      "Train Epoch:9024 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9024\n",
      "Train Epoch:9025 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9025\n",
      "Train Epoch:9026 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9026\n",
      "Train Epoch:9027 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9027\n",
      "Train Epoch:9028 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9028\n",
      "Train Epoch:9029 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9029\n",
      "Train Epoch:9030 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9030\n",
      "Train Epoch:9031 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9031\n",
      "Train Epoch:9032 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9032\n",
      "Train Epoch:9033 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9033\n",
      "Train Epoch:9034 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9034\n",
      "Train Epoch:9035 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9035\n",
      "Train Epoch:9036 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9036\n",
      "Train Epoch:9037 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9037\n",
      "Train Epoch:9038 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9038\n",
      "Train Epoch:9039 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9039\n",
      "Train Epoch:9040 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9040\n",
      "Train Epoch:9041 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9041\n",
      "Train Epoch:9042 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9042\n",
      "Train Epoch:9043 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9043\n",
      "Train Epoch:9044 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9044\n",
      "Train Epoch:9045 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9045\n",
      "Train Epoch:9046 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9046\n",
      "Train Epoch:9047 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9047\n",
      "Train Epoch:9048 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9048\n",
      "Train Epoch:9049 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9049\n",
      "Train Epoch:9050 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9050\n",
      "Train Epoch:9051 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9051\n",
      "Train Epoch:9052 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9052\n",
      "Train Epoch:9053 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9053\n",
      "Train Epoch:9054 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9054\n",
      "Train Epoch:9055 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9055\n",
      "Train Epoch:9056 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9056\n",
      "Train Epoch:9057 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9057\n",
      "Train Epoch:9058 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9058\n",
      "Train Epoch:9059 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9059\n",
      "Train Epoch:9060 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9060\n",
      "Train Epoch:9061 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9061\n",
      "Train Epoch:9062 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9062\n",
      "Train Epoch:9063 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9063\n",
      "Train Epoch:9064 learning rate:1.0000e-05, Loss_tot:0.1624,\n",
      "Save model 9064\n",
      "Train Epoch:9065 learning rate:1.0000e-05, Loss_tot:0.1621,\n",
      "Save model 9065\n",
      "Train Epoch:9066 learning rate:1.0000e-05, Loss_tot:0.1618,\n",
      "Save model 9066\n",
      "Train Epoch:9067 learning rate:1.0000e-05, Loss_tot:0.1612,\n",
      "Save model 9067\n",
      "Train Epoch:9068 learning rate:1.0000e-05, Loss_tot:0.1604,\n",
      "Save model 9068\n",
      "Train Epoch:9069 learning rate:1.0000e-05, Loss_tot:0.1596,\n",
      "Save model 9069\n",
      "Train Epoch:9070 learning rate:1.0000e-05, Loss_tot:0.1587,\n",
      "Save model 9070\n",
      "Train Epoch:9071 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "Save model 9071\n",
      "Train Epoch:9072 learning rate:1.0000e-05, Loss_tot:0.1565,\n",
      "Save model 9072\n",
      "Train Epoch:9073 learning rate:1.0000e-05, Loss_tot:0.1552,\n",
      "Save model 9073\n",
      "Train Epoch:9074 learning rate:1.0000e-05, Loss_tot:0.1538,\n",
      "Save model 9074\n",
      "Train Epoch:9075 learning rate:1.0000e-05, Loss_tot:0.1527,\n",
      "Save model 9075\n",
      "Train Epoch:9076 learning rate:1.0000e-05, Loss_tot:0.1514,\n",
      "Save model 9076\n",
      "Train Epoch:9077 learning rate:1.0000e-05, Loss_tot:0.1500,\n",
      "Save model 9077\n",
      "Train Epoch:9078 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "Save model 9078\n",
      "Train Epoch:9079 learning rate:1.0000e-05, Loss_tot:0.1468,\n",
      "Save model 9079\n",
      "Train Epoch:9080 learning rate:1.0000e-05, Loss_tot:0.1455,\n",
      "Save model 9080\n",
      "Train Epoch:9081 learning rate:1.0000e-05, Loss_tot:0.1441,\n",
      "Save model 9081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9082 learning rate:1.0000e-05, Loss_tot:0.1426,\n",
      "Save model 9082\n",
      "Train Epoch:9083 learning rate:1.0000e-05, Loss_tot:0.0997,\n",
      "Save model 9083\n",
      "Train Epoch:9084 learning rate:1.0000e-05, Loss_tot:0.1008,\n",
      "Save model 9084\n",
      "Train Epoch:9085 learning rate:1.0000e-05, Loss_tot:0.1014,\n",
      "Save model 9085\n",
      "Train Epoch:9086 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "Save model 9086\n",
      "Train Epoch:9087 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "Save model 9087\n",
      "Train Epoch:9088 learning rate:1.0000e-05, Loss_tot:0.1022,\n",
      "Save model 9088\n",
      "Train Epoch:9089 learning rate:1.0000e-05, Loss_tot:0.1021,\n",
      "Save model 9089\n",
      "Train Epoch:9090 learning rate:1.0000e-05, Loss_tot:0.1018,\n",
      "Save model 9090\n",
      "Train Epoch:9091 learning rate:1.0000e-05, Loss_tot:0.1013,\n",
      "Save model 9091\n",
      "Train Epoch:9092 learning rate:1.0000e-05, Loss_tot:0.1006,\n",
      "Save model 9092\n",
      "Train Epoch:9093 learning rate:1.0000e-05, Loss_tot:0.0998,\n",
      "Save model 9093\n",
      "Train Epoch:9094 learning rate:1.0000e-05, Loss_tot:0.0989,\n",
      "Save model 9094\n",
      "Train Epoch:9095 learning rate:1.0000e-05, Loss_tot:0.0979,\n",
      "Save model 9095\n",
      "Train Epoch:9096 learning rate:1.0000e-05, Loss_tot:0.0967,\n",
      "Save model 9096\n",
      "Train Epoch:9097 learning rate:1.0000e-05, Loss_tot:0.0955,\n",
      "Save model 9097\n",
      "Train Epoch:9098 learning rate:1.0000e-05, Loss_tot:0.0943,\n",
      "Save model 9098\n",
      "Train Epoch:9099 learning rate:1.0000e-05, Loss_tot:0.0930,\n",
      "Save model 9099\n",
      "Train Epoch:9100 learning rate:1.0000e-05, Loss_tot:0.0916,\n",
      "Save model 9100\n",
      "Train Epoch:9101 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 9101\n",
      "Train Epoch:9102 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 9102\n",
      "Train Epoch:9103 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 9103\n",
      "Train Epoch:9104 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 9104\n",
      "Train Epoch:9105 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 9105\n",
      "Train Epoch:9106 learning rate:1.0000e-05, Loss_tot:0.0870,\n",
      "Save model 9106\n",
      "Train Epoch:9107 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 9107\n",
      "Train Epoch:9108 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 9108\n",
      "Train Epoch:9109 learning rate:1.0000e-05, Loss_tot:0.0864,\n",
      "Save model 9109\n",
      "Train Epoch:9110 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9110\n",
      "Train Epoch:9111 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9111\n",
      "Train Epoch:9112 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9112\n",
      "Train Epoch:9113 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9113\n",
      "Train Epoch:9114 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9114\n",
      "Train Epoch:9115 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9115\n",
      "Train Epoch:9116 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9116\n",
      "Train Epoch:9117 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9117\n",
      "Train Epoch:9118 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9118\n",
      "Train Epoch:9119 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9119\n",
      "Train Epoch:9120 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9120\n",
      "Train Epoch:9121 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9121\n",
      "Train Epoch:9122 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9122\n",
      "Train Epoch:9123 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9123\n",
      "Train Epoch:9124 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9124\n",
      "Train Epoch:9125 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9125\n",
      "Train Epoch:9126 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9126\n",
      "Train Epoch:9127 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9127\n",
      "Train Epoch:9128 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9128\n",
      "Train Epoch:9129 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9129\n",
      "Train Epoch:9130 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9130\n",
      "Train Epoch:9131 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9131\n",
      "Train Epoch:9132 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9132\n",
      "Train Epoch:9133 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9133\n",
      "Train Epoch:9134 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9134\n",
      "Train Epoch:9135 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9135\n",
      "Train Epoch:9136 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9136\n",
      "Train Epoch:9137 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9137\n",
      "Train Epoch:9138 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9138\n",
      "Train Epoch:9139 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9139\n",
      "Train Epoch:9140 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9140\n",
      "Train Epoch:9141 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9141\n",
      "Train Epoch:9142 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9142\n",
      "Train Epoch:9143 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9143\n",
      "Train Epoch:9144 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9144\n",
      "Train Epoch:9145 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9145\n",
      "Train Epoch:9146 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9146\n",
      "Train Epoch:9147 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9147\n",
      "Train Epoch:9148 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9148\n",
      "Train Epoch:9149 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9149\n",
      "Train Epoch:9150 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9150\n",
      "Train Epoch:9151 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9151\n",
      "Train Epoch:9152 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9152\n",
      "Train Epoch:9153 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9153\n",
      "Train Epoch:9154 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9154\n",
      "Train Epoch:9155 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9155\n",
      "Train Epoch:9156 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9156\n",
      "Train Epoch:9157 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9157\n",
      "Train Epoch:9158 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9158\n",
      "Train Epoch:9159 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9159\n",
      "Train Epoch:9160 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9160\n",
      "Train Epoch:9161 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9161\n",
      "Train Epoch:9162 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9162\n",
      "Train Epoch:9163 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9163\n",
      "Train Epoch:9164 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9164\n",
      "Train Epoch:9165 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9165\n",
      "Train Epoch:9166 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9166\n",
      "Train Epoch:9167 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9167\n",
      "Train Epoch:9168 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9168\n",
      "Train Epoch:9169 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9169\n",
      "Train Epoch:9170 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9170\n",
      "Train Epoch:9171 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9171\n",
      "Train Epoch:9172 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9172\n",
      "Train Epoch:9173 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9173\n",
      "Train Epoch:9174 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9174\n",
      "Train Epoch:9175 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9175\n",
      "Train Epoch:9176 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9176\n",
      "Train Epoch:9177 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9177\n",
      "Train Epoch:9178 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9178\n",
      "Train Epoch:9179 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9179\n",
      "Train Epoch:9180 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9180\n",
      "Train Epoch:9181 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9181\n",
      "Train Epoch:9182 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9182\n",
      "Train Epoch:9183 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9183\n",
      "Train Epoch:9184 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9184\n",
      "Train Epoch:9185 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9185\n",
      "Train Epoch:9186 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9186\n",
      "Train Epoch:9187 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9187\n",
      "Train Epoch:9188 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9188\n",
      "Train Epoch:9189 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9190 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9190\n",
      "Train Epoch:9191 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9191\n",
      "Train Epoch:9192 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9192\n",
      "Train Epoch:9193 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9193\n",
      "Train Epoch:9194 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9194\n",
      "Train Epoch:9195 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9195\n",
      "Train Epoch:9196 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9196\n",
      "Train Epoch:9197 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9197\n",
      "Train Epoch:9198 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9198\n",
      "Train Epoch:9199 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9199\n",
      "Train Epoch:9200 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9200\n",
      "Train Epoch:9201 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9201\n",
      "Train Epoch:9202 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9202\n",
      "Train Epoch:9203 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "save model\n",
      "Save model 9203\n",
      "Train Epoch:9204 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9204\n",
      "Train Epoch:9205 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9205\n",
      "Train Epoch:9206 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9206\n",
      "Train Epoch:9207 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9207\n",
      "Train Epoch:9208 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9208\n",
      "Train Epoch:9209 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9209\n",
      "Train Epoch:9210 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9210\n",
      "Train Epoch:9211 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9211\n",
      "Train Epoch:9212 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9212\n",
      "Train Epoch:9213 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9213\n",
      "Train Epoch:9214 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9214\n",
      "Train Epoch:9215 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9215\n",
      "Train Epoch:9216 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9216\n",
      "Train Epoch:9217 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9217\n",
      "Train Epoch:9218 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9218\n",
      "Train Epoch:9219 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9219\n",
      "Train Epoch:9220 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9220\n",
      "Train Epoch:9221 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9221\n",
      "Train Epoch:9222 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9222\n",
      "Train Epoch:9223 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9223\n",
      "Train Epoch:9224 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9224\n",
      "Train Epoch:9225 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9225\n",
      "Train Epoch:9226 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9226\n",
      "Train Epoch:9227 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9227\n",
      "Train Epoch:9228 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9228\n",
      "Train Epoch:9229 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9229\n",
      "Train Epoch:9230 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9230\n",
      "Train Epoch:9231 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9231\n",
      "Train Epoch:9232 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9232\n",
      "Train Epoch:9233 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9233\n",
      "Train Epoch:9234 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9234\n",
      "Train Epoch:9235 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9235\n",
      "Train Epoch:9236 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9236\n",
      "Train Epoch:9237 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9237\n",
      "Train Epoch:9238 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "save model\n",
      "Save model 9238\n",
      "Train Epoch:9239 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9239\n",
      "Train Epoch:9240 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9240\n",
      "Train Epoch:9241 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9241\n",
      "Train Epoch:9242 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9242\n",
      "Train Epoch:9243 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9243\n",
      "Train Epoch:9244 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9244\n",
      "Train Epoch:9245 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9245\n",
      "Train Epoch:9246 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9246\n",
      "Train Epoch:9247 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9247\n",
      "Train Epoch:9248 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9248\n",
      "Train Epoch:9249 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9249\n",
      "Train Epoch:9250 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9250\n",
      "Train Epoch:9251 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9251\n",
      "Train Epoch:9252 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9252\n",
      "Train Epoch:9253 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9253\n",
      "Train Epoch:9254 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9254\n",
      "Train Epoch:9255 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9255\n",
      "Train Epoch:9256 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9256\n",
      "Train Epoch:9257 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9257\n",
      "Train Epoch:9258 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9258\n",
      "Train Epoch:9259 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9259\n",
      "Train Epoch:9260 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9260\n",
      "Train Epoch:9261 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9261\n",
      "Train Epoch:9262 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9262\n",
      "Train Epoch:9263 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9263\n",
      "Train Epoch:9264 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9264\n",
      "Train Epoch:9265 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9265\n",
      "Train Epoch:9266 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9266\n",
      "Train Epoch:9267 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9267\n",
      "Train Epoch:9268 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9268\n",
      "Train Epoch:9269 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9269\n",
      "Train Epoch:9270 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9270\n",
      "Train Epoch:9271 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9271\n",
      "Train Epoch:9272 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9272\n",
      "Train Epoch:9273 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9273\n",
      "Train Epoch:9274 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9274\n",
      "Train Epoch:9275 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9275\n",
      "Train Epoch:9276 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9276\n",
      "Train Epoch:9277 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9277\n",
      "Train Epoch:9278 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9278\n",
      "Train Epoch:9279 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9279\n",
      "Train Epoch:9280 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9280\n",
      "Train Epoch:9281 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9281\n",
      "Train Epoch:9282 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9282\n",
      "Train Epoch:9283 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9283\n",
      "Train Epoch:9284 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9284\n",
      "Train Epoch:9285 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9285\n",
      "Train Epoch:9286 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9286\n",
      "Train Epoch:9287 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "save model\n",
      "Save model 9287\n",
      "Train Epoch:9288 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9288\n",
      "Train Epoch:9289 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9289\n",
      "Train Epoch:9290 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9290\n",
      "Train Epoch:9291 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9291\n",
      "Train Epoch:9292 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9292\n",
      "Train Epoch:9293 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9293\n",
      "Train Epoch:9294 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9295 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9295\n",
      "Train Epoch:9296 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9296\n",
      "Train Epoch:9297 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9297\n",
      "Train Epoch:9298 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9298\n",
      "Train Epoch:9299 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9299\n",
      "Train Epoch:9300 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9300\n",
      "Train Epoch:9301 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9301\n",
      "Train Epoch:9302 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9302\n",
      "Train Epoch:9303 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9303\n",
      "Train Epoch:9304 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9304\n",
      "Train Epoch:9305 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9305\n",
      "Train Epoch:9306 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9306\n",
      "Train Epoch:9307 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9307\n",
      "Train Epoch:9308 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9308\n",
      "Train Epoch:9309 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9309\n",
      "Train Epoch:9310 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9310\n",
      "Train Epoch:9311 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9311\n",
      "Train Epoch:9312 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9312\n",
      "Train Epoch:9313 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9313\n",
      "Train Epoch:9314 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9314\n",
      "Train Epoch:9315 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9315\n",
      "Train Epoch:9316 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9316\n",
      "Train Epoch:9317 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9317\n",
      "Train Epoch:9318 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9318\n",
      "Train Epoch:9319 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9319\n",
      "Train Epoch:9320 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9320\n",
      "Train Epoch:9321 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9321\n",
      "Train Epoch:9322 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9322\n",
      "Train Epoch:9323 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9323\n",
      "Train Epoch:9324 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9324\n",
      "Train Epoch:9325 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9325\n",
      "Train Epoch:9326 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9326\n",
      "Train Epoch:9327 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9327\n",
      "Train Epoch:9328 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9328\n",
      "Train Epoch:9329 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9329\n",
      "Train Epoch:9330 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "save model\n",
      "Save model 9330\n",
      "Train Epoch:9331 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9331\n",
      "Train Epoch:9332 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9332\n",
      "Train Epoch:9333 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9333\n",
      "Train Epoch:9334 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9334\n",
      "Train Epoch:9335 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9335\n",
      "Train Epoch:9336 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9336\n",
      "Train Epoch:9337 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9337\n",
      "Train Epoch:9338 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9338\n",
      "Train Epoch:9339 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9339\n",
      "Train Epoch:9340 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9340\n",
      "Train Epoch:9341 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9341\n",
      "Train Epoch:9342 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9342\n",
      "Train Epoch:9343 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9343\n",
      "Train Epoch:9344 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9344\n",
      "Train Epoch:9345 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9345\n",
      "Train Epoch:9346 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9346\n",
      "Train Epoch:9347 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9347\n",
      "Train Epoch:9348 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9348\n",
      "Train Epoch:9349 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9349\n",
      "Train Epoch:9350 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9350\n",
      "Train Epoch:9351 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9351\n",
      "Train Epoch:9352 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9352\n",
      "Train Epoch:9353 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9353\n",
      "Train Epoch:9354 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9354\n",
      "Train Epoch:9355 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9355\n",
      "Train Epoch:9356 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9356\n",
      "Train Epoch:9357 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9357\n",
      "Train Epoch:9358 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9358\n",
      "Train Epoch:9359 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9359\n",
      "Train Epoch:9360 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9360\n",
      "Train Epoch:9361 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9361\n",
      "Train Epoch:9362 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9362\n",
      "Train Epoch:9363 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9363\n",
      "Train Epoch:9364 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9364\n",
      "Train Epoch:9365 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9365\n",
      "Train Epoch:9366 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9366\n",
      "Train Epoch:9367 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9367\n",
      "Train Epoch:9368 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9368\n",
      "Train Epoch:9369 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9369\n",
      "Train Epoch:9370 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9370\n",
      "Train Epoch:9371 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9371\n",
      "Train Epoch:9372 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9372\n",
      "Train Epoch:9373 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9373\n",
      "Train Epoch:9374 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "save model\n",
      "Save model 9374\n",
      "Train Epoch:9375 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9375\n",
      "Train Epoch:9376 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9376\n",
      "Train Epoch:9377 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9377\n",
      "Train Epoch:9378 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9378\n",
      "Train Epoch:9379 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9379\n",
      "Train Epoch:9380 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9380\n",
      "Train Epoch:9381 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9381\n",
      "Train Epoch:9382 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9382\n",
      "Train Epoch:9383 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9383\n",
      "Train Epoch:9384 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9384\n",
      "Train Epoch:9385 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9385\n",
      "Train Epoch:9386 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9386\n",
      "Train Epoch:9387 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9387\n",
      "Train Epoch:9388 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9388\n",
      "Train Epoch:9389 learning rate:1.0000e-05, Loss_tot:0.1617,\n",
      "Save model 9389\n",
      "Train Epoch:9390 learning rate:1.0000e-05, Loss_tot:0.1615,\n",
      "Save model 9390\n",
      "Train Epoch:9391 learning rate:1.0000e-05, Loss_tot:0.1611,\n",
      "Save model 9391\n",
      "Train Epoch:9392 learning rate:1.0000e-05, Loss_tot:0.1607,\n",
      "Save model 9392\n",
      "Train Epoch:9393 learning rate:1.0000e-05, Loss_tot:0.1601,\n",
      "Save model 9393\n",
      "Train Epoch:9394 learning rate:1.0000e-05, Loss_tot:0.1594,\n",
      "Save model 9394\n",
      "Train Epoch:9395 learning rate:1.0000e-05, Loss_tot:0.1585,\n",
      "Save model 9395\n",
      "Train Epoch:9396 learning rate:1.0000e-05, Loss_tot:0.1577,\n",
      "Save model 9396\n",
      "Train Epoch:9397 learning rate:1.0000e-05, Loss_tot:0.1567,\n",
      "Save model 9397\n",
      "Train Epoch:9398 learning rate:1.0000e-05, Loss_tot:0.1556,\n",
      "Save model 9398\n",
      "Train Epoch:9399 learning rate:1.0000e-05, Loss_tot:0.1545,\n",
      "Save model 9399\n",
      "Train Epoch:9400 learning rate:1.0000e-05, Loss_tot:0.1534,\n",
      "Save model 9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9401 learning rate:1.0000e-05, Loss_tot:0.1522,\n",
      "Save model 9401\n",
      "Train Epoch:9402 learning rate:1.0000e-05, Loss_tot:0.1510,\n",
      "Save model 9402\n",
      "Train Epoch:9403 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 9403\n",
      "Train Epoch:9404 learning rate:1.0000e-05, Loss_tot:0.0897,\n",
      "Save model 9404\n",
      "Train Epoch:9405 learning rate:1.0000e-05, Loss_tot:0.0902,\n",
      "Save model 9405\n",
      "Train Epoch:9406 learning rate:1.0000e-05, Loss_tot:0.0907,\n",
      "Save model 9406\n",
      "Train Epoch:9407 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 9407\n",
      "Train Epoch:9408 learning rate:1.0000e-05, Loss_tot:0.0910,\n",
      "Save model 9408\n",
      "Train Epoch:9409 learning rate:1.0000e-05, Loss_tot:0.0908,\n",
      "Save model 9409\n",
      "Train Epoch:9410 learning rate:1.0000e-05, Loss_tot:0.0905,\n",
      "Save model 9410\n",
      "Train Epoch:9411 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 9411\n",
      "Train Epoch:9412 learning rate:1.0000e-05, Loss_tot:0.0896,\n",
      "Save model 9412\n",
      "Train Epoch:9413 learning rate:1.0000e-05, Loss_tot:0.0890,\n",
      "Save model 9413\n",
      "Train Epoch:9414 learning rate:1.0000e-05, Loss_tot:0.0886,\n",
      "Save model 9414\n",
      "Train Epoch:9415 learning rate:1.0000e-05, Loss_tot:0.0881,\n",
      "Save model 9415\n",
      "Train Epoch:9416 learning rate:1.0000e-05, Loss_tot:0.0877,\n",
      "Save model 9416\n",
      "Train Epoch:9417 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 9417\n",
      "Train Epoch:9418 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 9418\n",
      "Train Epoch:9419 learning rate:1.0000e-05, Loss_tot:0.0868,\n",
      "Save model 9419\n",
      "Train Epoch:9420 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 9420\n",
      "Train Epoch:9421 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9421\n",
      "Train Epoch:9422 learning rate:1.0000e-05, Loss_tot:0.0862,\n",
      "Save model 9422\n",
      "Train Epoch:9423 learning rate:1.0000e-05, Loss_tot:0.0860,\n",
      "Save model 9423\n",
      "Train Epoch:9424 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9424\n",
      "Train Epoch:9425 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9425\n",
      "Train Epoch:9426 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9426\n",
      "Train Epoch:9427 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9427\n",
      "Train Epoch:9428 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9428\n",
      "Train Epoch:9429 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9429\n",
      "Train Epoch:9430 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9430\n",
      "Train Epoch:9431 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9431\n",
      "Train Epoch:9432 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9432\n",
      "Train Epoch:9433 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9433\n",
      "Train Epoch:9434 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9434\n",
      "Train Epoch:9435 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9435\n",
      "Train Epoch:9436 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9436\n",
      "Train Epoch:9437 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9437\n",
      "Train Epoch:9438 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9438\n",
      "Train Epoch:9439 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9439\n",
      "Train Epoch:9440 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9440\n",
      "Train Epoch:9441 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9441\n",
      "Train Epoch:9442 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9442\n",
      "Train Epoch:9443 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9443\n",
      "Train Epoch:9444 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9444\n",
      "Train Epoch:9445 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9445\n",
      "Train Epoch:9446 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9446\n",
      "Train Epoch:9447 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9447\n",
      "Train Epoch:9448 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9448\n",
      "Train Epoch:9449 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9449\n",
      "Train Epoch:9450 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9450\n",
      "Train Epoch:9451 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9451\n",
      "Train Epoch:9452 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9452\n",
      "Train Epoch:9453 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9453\n",
      "Train Epoch:9454 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9454\n",
      "Train Epoch:9455 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9455\n",
      "Train Epoch:9456 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9456\n",
      "Train Epoch:9457 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9457\n",
      "Train Epoch:9458 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9458\n",
      "Train Epoch:9459 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9459\n",
      "Train Epoch:9460 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9460\n",
      "Train Epoch:9461 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9461\n",
      "Train Epoch:9462 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9462\n",
      "Train Epoch:9463 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9463\n",
      "Train Epoch:9464 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9464\n",
      "Train Epoch:9465 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9465\n",
      "Train Epoch:9466 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9466\n",
      "Train Epoch:9467 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9467\n",
      "Train Epoch:9468 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9468\n",
      "Train Epoch:9469 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9469\n",
      "Train Epoch:9470 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9470\n",
      "Train Epoch:9471 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9471\n",
      "Train Epoch:9472 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9472\n",
      "Train Epoch:9473 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9473\n",
      "Train Epoch:9474 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9474\n",
      "Train Epoch:9475 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9475\n",
      "Train Epoch:9476 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9476\n",
      "Train Epoch:9477 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9477\n",
      "Train Epoch:9478 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9478\n",
      "Train Epoch:9479 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9479\n",
      "Train Epoch:9480 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9480\n",
      "Train Epoch:9481 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9481\n",
      "Train Epoch:9482 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9482\n",
      "Train Epoch:9483 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9483\n",
      "Train Epoch:9484 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9484\n",
      "Train Epoch:9485 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9485\n",
      "Train Epoch:9486 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9486\n",
      "Train Epoch:9487 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9487\n",
      "Train Epoch:9488 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9488\n",
      "Train Epoch:9489 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9489\n",
      "Train Epoch:9490 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9490\n",
      "Train Epoch:9491 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9491\n",
      "Train Epoch:9492 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9492\n",
      "Train Epoch:9493 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9493\n",
      "Train Epoch:9494 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9494\n",
      "Train Epoch:9495 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9495\n",
      "Train Epoch:9496 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9496\n",
      "Train Epoch:9497 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9497\n",
      "Train Epoch:9498 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9498\n",
      "Train Epoch:9499 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9499\n",
      "Train Epoch:9500 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9500\n",
      "Train Epoch:9501 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9501\n",
      "Train Epoch:9502 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9502\n",
      "Train Epoch:9503 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9503\n",
      "Train Epoch:9504 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9504\n",
      "Train Epoch:9505 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9505\n",
      "Train Epoch:9506 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9506\n",
      "Train Epoch:9507 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9507\n",
      "Train Epoch:9508 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9509 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9509\n",
      "Train Epoch:9510 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9510\n",
      "Train Epoch:9511 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9511\n",
      "Train Epoch:9512 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9512\n",
      "Train Epoch:9513 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9513\n",
      "Train Epoch:9514 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9514\n",
      "Train Epoch:9515 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9515\n",
      "Train Epoch:9516 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9516\n",
      "Train Epoch:9517 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9517\n",
      "Train Epoch:9518 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9518\n",
      "Train Epoch:9519 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9519\n",
      "Train Epoch:9520 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "save model\n",
      "Save model 9520\n",
      "Train Epoch:9521 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9521\n",
      "Train Epoch:9522 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9522\n",
      "Train Epoch:9523 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9523\n",
      "Train Epoch:9524 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9524\n",
      "Train Epoch:9525 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9525\n",
      "Train Epoch:9526 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9526\n",
      "Train Epoch:9527 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9527\n",
      "Train Epoch:9528 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9528\n",
      "Train Epoch:9529 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9529\n",
      "Train Epoch:9530 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9530\n",
      "Train Epoch:9531 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9531\n",
      "Train Epoch:9532 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9532\n",
      "Train Epoch:9533 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9533\n",
      "Train Epoch:9534 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9534\n",
      "Train Epoch:9535 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9535\n",
      "Train Epoch:9536 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9536\n",
      "Train Epoch:9537 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9537\n",
      "Train Epoch:9538 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9538\n",
      "Train Epoch:9539 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9539\n",
      "Train Epoch:9540 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9540\n",
      "Train Epoch:9541 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9541\n",
      "Train Epoch:9542 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9542\n",
      "Train Epoch:9543 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9543\n",
      "Train Epoch:9544 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9544\n",
      "Train Epoch:9545 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9545\n",
      "Train Epoch:9546 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9546\n",
      "Train Epoch:9547 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9547\n",
      "Train Epoch:9548 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9548\n",
      "Train Epoch:9549 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9549\n",
      "Train Epoch:9550 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9550\n",
      "Train Epoch:9551 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9551\n",
      "Train Epoch:9552 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9552\n",
      "Train Epoch:9553 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9553\n",
      "Train Epoch:9554 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9554\n",
      "Train Epoch:9555 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9555\n",
      "Train Epoch:9556 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9556\n",
      "Train Epoch:9557 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9557\n",
      "Train Epoch:9558 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9558\n",
      "Train Epoch:9559 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9559\n",
      "Train Epoch:9560 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9560\n",
      "Train Epoch:9561 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "save model\n",
      "Save model 9561\n",
      "Train Epoch:9562 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9562\n",
      "Train Epoch:9563 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9563\n",
      "Train Epoch:9564 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9564\n",
      "Train Epoch:9565 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9565\n",
      "Train Epoch:9566 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9566\n",
      "Train Epoch:9567 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9567\n",
      "Train Epoch:9568 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9568\n",
      "Train Epoch:9569 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9569\n",
      "Train Epoch:9570 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9570\n",
      "Train Epoch:9571 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9571\n",
      "Train Epoch:9572 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9572\n",
      "Train Epoch:9573 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9573\n",
      "Train Epoch:9574 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9574\n",
      "Train Epoch:9575 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9575\n",
      "Train Epoch:9576 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9576\n",
      "Train Epoch:9577 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9577\n",
      "Train Epoch:9578 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9578\n",
      "Train Epoch:9579 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9579\n",
      "Train Epoch:9580 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9580\n",
      "Train Epoch:9581 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9581\n",
      "Train Epoch:9582 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9582\n",
      "Train Epoch:9583 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9583\n",
      "Train Epoch:9584 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9584\n",
      "Train Epoch:9585 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9585\n",
      "Train Epoch:9586 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9586\n",
      "Train Epoch:9587 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9587\n",
      "Train Epoch:9588 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9588\n",
      "Train Epoch:9589 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9589\n",
      "Train Epoch:9590 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9590\n",
      "Train Epoch:9591 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9591\n",
      "Train Epoch:9592 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9592\n",
      "Train Epoch:9593 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9593\n",
      "Train Epoch:9594 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9594\n",
      "Train Epoch:9595 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9595\n",
      "Train Epoch:9596 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9596\n",
      "Train Epoch:9597 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9597\n",
      "Train Epoch:9598 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9598\n",
      "Train Epoch:9599 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9599\n",
      "Train Epoch:9600 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9600\n",
      "Train Epoch:9601 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9601\n",
      "Train Epoch:9602 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9602\n",
      "Train Epoch:9603 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9603\n",
      "Train Epoch:9604 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9604\n",
      "Train Epoch:9605 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9605\n",
      "Train Epoch:9606 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9606\n",
      "Train Epoch:9607 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9607\n",
      "Train Epoch:9608 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "save model\n",
      "Save model 9608\n",
      "Train Epoch:9609 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9609\n",
      "Train Epoch:9610 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9610\n",
      "Train Epoch:9611 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9611\n",
      "Train Epoch:9612 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9612\n",
      "Train Epoch:9613 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9613\n",
      "Train Epoch:9614 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9615 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9615\n",
      "Train Epoch:9616 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9616\n",
      "Train Epoch:9617 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9617\n",
      "Train Epoch:9618 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9618\n",
      "Train Epoch:9619 learning rate:1.0000e-05, Loss_tot:0.1609,\n",
      "Save model 9619\n",
      "Train Epoch:9620 learning rate:1.0000e-05, Loss_tot:0.1608,\n",
      "Save model 9620\n",
      "Train Epoch:9621 learning rate:1.0000e-05, Loss_tot:0.1604,\n",
      "Save model 9621\n",
      "Train Epoch:9622 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "Save model 9622\n",
      "Train Epoch:9623 learning rate:1.0000e-05, Loss_tot:0.1595,\n",
      "Save model 9623\n",
      "Train Epoch:9624 learning rate:1.0000e-05, Loss_tot:0.1588,\n",
      "Save model 9624\n",
      "Train Epoch:9625 learning rate:1.0000e-05, Loss_tot:0.1580,\n",
      "Save model 9625\n",
      "Train Epoch:9626 learning rate:1.0000e-05, Loss_tot:0.1571,\n",
      "Save model 9626\n",
      "Train Epoch:9627 learning rate:1.0000e-05, Loss_tot:0.1563,\n",
      "Save model 9627\n",
      "Train Epoch:9628 learning rate:1.0000e-05, Loss_tot:0.1554,\n",
      "Save model 9628\n",
      "Train Epoch:9629 learning rate:1.0000e-05, Loss_tot:0.1543,\n",
      "Save model 9629\n",
      "Train Epoch:9630 learning rate:1.0000e-05, Loss_tot:0.1532,\n",
      "Save model 9630\n",
      "Train Epoch:9631 learning rate:1.0000e-05, Loss_tot:0.1520,\n",
      "Save model 9631\n",
      "Train Epoch:9632 learning rate:1.0000e-05, Loss_tot:0.1509,\n",
      "Save model 9632\n",
      "Train Epoch:9633 learning rate:1.0000e-05, Loss_tot:0.1498,\n",
      "Save model 9633\n",
      "Train Epoch:9634 learning rate:1.0000e-05, Loss_tot:0.1485,\n",
      "Save model 9634\n",
      "Train Epoch:9635 learning rate:1.0000e-05, Loss_tot:0.1472,\n",
      "Save model 9635\n",
      "Train Epoch:9636 learning rate:1.0000e-05, Loss_tot:0.1460,\n",
      "Save model 9636\n",
      "Train Epoch:9637 learning rate:1.0000e-05, Loss_tot:0.0925,\n",
      "Save model 9637\n",
      "Train Epoch:9638 learning rate:1.0000e-05, Loss_tot:0.0935,\n",
      "Save model 9638\n",
      "Train Epoch:9639 learning rate:1.0000e-05, Loss_tot:0.0942,\n",
      "Save model 9639\n",
      "Train Epoch:9640 learning rate:1.0000e-05, Loss_tot:0.0946,\n",
      "Save model 9640\n",
      "Train Epoch:9641 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 9641\n",
      "Train Epoch:9642 learning rate:1.0000e-05, Loss_tot:0.0949,\n",
      "Save model 9642\n",
      "Train Epoch:9643 learning rate:1.0000e-05, Loss_tot:0.0947,\n",
      "Save model 9643\n",
      "Train Epoch:9644 learning rate:1.0000e-05, Loss_tot:0.0944,\n",
      "Save model 9644\n",
      "Train Epoch:9645 learning rate:1.0000e-05, Loss_tot:0.0939,\n",
      "Save model 9645\n",
      "Train Epoch:9646 learning rate:1.0000e-05, Loss_tot:0.0933,\n",
      "Save model 9646\n",
      "Train Epoch:9647 learning rate:1.0000e-05, Loss_tot:0.0926,\n",
      "Save model 9647\n",
      "Train Epoch:9648 learning rate:1.0000e-05, Loss_tot:0.0917,\n",
      "Save model 9648\n",
      "Train Epoch:9649 learning rate:1.0000e-05, Loss_tot:0.0909,\n",
      "Save model 9649\n",
      "Train Epoch:9650 learning rate:1.0000e-05, Loss_tot:0.0901,\n",
      "Save model 9650\n",
      "Train Epoch:9651 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 9651\n",
      "Train Epoch:9652 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 9652\n",
      "Train Epoch:9653 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 9653\n",
      "Train Epoch:9654 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 9654\n",
      "Train Epoch:9655 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 9655\n",
      "Train Epoch:9656 learning rate:1.0000e-05, Loss_tot:0.0867,\n",
      "Save model 9656\n",
      "Train Epoch:9657 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9657\n",
      "Train Epoch:9658 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9658\n",
      "Train Epoch:9659 learning rate:1.0000e-05, Loss_tot:0.0859,\n",
      "Save model 9659\n",
      "Train Epoch:9660 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9660\n",
      "Train Epoch:9661 learning rate:1.0000e-05, Loss_tot:0.0856,\n",
      "Save model 9661\n",
      "Train Epoch:9662 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9662\n",
      "Train Epoch:9663 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9663\n",
      "Train Epoch:9664 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9664\n",
      "Train Epoch:9665 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9665\n",
      "Train Epoch:9666 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9666\n",
      "Train Epoch:9667 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9667\n",
      "Train Epoch:9668 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9668\n",
      "Train Epoch:9669 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9669\n",
      "Train Epoch:9670 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9670\n",
      "Train Epoch:9671 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9671\n",
      "Train Epoch:9672 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9672\n",
      "Train Epoch:9673 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9673\n",
      "Train Epoch:9674 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9674\n",
      "Train Epoch:9675 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9675\n",
      "Train Epoch:9676 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9676\n",
      "Train Epoch:9677 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9677\n",
      "Train Epoch:9678 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9678\n",
      "Train Epoch:9679 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9679\n",
      "Train Epoch:9680 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9680\n",
      "Train Epoch:9681 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9681\n",
      "Train Epoch:9682 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9682\n",
      "Train Epoch:9683 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9683\n",
      "Train Epoch:9684 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9684\n",
      "Train Epoch:9685 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9685\n",
      "Train Epoch:9686 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9686\n",
      "Train Epoch:9687 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9687\n",
      "Train Epoch:9688 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9688\n",
      "Train Epoch:9689 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9689\n",
      "Train Epoch:9690 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9690\n",
      "Train Epoch:9691 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9691\n",
      "Train Epoch:9692 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9692\n",
      "Train Epoch:9693 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9693\n",
      "Train Epoch:9694 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9694\n",
      "Train Epoch:9695 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9695\n",
      "Train Epoch:9696 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9696\n",
      "Train Epoch:9697 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9697\n",
      "Train Epoch:9698 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9698\n",
      "Train Epoch:9699 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9699\n",
      "Train Epoch:9700 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9700\n",
      "Train Epoch:9701 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9701\n",
      "Train Epoch:9702 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9702\n",
      "Train Epoch:9703 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9703\n",
      "Train Epoch:9704 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9704\n",
      "Train Epoch:9705 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9705\n",
      "Train Epoch:9706 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9706\n",
      "Train Epoch:9707 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9707\n",
      "Train Epoch:9708 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9708\n",
      "Train Epoch:9709 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9709\n",
      "Train Epoch:9710 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9710\n",
      "Train Epoch:9711 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9711\n",
      "Train Epoch:9712 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9712\n",
      "Train Epoch:9713 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9713\n",
      "Train Epoch:9714 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9714\n",
      "Train Epoch:9715 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9715\n",
      "Train Epoch:9716 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9716\n",
      "Train Epoch:9717 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9717\n",
      "Train Epoch:9718 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9718\n",
      "Train Epoch:9719 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9719\n",
      "Train Epoch:9720 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9720\n",
      "Train Epoch:9721 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9721\n",
      "Train Epoch:9722 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9723 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9723\n",
      "Train Epoch:9724 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9724\n",
      "Train Epoch:9725 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9725\n",
      "Train Epoch:9726 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9726\n",
      "Train Epoch:9727 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9727\n",
      "Train Epoch:9728 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9728\n",
      "Train Epoch:9729 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9729\n",
      "Train Epoch:9730 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9730\n",
      "Train Epoch:9731 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9731\n",
      "Train Epoch:9732 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9732\n",
      "Train Epoch:9733 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9733\n",
      "Train Epoch:9734 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9734\n",
      "Train Epoch:9735 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9735\n",
      "Train Epoch:9736 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9736\n",
      "Train Epoch:9737 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9737\n",
      "Train Epoch:9738 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9738\n",
      "Train Epoch:9739 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9739\n",
      "Train Epoch:9740 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9740\n",
      "Train Epoch:9741 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9741\n",
      "Train Epoch:9742 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9742\n",
      "Train Epoch:9743 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9743\n",
      "Train Epoch:9744 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9744\n",
      "Train Epoch:9745 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9745\n",
      "Train Epoch:9746 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9746\n",
      "Train Epoch:9747 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9747\n",
      "Train Epoch:9748 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9748\n",
      "Train Epoch:9749 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9749\n",
      "Train Epoch:9750 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9750\n",
      "Train Epoch:9751 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9751\n",
      "Train Epoch:9752 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9752\n",
      "Train Epoch:9753 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9753\n",
      "Train Epoch:9754 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9754\n",
      "Train Epoch:9755 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9755\n",
      "Train Epoch:9756 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9756\n",
      "Train Epoch:9757 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9757\n",
      "Train Epoch:9758 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9758\n",
      "Train Epoch:9759 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9759\n",
      "Train Epoch:9760 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9760\n",
      "Train Epoch:9761 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9761\n",
      "Train Epoch:9762 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9762\n",
      "Train Epoch:9763 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9763\n",
      "Train Epoch:9764 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9764\n",
      "Train Epoch:9765 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9765\n",
      "Train Epoch:9766 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9766\n",
      "Train Epoch:9767 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9767\n",
      "Train Epoch:9768 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9768\n",
      "Train Epoch:9769 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9769\n",
      "Train Epoch:9770 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "save model\n",
      "Save model 9770\n",
      "Train Epoch:9771 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9771\n",
      "Train Epoch:9772 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9772\n",
      "Train Epoch:9773 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9773\n",
      "Train Epoch:9774 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9774\n",
      "Train Epoch:9775 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9775\n",
      "Train Epoch:9776 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "save model\n",
      "Save model 9776\n",
      "Train Epoch:9777 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9777\n",
      "Train Epoch:9778 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9778\n",
      "Train Epoch:9779 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9779\n",
      "Train Epoch:9780 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9780\n",
      "Train Epoch:9781 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9781\n",
      "Train Epoch:9782 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9782\n",
      "Train Epoch:9783 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9783\n",
      "Train Epoch:9784 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9784\n",
      "Train Epoch:9785 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9785\n",
      "Train Epoch:9786 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9786\n",
      "Train Epoch:9787 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9787\n",
      "Train Epoch:9788 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9788\n",
      "Train Epoch:9789 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9789\n",
      "Train Epoch:9790 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9790\n",
      "Train Epoch:9791 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9791\n",
      "Train Epoch:9792 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9792\n",
      "Train Epoch:9793 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9793\n",
      "Train Epoch:9794 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "save model\n",
      "Save model 9794\n",
      "Train Epoch:9795 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9795\n",
      "Train Epoch:9796 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9796\n",
      "Train Epoch:9797 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9797\n",
      "Train Epoch:9798 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "save model\n",
      "Save model 9798\n",
      "Train Epoch:9799 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9799\n",
      "Train Epoch:9800 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9800\n",
      "Train Epoch:9801 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "save model\n",
      "Save model 9801\n",
      "Train Epoch:9802 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9802\n",
      "Train Epoch:9803 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9803\n",
      "Train Epoch:9804 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "save model\n",
      "Save model 9804\n",
      "Train Epoch:9805 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9805\n",
      "Train Epoch:9806 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9806\n",
      "Train Epoch:9807 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9807\n",
      "Train Epoch:9808 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9808\n",
      "Train Epoch:9809 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9809\n",
      "Train Epoch:9810 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9810\n",
      "Train Epoch:9811 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9811\n",
      "Train Epoch:9812 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9812\n",
      "Train Epoch:9813 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9813\n",
      "Train Epoch:9814 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9814\n",
      "Train Epoch:9815 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9815\n",
      "Train Epoch:9816 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9816\n",
      "Train Epoch:9817 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9817\n",
      "Train Epoch:9818 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9818\n",
      "Train Epoch:9819 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9819\n",
      "Train Epoch:9820 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9820\n",
      "Train Epoch:9821 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9821\n",
      "Train Epoch:9822 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9822\n",
      "Train Epoch:9823 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9823\n",
      "Train Epoch:9824 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9824\n",
      "Train Epoch:9825 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9825\n",
      "Train Epoch:9826 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9826\n",
      "Train Epoch:9827 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9827\n",
      "Train Epoch:9828 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9828\n",
      "Train Epoch:9829 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9830 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9830\n",
      "Train Epoch:9831 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9831\n",
      "Train Epoch:9832 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9832\n",
      "Train Epoch:9833 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9833\n",
      "Train Epoch:9834 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9834\n",
      "Train Epoch:9835 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9835\n",
      "Train Epoch:9836 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9836\n",
      "Train Epoch:9837 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9837\n",
      "Train Epoch:9838 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9838\n",
      "Train Epoch:9839 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9839\n",
      "Train Epoch:9840 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9840\n",
      "Train Epoch:9841 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9841\n",
      "Train Epoch:9842 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9842\n",
      "Train Epoch:9843 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9843\n",
      "Train Epoch:9844 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9844\n",
      "Train Epoch:9845 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9845\n",
      "Train Epoch:9846 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9846\n",
      "Train Epoch:9847 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9847\n",
      "Train Epoch:9848 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9848\n",
      "Train Epoch:9849 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9849\n",
      "Train Epoch:9850 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9850\n",
      "Train Epoch:9851 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9851\n",
      "Train Epoch:9852 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9852\n",
      "Train Epoch:9853 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9853\n",
      "Train Epoch:9854 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9854\n",
      "Train Epoch:9855 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9855\n",
      "Train Epoch:9856 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9856\n",
      "Train Epoch:9857 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9857\n",
      "Train Epoch:9858 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9858\n",
      "Train Epoch:9859 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9859\n",
      "Train Epoch:9860 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "save model\n",
      "Save model 9860\n",
      "Train Epoch:9861 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9861\n",
      "Train Epoch:9862 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9862\n",
      "Train Epoch:9863 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9863\n",
      "Train Epoch:9864 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9864\n",
      "Train Epoch:9865 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9865\n",
      "Train Epoch:9866 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9866\n",
      "Train Epoch:9867 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9867\n",
      "Train Epoch:9868 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9868\n",
      "Train Epoch:9869 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9869\n",
      "Train Epoch:9870 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9870\n",
      "Train Epoch:9871 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9871\n",
      "Train Epoch:9872 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9872\n",
      "Train Epoch:9873 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9873\n",
      "Train Epoch:9874 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9874\n",
      "Train Epoch:9875 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9875\n",
      "Train Epoch:9876 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9876\n",
      "Train Epoch:9877 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9877\n",
      "Train Epoch:9878 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9878\n",
      "Train Epoch:9879 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9879\n",
      "Train Epoch:9880 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9880\n",
      "Train Epoch:9881 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9881\n",
      "Train Epoch:9882 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9882\n",
      "Train Epoch:9883 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9883\n",
      "Train Epoch:9884 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9884\n",
      "Train Epoch:9885 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9885\n",
      "Train Epoch:9886 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9886\n",
      "Train Epoch:9887 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9887\n",
      "Train Epoch:9888 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9888\n",
      "Train Epoch:9889 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9889\n",
      "Train Epoch:9890 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9890\n",
      "Train Epoch:9891 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9891\n",
      "Train Epoch:9892 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9892\n",
      "Train Epoch:9893 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9893\n",
      "Train Epoch:9894 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9894\n",
      "Train Epoch:9895 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9895\n",
      "Train Epoch:9896 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9896\n",
      "Train Epoch:9897 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9897\n",
      "Train Epoch:9898 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9898\n",
      "Train Epoch:9899 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "save model\n",
      "Save model 9899\n",
      "Train Epoch:9900 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9900\n",
      "Train Epoch:9901 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9901\n",
      "Train Epoch:9902 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9902\n",
      "Train Epoch:9903 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9903\n",
      "Train Epoch:9904 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9904\n",
      "Train Epoch:9905 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9905\n",
      "Train Epoch:9906 learning rate:1.0000e-05, Loss_tot:0.1603,\n",
      "Save model 9906\n",
      "Train Epoch:9907 learning rate:1.0000e-05, Loss_tot:0.1602,\n",
      "Save model 9907\n",
      "Train Epoch:9908 learning rate:1.0000e-05, Loss_tot:0.1600,\n",
      "Save model 9908\n",
      "Train Epoch:9909 learning rate:1.0000e-05, Loss_tot:0.1595,\n",
      "Save model 9909\n",
      "Train Epoch:9910 learning rate:1.0000e-05, Loss_tot:0.1589,\n",
      "Save model 9910\n",
      "Train Epoch:9911 learning rate:1.0000e-05, Loss_tot:0.1582,\n",
      "Save model 9911\n",
      "Train Epoch:9912 learning rate:1.0000e-05, Loss_tot:0.1576,\n",
      "Save model 9912\n",
      "Train Epoch:9913 learning rate:1.0000e-05, Loss_tot:0.1569,\n",
      "Save model 9913\n",
      "Train Epoch:9914 learning rate:1.0000e-05, Loss_tot:0.1560,\n",
      "Save model 9914\n",
      "Train Epoch:9915 learning rate:1.0000e-05, Loss_tot:0.1550,\n",
      "Save model 9915\n",
      "Train Epoch:9916 learning rate:1.0000e-05, Loss_tot:0.1539,\n",
      "Save model 9916\n",
      "Train Epoch:9917 learning rate:1.0000e-05, Loss_tot:0.1529,\n",
      "Save model 9917\n",
      "Train Epoch:9918 learning rate:1.0000e-05, Loss_tot:0.1518,\n",
      "Save model 9918\n",
      "Train Epoch:9919 learning rate:1.0000e-05, Loss_tot:0.1507,\n",
      "Save model 9919\n",
      "Train Epoch:9920 learning rate:1.0000e-05, Loss_tot:0.0874,\n",
      "Save model 9920\n",
      "Train Epoch:9921 learning rate:1.0000e-05, Loss_tot:0.0879,\n",
      "Save model 9921\n",
      "Train Epoch:9922 learning rate:1.0000e-05, Loss_tot:0.0884,\n",
      "Save model 9922\n",
      "Train Epoch:9923 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 9923\n",
      "Train Epoch:9924 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 9924\n",
      "Train Epoch:9925 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 9925\n",
      "Train Epoch:9926 learning rate:1.0000e-05, Loss_tot:0.0893,\n",
      "Save model 9926\n",
      "Train Epoch:9927 learning rate:1.0000e-05, Loss_tot:0.0892,\n",
      "Save model 9927\n",
      "Train Epoch:9928 learning rate:1.0000e-05, Loss_tot:0.0891,\n",
      "Save model 9928\n",
      "Train Epoch:9929 learning rate:1.0000e-05, Loss_tot:0.0888,\n",
      "Save model 9929\n",
      "Train Epoch:9930 learning rate:1.0000e-05, Loss_tot:0.0885,\n",
      "Save model 9930\n",
      "Train Epoch:9931 learning rate:1.0000e-05, Loss_tot:0.0883,\n",
      "Save model 9931\n",
      "Train Epoch:9932 learning rate:1.0000e-05, Loss_tot:0.0880,\n",
      "Save model 9932\n",
      "Train Epoch:9933 learning rate:1.0000e-05, Loss_tot:0.0876,\n",
      "Save model 9933\n",
      "Train Epoch:9934 learning rate:1.0000e-05, Loss_tot:0.0873,\n",
      "Save model 9934\n",
      "Train Epoch:9935 learning rate:1.0000e-05, Loss_tot:0.0871,\n",
      "Save model 9935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:9936 learning rate:1.0000e-05, Loss_tot:0.0869,\n",
      "Save model 9936\n",
      "Train Epoch:9937 learning rate:1.0000e-05, Loss_tot:0.0866,\n",
      "Save model 9937\n",
      "Train Epoch:9938 learning rate:1.0000e-05, Loss_tot:0.0863,\n",
      "Save model 9938\n",
      "Train Epoch:9939 learning rate:1.0000e-05, Loss_tot:0.0861,\n",
      "Save model 9939\n",
      "Train Epoch:9940 learning rate:1.0000e-05, Loss_tot:0.0858,\n",
      "Save model 9940\n",
      "Train Epoch:9941 learning rate:1.0000e-05, Loss_tot:0.0857,\n",
      "Save model 9941\n",
      "Train Epoch:9942 learning rate:1.0000e-05, Loss_tot:0.0855,\n",
      "Save model 9942\n",
      "Train Epoch:9943 learning rate:1.0000e-05, Loss_tot:0.0854,\n",
      "Save model 9943\n",
      "Train Epoch:9944 learning rate:1.0000e-05, Loss_tot:0.0853,\n",
      "Save model 9944\n",
      "Train Epoch:9945 learning rate:1.0000e-05, Loss_tot:0.0852,\n",
      "Save model 9945\n",
      "Train Epoch:9946 learning rate:1.0000e-05, Loss_tot:0.0851,\n",
      "Save model 9946\n",
      "Train Epoch:9947 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9947\n",
      "Train Epoch:9948 learning rate:1.0000e-05, Loss_tot:0.0850,\n",
      "Save model 9948\n",
      "Train Epoch:9949 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9949\n",
      "Train Epoch:9950 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9950\n",
      "Train Epoch:9951 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9951\n",
      "Train Epoch:9952 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9952\n",
      "Train Epoch:9953 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9953\n",
      "Train Epoch:9954 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9954\n",
      "Train Epoch:9955 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9955\n",
      "Train Epoch:9956 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9956\n",
      "Train Epoch:9957 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9957\n",
      "Train Epoch:9958 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9958\n",
      "Train Epoch:9959 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9959\n",
      "Train Epoch:9960 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9960\n",
      "Train Epoch:9961 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9961\n",
      "Train Epoch:9962 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9962\n",
      "Train Epoch:9963 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9963\n",
      "Train Epoch:9964 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9964\n",
      "Train Epoch:9965 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9965\n",
      "Train Epoch:9966 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9966\n",
      "Train Epoch:9967 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9967\n",
      "Train Epoch:9968 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9968\n",
      "Train Epoch:9969 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9969\n",
      "Train Epoch:9970 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9970\n",
      "Train Epoch:9971 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9971\n",
      "Train Epoch:9972 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9972\n",
      "Train Epoch:9973 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9973\n",
      "Train Epoch:9974 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9974\n",
      "Train Epoch:9975 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9975\n",
      "Train Epoch:9976 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9976\n",
      "Train Epoch:9977 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9977\n",
      "Train Epoch:9978 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9978\n",
      "Train Epoch:9979 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9979\n",
      "Train Epoch:9980 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9980\n",
      "Train Epoch:9981 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9981\n",
      "Train Epoch:9982 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9982\n",
      "Train Epoch:9983 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9983\n",
      "Train Epoch:9984 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9984\n",
      "Train Epoch:9985 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9985\n",
      "Train Epoch:9986 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9986\n",
      "Train Epoch:9987 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9987\n",
      "Train Epoch:9988 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9988\n",
      "Train Epoch:9989 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9989\n",
      "Train Epoch:9990 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9990\n",
      "Train Epoch:9991 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9991\n",
      "Train Epoch:9992 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9992\n",
      "Train Epoch:9993 learning rate:1.0000e-05, Loss_tot:0.0849,\n",
      "Save model 9993\n",
      "Train Epoch:9994 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9994\n",
      "Train Epoch:9995 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9995\n",
      "Train Epoch:9996 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 9996\n",
      "Train Epoch:9997 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9997\n",
      "Train Epoch:9998 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9998\n",
      "Train Epoch:9999 learning rate:1.0000e-05, Loss_tot:0.0848,\n",
      "Save model 9999\n",
      "Train Epoch:10000 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 10000\n",
      "Train Epoch:10001 learning rate:1.0000e-05, Loss_tot:0.0847,\n",
      "Save model 10001\n",
      "Training time =  15991.685837984085\n"
     ]
    }
   ],
   "source": [
    "# some parameters\n",
    "epochs = 10001\n",
    "path_data = \"./data/\"\n",
    "path_model = \"./model/\"\n",
    "path_pic = \"./pic/\" \n",
    "learning_rate = 1e-5\n",
    "rmin = 0.05\n",
    "rmax = 3\n",
    "n = 150\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DNN_train(rmin, rmax, n,learning_rate, epochs, path_model, device,func_LE_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af904b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
